Graph(num_nodes=20, num_edges=292,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})
cuda:1
Namespace(print_freq=1, tb_freq=1, save_freq=1, batch_size=32, num_workers=12, num_copies=6, num_samples=2000, epochs=100, optimizer='adam', learning_rate=0.005, lr_decay_epochs=[120, 160, 200], lr_decay_rate=0.0, beta1=0.9, beta2=0.999, weight_decay=1e-05, momentum=0.9, clip_norm=1.0, resume='', aug='1st', exp='Pretrain', dataset='dgl', model='gin', num_layer=5, readout='avg', set2set_lstm_layer=3, set2set_iter=6, norm=True, nce_k=16384, nce_t=0.07, rw_hops=256, subgraph_size=128, restart_prob=0.8, hidden_size=64, positional_embedding_size=32, max_node_freq=16, max_edge_freq=16, max_degree=512, freq_embedding_size=16, degree_embedding_size=16, model_path='saved', tb_path='tensorboard', load_path=None, moco=True, finetune=False, alpha=0.999, gpu=3, seed=0, fold_idx=0, cv=False, cvrun=-1, positional_embedding_multi=3, model_name='Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_multi_3_momentum_0.999', model_folder='saved/Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_multi_3_momentum_0.999', tb_folder='tensorboard/Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_multi_3_momentum_0.999')
Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_multi_3_momentum_0.999
Use GPU: 3 for training
setting random seeds
before construct dataset 8.486141204833984
load graph done
before construct dataloader 8.486141204833984
before training 8.486141204833984
output 64
output 64
using queue shape: (16384,64)
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [0][1/750]	BT 21.728 (21.728)	DT 20.435 (20.435)	loss 5.526 (5.526)	gnorm 56.070 (56.070)	prob 5.776 (5.7763)	GS 31.078 (31.078)	mem 44.912
Train: [0][2/750]	BT 1.070 (11.399)	DT 0.900 (10.667)	loss 11.574 (11.574)	gnorm 7425.408 (7425.408)	prob -0.284 (-0.2839)	GS 30.891 (30.891)	mem 45.057
Train: [0][3/750]	BT 0.101 (7.633)	DT 0.003 (7.112)	loss 10.825 (10.825)	gnorm 6015229.500 (6015229.500)	prob 0.463 (0.4629)	GS 29.094 (29.094)	mem 45.278
Train: [0][4/750]	BT 2.050 (6.237)	DT 1.943 (5.820)	loss 7.048 (7.048)	gnorm 214699712.000 (214699712.000)	prob 5.699 (5.6993)	GS 35.453 (35.453)	mem 45.113
Train: [0][5/750]	BT 0.149 (5.020)	DT 0.002 (4.656)	loss 6.257 (6.257)	gnorm 188592256.000 (188592256.000)	prob 6.376 (6.3764)	GS 39.203 (39.203)	mem 45.152
Train: [0][6/750]	BT 0.162 (4.210)	DT 0.004 (3.881)	loss 6.826 (6.826)	gnorm 136279392.000 (136279392.000)	prob 6.098 (6.0979)	GS 33.812 (33.812)	mem 45.221
Train: [0][7/750]	BT 0.221 (3.640)	DT 0.008 (3.328)	loss 6.652 (6.652)	gnorm 142529792.000 (142529792.000)	prob 6.220 (6.2205)	GS 36.719 (36.719)	mem 45.130
Train: [0][8/750]	BT 0.092 (3.197)	DT 0.004 (2.912)	loss 6.478 (6.478)	gnorm 120979808.000 (120979808.000)	prob 6.107 (6.1069)	GS 35.703 (35.703)	mem 45.135
Train: [0][9/750]	BT 0.177 (2.861)	DT 0.002 (2.589)	loss 6.274 (6.274)	gnorm 103364968.000 (103364968.000)	prob 6.723 (6.7228)	GS 29.438 (29.438)	mem 45.213
Train: [0][10/750]	BT 0.192 (2.594)	DT 0.010 (2.331)	loss 6.634 (6.634)	gnorm 96547592.000 (96547592.000)	prob 6.503 (6.5032)	GS 33.375 (33.375)	mem 45.191
Train: [0][11/750]	BT 0.096 (2.367)	DT 0.003 (2.119)	loss 6.756 (6.756)	gnorm 90322352.000 (90322352.000)	prob 6.409 (6.4091)	GS 36.500 (36.500)	mem 45.143
Train: [0][12/750]	BT 0.647 (2.224)	DT 0.524 (1.987)	loss 7.185 (7.185)	gnorm 82900000.000 (82900000.000)	prob 5.801 (5.8014)	GS 35.344 (35.344)	mem 45.219
Train: [0][13/750]	BT 4.837 (2.425)	DT 4.715 (2.196)	loss 7.298 (7.298)	gnorm 75779896.000 (75779896.000)	prob 5.953 (5.9526)	GS 33.844 (33.844)	mem 45.407
Train: [0][14/750]	BT 6.723 (2.732)	DT 6.615 (2.512)	loss 7.066 (7.066)	gnorm 67918384.000 (67918384.000)	prob 6.292 (6.2924)	GS 33.750 (33.750)	mem 45.723
Train: [0][15/750]	BT 0.285 (2.569)	DT 0.002 (2.345)	loss 7.576 (7.576)	gnorm 69287808.000 (69287808.000)	prob 5.838 (5.8385)	GS 34.203 (34.203)	mem 45.816
Train: [0][16/750]	BT 0.256 (2.424)	DT 0.011 (2.199)	loss 7.203 (7.203)	gnorm 67612008.000 (67612008.000)	prob 6.026 (6.0258)	GS 35.156 (35.156)	mem 45.582
Train: [0][17/750]	BT 5.223 (2.589)	DT 5.103 (2.370)	loss 7.759 (7.759)	gnorm 70949296.000 (70949296.000)	prob 5.624 (5.6239)	GS 43.219 (43.219)	mem 45.753
Train: [0][18/750]	BT 0.089 (2.450)	DT 0.001 (2.238)	loss 7.336 (7.336)	gnorm 62691828.000 (62691828.000)	prob 5.865 (5.8648)	GS 36.828 (36.828)	mem 45.788
Train: [0][19/750]	BT 0.108 (2.327)	DT 0.002 (2.120)	loss 6.631 (6.631)	gnorm 55376728.000 (55376728.000)	prob 6.850 (6.8495)	GS 30.250 (30.250)	mem 45.764
Train: [0][20/750]	BT 0.096 (2.215)	DT 0.002 (2.014)	loss 7.781 (7.781)	gnorm 62626116.000 (62626116.000)	prob 5.265 (5.2646)	GS 34.453 (34.453)	mem 45.767
Train: [0][21/750]	BT 0.234 (2.121)	DT 0.002 (1.919)	loss 7.376 (7.376)	gnorm 55971988.000 (55971988.000)	prob 6.267 (6.2666)	GS 29.734 (29.734)	mem 45.773
Train: [0][22/750]	BT 0.109 (2.029)	DT 0.002 (1.832)	loss 8.478 (8.478)	gnorm 57986976.000 (57986976.000)	prob 4.546 (4.5459)	GS 30.938 (30.938)	mem 45.776
Train: [0][23/750]	BT 0.121 (1.946)	DT 0.002 (1.752)	loss 7.236 (7.236)	gnorm 46769284.000 (46769284.000)	prob 6.014 (6.0137)	GS 29.922 (29.922)	mem 45.835
Train: [0][24/750]	BT 0.166 (1.872)	DT 0.005 (1.679)	loss 7.291 (7.291)	gnorm 44232884.000 (44232884.000)	prob 5.863 (5.8628)	GS 34.688 (34.688)	mem 45.928
Train: [0][25/750]	BT 0.145 (1.803)	DT 0.004 (1.612)	loss 7.278 (7.278)	gnorm 44128184.000 (44128184.000)	prob 6.398 (6.3981)	GS 31.312 (31.312)	mem 45.889
Train: [0][26/750]	BT 8.208 (2.049)	DT 8.120 (1.862)	loss 7.590 (7.590)	gnorm 45277988.000 (45277988.000)	prob 6.011 (6.0108)	GS 36.922 (36.922)	mem 46.099
Train: [0][27/750]	BT 0.236 (1.982)	DT 0.002 (1.794)	loss 7.641 (7.641)	gnorm 43548408.000 (43548408.000)	prob 5.771 (5.7712)	GS 30.719 (30.719)	mem 46.010
Train: [0][28/750]	BT 0.138 (1.916)	DT 0.002 (1.730)	loss 8.649 (8.649)	gnorm 46439528.000 (46439528.000)	prob 4.299 (4.2995)	GS 32.156 (32.156)	mem 46.012
Train: [0][29/750]	BT 1.077 (1.887)	DT 0.943 (1.702)	loss 7.957 (7.957)	gnorm 40485184.000 (40485184.000)	prob 5.380 (5.3799)	GS 33.891 (33.891)	mem 46.164
Train: [0][30/750]	BT 0.120 (1.829)	DT 0.005 (1.646)	loss 7.997 (7.997)	gnorm 40291224.000 (40291224.000)	prob 4.866 (4.8662)	GS 30.547 (30.547)	mem 46.090
Train: [0][31/750]	BT 0.129 (1.774)	DT 0.008 (1.593)	loss 7.697 (7.697)	gnorm 36108732.000 (36108732.000)	prob 5.752 (5.7517)	GS 29.906 (29.906)	mem 46.138
Train: [0][32/750]	BT 0.091 (1.721)	DT 0.003 (1.543)	loss 7.921 (7.921)	gnorm 35512444.000 (35512444.000)	prob 5.543 (5.5433)	GS 34.109 (34.109)	mem 46.060
Train: [0][33/750]	BT 0.127 (1.673)	DT 0.002 (1.497)	loss 7.926 (7.926)	gnorm 34219264.000 (34219264.000)	prob 5.729 (5.7291)	GS 29.859 (29.859)	mem 46.112
Train: [0][34/750]	BT 0.109 (1.627)	DT 0.003 (1.453)	loss 7.780 (7.780)	gnorm 34806340.000 (34806340.000)	prob 5.486 (5.4855)	GS 32.188 (32.188)	mem 46.076
Train: [0][35/750]	BT 0.137 (1.584)	DT 0.020 (1.412)	loss 8.521 (8.521)	gnorm 37049180.000 (37049180.000)	prob 4.685 (4.6851)	GS 30.031 (30.031)	mem 46.080
Train: [0][36/750]	BT 0.087 (1.543)	DT 0.004 (1.373)	loss 7.678 (7.678)	gnorm 33935720.000 (33935720.000)	prob 5.727 (5.7273)	GS 33.516 (33.516)	mem 46.102
Train: [0][37/750]	BT 0.166 (1.506)	DT 0.002 (1.336)	loss 8.061 (8.061)	gnorm 33650980.000 (33650980.000)	prob 5.506 (5.5057)	GS 34.641 (34.641)	mem 46.159
Train: [0][38/750]	BT 12.585 (1.797)	DT 12.494 (1.629)	loss 7.748 (7.748)	gnorm 31669314.000 (31669314.000)	prob 5.827 (5.8270)	GS 33.016 (33.016)	mem 46.291
Train: [0][39/750]	BT 0.111 (1.754)	DT 0.002 (1.587)	loss 7.527 (7.527)	gnorm 29257048.000 (29257048.000)	prob 6.065 (6.0651)	GS 32.781 (32.781)	mem 46.292
Train: [0][40/750]	BT 0.125 (1.713)	DT 0.007 (1.548)	loss 7.895 (7.895)	gnorm 26861600.000 (26861600.000)	prob 5.820 (5.8201)	GS 33.109 (33.109)	mem 46.331
Train: [0][41/750]	BT 0.204 (1.676)	DT 0.007 (1.510)	loss 8.126 (8.126)	gnorm 30291646.000 (30291646.000)	prob 5.327 (5.3271)	GS 30.328 (30.328)	mem 46.352
Train: [0][42/750]	BT 0.086 (1.638)	DT 0.003 (1.475)	loss 8.447 (8.447)	gnorm 29811686.000 (29811686.000)	prob 5.089 (5.0890)	GS 33.906 (33.906)	mem 46.319
Train: [0][43/750]	BT 0.133 (1.603)	DT 0.001 (1.440)	loss 7.541 (7.541)	gnorm 28497568.000 (28497568.000)	prob 6.381 (6.3810)	GS 30.500 (30.500)	mem 46.324
Train: [0][44/750]	BT 0.135 (1.570)	DT 0.032 (1.408)	loss 8.293 (8.293)	gnorm 30260510.000 (30260510.000)	prob 4.982 (4.9822)	GS 33.188 (33.188)	mem 46.327
Train: [0][45/750]	BT 0.153 (1.539)	DT 0.002 (1.377)	loss 8.133 (8.133)	gnorm 27723146.000 (27723146.000)	prob 5.937 (5.9373)	GS 30.109 (30.109)	mem 46.329
Train: [0][46/750]	BT 0.213 (1.510)	DT 0.028 (1.348)	loss 8.340 (8.340)	gnorm 25735454.000 (25735454.000)	prob 4.897 (4.8971)	GS 35.078 (35.078)	mem 46.337
Train: [0][47/750]	BT 0.121 (1.480)	DT 0.006 (1.319)	loss 7.837 (7.837)	gnorm 27265342.000 (27265342.000)	prob 6.073 (6.0730)	GS 32.859 (32.859)	mem 46.343
Train: [0][48/750]	BT 0.165 (1.453)	DT 0.002 (1.292)	loss 8.377 (8.377)	gnorm 25343520.000 (25343520.000)	prob 4.789 (4.7890)	GS 32.594 (32.594)	mem 46.364
Train: [0][49/750]	BT 0.100 (1.425)	DT 0.008 (1.265)	loss 8.816 (8.816)	gnorm 27551702.000 (27551702.000)	prob 5.407 (5.4069)	GS 30.156 (30.156)	mem 46.426
Train: [0][50/750]	BT 13.851 (1.674)	DT 13.684 (1.514)	loss 8.174 (8.174)	gnorm 25814112.000 (25814112.000)	prob 5.327 (5.3273)	GS 37.656 (37.656)	mem 45.706
Train: [0][51/750]	BT 0.121 (1.643)	DT 0.002 (1.484)	loss 7.769 (7.769)	gnorm 23365596.000 (23365596.000)	prob 6.173 (6.1728)	GS 33.375 (33.375)	mem 45.707
Train: [0][52/750]	BT 0.130 (1.614)	DT 0.009 (1.456)	loss 7.771 (7.771)	gnorm 22263386.000 (22263386.000)	prob 5.686 (5.6864)	GS 31.328 (31.328)	mem 45.753
Train: [0][53/750]	BT 0.136 (1.586)	DT 0.009 (1.429)	loss 7.903 (7.903)	gnorm 24240122.000 (24240122.000)	prob 6.038 (6.0382)	GS 31.172 (31.172)	mem 45.734
Train: [0][54/750]	BT 0.119 (1.559)	DT 0.010 (1.402)	loss 8.669 (8.669)	gnorm 23891544.000 (23891544.000)	prob 4.898 (4.8978)	GS 36.734 (36.734)	mem 45.788
Train: [0][55/750]	BT 0.174 (1.534)	DT 0.002 (1.377)	loss 8.534 (8.534)	gnorm 22978524.000 (22978524.000)	prob 4.986 (4.9855)	GS 31.375 (31.375)	mem 45.879
Train: [0][56/750]	BT 0.182 (1.510)	DT 0.004 (1.352)	loss 7.498 (7.498)	gnorm 20250532.000 (20250532.000)	prob 6.260 (6.2599)	GS 34.562 (34.562)	mem 45.739
Train: [0][57/750]	BT 0.263 (1.488)	DT 0.002 (1.329)	loss 8.346 (8.346)	gnorm 22945746.000 (22945746.000)	prob 5.548 (5.5477)	GS 32.531 (32.531)	mem 45.742
Train: [0][58/750]	BT 0.284 (1.467)	DT 0.006 (1.306)	loss 9.015 (9.015)	gnorm 22587252.000 (22587252.000)	prob 4.267 (4.2666)	GS 33.766 (33.766)	mem 45.744
Train: [0][59/750]	BT 0.170 (1.445)	DT 0.035 (1.284)	loss 8.376 (8.376)	gnorm 19106326.000 (19106326.000)	prob 5.561 (5.5607)	GS 25.984 (25.984)	mem 45.756
Train: [0][60/750]	BT 0.136 (1.423)	DT 0.010 (1.263)	loss 8.292 (8.292)	gnorm 22135228.000 (22135228.000)	prob 5.501 (5.5006)	GS 36.734 (36.734)	mem 45.793
Train: [0][61/750]	BT 0.197 (1.403)	DT 0.008 (1.242)	loss 8.069 (8.069)	gnorm 19465782.000 (19465782.000)	prob 5.695 (5.6950)	GS 32.078 (32.078)	mem 45.857
Train: [0][62/750]	BT 12.770 (1.587)	DT 12.658 (1.427)	loss 8.394 (8.394)	gnorm 18233504.000 (18233504.000)	prob 5.609 (5.6091)	GS 29.719 (29.719)	mem 46.016
Train: [0][63/750]	BT 0.265 (1.566)	DT 0.012 (1.404)	loss 8.058 (8.058)	gnorm 19255782.000 (19255782.000)	prob 5.446 (5.4456)	GS 38.359 (38.359)	mem 45.843
Train: [0][64/750]	BT 0.086 (1.542)	DT 0.003 (1.382)	loss 8.272 (8.272)	gnorm 17863764.000 (17863764.000)	prob 5.044 (5.0438)	GS 30.141 (30.141)	mem 45.800
Train: [0][65/750]	BT 0.131 (1.521)	DT 0.003 (1.361)	loss 8.462 (8.462)	gnorm 18278774.000 (18278774.000)	prob 5.057 (5.0570)	GS 32.562 (32.562)	mem 45.802
Train: [0][66/750]	BT 0.106 (1.499)	DT 0.002 (1.340)	loss 9.019 (9.019)	gnorm 19401056.000 (19401056.000)	prob 4.698 (4.6979)	GS 31.266 (31.266)	mem 45.881
Train: [0][67/750]	BT 0.110 (1.479)	DT 0.003 (1.320)	loss 8.694 (8.694)	gnorm 17878966.000 (17878966.000)	prob 4.551 (4.5513)	GS 33.031 (33.031)	mem 45.862
Train: [0][68/750]	BT 0.134 (1.459)	DT 0.008 (1.301)	loss 8.128 (8.128)	gnorm 16991110.000 (16991110.000)	prob 5.301 (5.3013)	GS 35.469 (35.469)	mem 45.809
Train: [0][69/750]	BT 0.077 (1.439)	DT 0.001 (1.282)	loss 7.885 (7.885)	gnorm 17811896.000 (17811896.000)	prob 5.529 (5.5286)	GS 32.844 (32.844)	mem 45.810
Train: [0][70/750]	BT 0.320 (1.423)	DT 0.002 (1.264)	loss 8.655 (8.655)	gnorm 17211952.000 (17211952.000)	prob 4.818 (4.8179)	GS 34.266 (34.266)	mem 45.851
Train: [0][71/750]	BT 0.114 (1.404)	DT 0.005 (1.246)	loss 8.194 (8.194)	gnorm 17148760.000 (17148760.000)	prob 5.570 (5.5698)	GS 31.203 (31.203)	mem 45.814
Train: [0][72/750]	BT 0.210 (1.388)	DT 0.003 (1.229)	loss 8.766 (8.766)	gnorm 15548147.000 (15548147.000)	prob 4.862 (4.8617)	GS 37.844 (37.844)	mem 45.814
Train: [0][73/750]	BT 0.132 (1.371)	DT 0.003 (1.212)	loss 8.172 (8.172)	gnorm 16645095.000 (16645095.000)	prob 5.665 (5.6647)	GS 29.828 (29.828)	mem 45.894
Train: [0][74/750]	BT 9.077 (1.475)	DT 8.982 (1.317)	loss 7.998 (7.998)	gnorm 15414453.000 (15414453.000)	prob 5.350 (5.3497)	GS 30.984 (30.984)	mem 45.978
Train: [0][75/750]	BT 0.098 (1.456)	DT 0.003 (1.300)	loss 8.164 (8.164)	gnorm 17329580.000 (17329580.000)	prob 5.430 (5.4302)	GS 37.734 (37.734)	mem 46.001
Train: [0][76/750]	BT 0.105 (1.439)	DT 0.002 (1.283)	loss 9.245 (9.245)	gnorm 16712738.000 (16712738.000)	prob 3.823 (3.8228)	GS 33.125 (33.125)	mem 45.909
Train: [0][77/750]	BT 0.118 (1.421)	DT 0.001 (1.266)	loss 8.025 (8.025)	gnorm 16163425.000 (16163425.000)	prob 5.395 (5.3947)	GS 36.328 (36.328)	mem 45.956
Train: [0][78/750]	BT 0.102 (1.405)	DT 0.006 (1.250)	loss 9.170 (9.170)	gnorm 15291939.000 (15291939.000)	prob 4.539 (4.5395)	GS 32.578 (32.578)	mem 45.958
Train: [0][79/750]	BT 0.116 (1.388)	DT 0.001 (1.234)	loss 8.754 (8.754)	gnorm 17424688.000 (17424688.000)	prob 5.286 (5.2862)	GS 32.172 (32.172)	mem 45.916
Train: [0][80/750]	BT 0.137 (1.373)	DT 0.002 (1.219)	loss 9.211 (9.211)	gnorm 16475107.000 (16475107.000)	prob 3.920 (3.9200)	GS 34.141 (34.141)	mem 45.919
Train: [0][81/750]	BT 0.089 (1.357)	DT 0.003 (1.204)	loss 8.769 (8.769)	gnorm 14421576.000 (14421576.000)	prob 5.727 (5.7270)	GS 29.562 (29.562)	mem 45.920
Train: [0][82/750]	BT 0.098 (1.341)	DT 0.003 (1.189)	loss 9.620 (9.620)	gnorm 16420192.000 (16420192.000)	prob 4.152 (4.1519)	GS 33.859 (33.859)	mem 45.956
Train: [0][83/750]	BT 0.193 (1.328)	DT 0.012 (1.175)	loss 8.633 (8.633)	gnorm 16129377.000 (16129377.000)	prob 4.928 (4.9280)	GS 40.766 (40.766)	mem 46.131
Train: [0][84/750]	BT 0.303 (1.315)	DT 0.011 (1.161)	loss 8.846 (8.846)	gnorm 14984074.000 (14984074.000)	prob 5.094 (5.0936)	GS 35.594 (35.594)	mem 46.175
Train: [0][85/750]	BT 0.338 (1.304)	DT 0.003 (1.147)	loss 8.459 (8.459)	gnorm 13682423.000 (13682423.000)	prob 5.228 (5.2278)	GS 28.734 (28.734)	mem 45.936
Train: [0][86/750]	BT 10.951 (1.416)	DT 10.818 (1.260)	loss 7.992 (7.992)	gnorm 13327780.000 (13327780.000)	prob 5.725 (5.7247)	GS 30.906 (30.906)	mem 46.141
Train: [0][87/750]	BT 0.104 (1.401)	DT 0.002 (1.245)	loss 8.527 (8.527)	gnorm 13480312.000 (13480312.000)	prob 5.125 (5.1248)	GS 31.016 (31.016)	mem 46.144
Train: [0][88/750]	BT 0.171 (1.387)	DT 0.057 (1.232)	loss 8.970 (8.970)	gnorm 13542563.000 (13542563.000)	prob 4.504 (4.5043)	GS 38.688 (38.688)	mem 46.146
Train: [0][89/750]	BT 0.203 (1.374)	DT 0.014 (1.218)	loss 9.035 (9.035)	gnorm 13141121.000 (13141121.000)	prob 4.288 (4.2882)	GS 28.734 (28.734)	mem 46.158
Train: [0][90/750]	BT 0.222 (1.361)	DT 0.010 (1.205)	loss 9.175 (9.175)	gnorm 13517286.000 (13517286.000)	prob 4.737 (4.7375)	GS 28.406 (28.406)	mem 46.211
Train: [0][91/750]	BT 0.125 (1.347)	DT 0.004 (1.191)	loss 9.132 (9.132)	gnorm 13972146.000 (13972146.000)	prob 4.645 (4.6453)	GS 27.844 (27.844)	mem 46.155
Train: [0][92/750]	BT 0.488 (1.338)	DT 0.403 (1.183)	loss 8.612 (8.612)	gnorm 14942371.000 (14942371.000)	prob 4.865 (4.8654)	GS 32.781 (32.781)	mem 46.161
Train: [0][93/750]	BT 0.079 (1.324)	DT 0.002 (1.170)	loss 8.584 (8.584)	gnorm 13305725.000 (13305725.000)	prob 4.865 (4.8651)	GS 32.031 (32.031)	mem 46.164
Train: [0][94/750]	BT 0.248 (1.313)	DT 0.022 (1.158)	loss 8.601 (8.601)	gnorm 12427770.000 (12427770.000)	prob 5.061 (5.0610)	GS 33.203 (33.203)	mem 46.211
Train: [0][95/750]	BT 0.167 (1.301)	DT 0.006 (1.146)	loss 8.965 (8.965)	gnorm 12097256.000 (12097256.000)	prob 4.695 (4.6949)	GS 29.859 (29.859)	mem 46.167
Train: [0][96/750]	BT 1.990 (1.308)	DT 1.846 (1.153)	loss 8.700 (8.700)	gnorm 13691093.000 (13691093.000)	prob 5.069 (5.0687)	GS 39.906 (39.906)	mem 46.408
Train: [0][97/750]	BT 0.206 (1.297)	DT 0.002 (1.141)	loss 8.666 (8.666)	gnorm 12590272.000 (12590272.000)	prob 4.475 (4.4752)	GS 33.000 (33.000)	mem 46.444
Train: [0][98/750]	BT 7.810 (1.363)	DT 7.587 (1.207)	loss 8.734 (8.734)	gnorm 11379009.000 (11379009.000)	prob 5.273 (5.2731)	GS 34.266 (34.266)	mem 46.496
Train: [0][99/750]	BT 0.134 (1.351)	DT 0.009 (1.195)	loss 8.965 (8.965)	gnorm 11723840.000 (11723840.000)	prob 4.925 (4.9245)	GS 32.594 (32.594)	mem 46.351
Train: [0][100/750]	BT 3.028 (1.368)	DT 2.905 (1.212)	loss 9.142 (9.142)	gnorm 11891999.000 (11891999.000)	prob 4.441 (4.4414)	GS 34.469 (34.469)	mem 46.363
Train: [0][101/750]	BT 0.164 (1.356)	DT 0.003 (1.200)	loss 9.053 (9.053)	gnorm 11309931.000 (11309931.000)	prob 4.968 (4.9683)	GS 25.844 (25.844)	mem 46.402
Train: [0][102/750]	BT 0.129 (1.344)	DT 0.002 (1.188)	loss 9.094 (9.094)	gnorm 10900352.000 (10900352.000)	prob 4.889 (4.8887)	GS 35.656 (35.656)	mem 46.403
Train: [0][103/750]	BT 0.094 (1.331)	DT 0.008 (1.177)	loss 8.780 (8.780)	gnorm 13215001.000 (13215001.000)	prob 5.482 (5.4824)	GS 32.859 (32.859)	mem 46.404
Train: [0][104/750]	BT 0.116 (1.320)	DT 0.001 (1.166)	loss 8.695 (8.695)	gnorm 12133142.000 (12133142.000)	prob 5.008 (5.0078)	GS 33.438 (33.438)	mem 46.406
Train: [0][105/750]	BT 0.105 (1.308)	DT 0.003 (1.155)	loss 8.507 (8.507)	gnorm 10730112.000 (10730112.000)	prob 6.183 (6.1831)	GS 31.047 (31.047)	mem 46.407
Train: [0][106/750]	BT 0.267 (1.298)	DT 0.003 (1.144)	loss 9.017 (9.017)	gnorm 10608244.000 (10608244.000)	prob 4.923 (4.9225)	GS 37.656 (37.656)	mem 46.410
Train: [0][107/750]	BT 0.115 (1.287)	DT 0.003 (1.133)	loss 8.166 (8.166)	gnorm 11011965.000 (11011965.000)	prob 6.360 (6.3598)	GS 30.828 (30.828)	mem 46.412
Train: [0][108/750]	BT 1.722 (1.291)	DT 1.468 (1.136)	loss 9.246 (9.246)	gnorm 13313529.000 (13313529.000)	prob 4.229 (4.2293)	GS 34.922 (34.922)	mem 46.497
Train: [0][109/750]	BT 0.111 (1.281)	DT 0.011 (1.126)	loss 9.235 (9.235)	gnorm 11310497.000 (11310497.000)	prob 4.967 (4.9671)	GS 30.766 (30.766)	mem 46.584
Train: [0][110/750]	BT 8.419 (1.345)	DT 8.196 (1.190)	loss 8.760 (8.760)	gnorm 10338707.000 (10338707.000)	prob 4.924 (4.9244)	GS 35.156 (35.156)	mem 46.590
Train: [0][111/750]	BT 0.209 (1.335)	DT 0.002 (1.179)	loss 9.356 (9.356)	gnorm 9805103.000 (9805103.000)	prob 4.899 (4.8993)	GS 27.844 (27.844)	mem 46.533
Train: [0][112/750]	BT 5.876 (1.376)	DT 5.727 (1.220)	loss 9.563 (9.563)	gnorm 10615596.000 (10615596.000)	prob 4.188 (4.1877)	GS 34.812 (34.812)	mem 45.713
Train: [0][113/750]	BT 0.117 (1.365)	DT 0.010 (1.209)	loss 8.767 (8.767)	gnorm 12256652.000 (12256652.000)	prob 4.707 (4.7074)	GS 30.172 (30.172)	mem 45.713
Train: [0][114/750]	BT 0.142 (1.354)	DT 0.001 (1.199)	loss 8.685 (8.685)	gnorm 8874517.000 (8874517.000)	prob 4.581 (4.5808)	GS 33.062 (33.062)	mem 45.714
Train: [0][115/750]	BT 0.154 (1.343)	DT 0.009 (1.188)	loss 9.355 (9.355)	gnorm 10080007.000 (10080007.000)	prob 4.688 (4.6879)	GS 32.500 (32.500)	mem 45.766
Train: [0][116/750]	BT 0.144 (1.333)	DT 0.004 (1.178)	loss 9.315 (9.315)	gnorm 10215033.000 (10215033.000)	prob 4.183 (4.1826)	GS 35.297 (35.297)	mem 45.718
Train: [0][117/750]	BT 0.120 (1.323)	DT 0.020 (1.168)	loss 9.027 (9.027)	gnorm 10883652.000 (10883652.000)	prob 5.039 (5.0392)	GS 34.422 (34.422)	mem 45.719
Train: [0][118/750]	BT 0.120 (1.313)	DT 0.002 (1.158)	loss 9.206 (9.206)	gnorm 11035928.000 (11035928.000)	prob 4.610 (4.6100)	GS 32.328 (32.328)	mem 45.719
Train: [0][119/750]	BT 0.081 (1.302)	DT 0.008 (1.149)	loss 8.468 (8.468)	gnorm 9519228.000 (9519228.000)	prob 5.870 (5.8698)	GS 29.688 (29.688)	mem 45.722
Train: [0][120/750]	BT 2.574 (1.313)	DT 2.398 (1.159)	loss 10.223 (10.223)	gnorm 9697577.000 (9697577.000)	prob 3.644 (3.6435)	GS 34.500 (34.500)	mem 45.917
Train: [0][121/750]	BT 0.126 (1.303)	DT 0.025 (1.150)	loss 9.056 (9.056)	gnorm 9402212.000 (9402212.000)	prob 4.762 (4.7617)	GS 31.812 (31.812)	mem 45.792
Train: [0][122/750]	BT 2.893 (1.316)	DT 2.730 (1.163)	loss 9.039 (9.039)	gnorm 9729521.000 (9729521.000)	prob 4.691 (4.6912)	GS 36.125 (36.125)	mem 45.880
Train: [0][123/750]	BT 0.158 (1.307)	DT 0.007 (1.153)	loss 9.308 (9.308)	gnorm 9667432.000 (9667432.000)	prob 4.711 (4.7110)	GS 32.188 (32.188)	mem 45.832
Train: [0][124/750]	BT 5.076 (1.337)	DT 4.893 (1.183)	loss 9.058 (9.058)	gnorm 9926903.000 (9926903.000)	prob 4.844 (4.8439)	GS 38.406 (38.406)	mem 45.800
Train: [0][125/750]	BT 0.143 (1.327)	DT 0.018 (1.174)	loss 8.956 (8.956)	gnorm 9556920.000 (9556920.000)	prob 5.103 (5.1028)	GS 33.344 (33.344)	mem 45.801
Train: [0][126/750]	BT 0.116 (1.318)	DT 0.001 (1.165)	loss 9.328 (9.328)	gnorm 8935423.000 (8935423.000)	prob 4.430 (4.4295)	GS 32.688 (32.688)	mem 45.855
Train: [0][127/750]	BT 0.180 (1.309)	DT 0.011 (1.156)	loss 9.587 (9.587)	gnorm 10472302.000 (10472302.000)	prob 4.665 (4.6646)	GS 33.938 (33.938)	mem 45.806
Train: [0][128/750]	BT 0.110 (1.299)	DT 0.009 (1.147)	loss 9.088 (9.088)	gnorm 9833816.000 (9833816.000)	prob 4.479 (4.4795)	GS 35.719 (35.719)	mem 45.806
Train: [0][129/750]	BT 0.064 (1.290)	DT 0.001 (1.138)	loss 8.977 (8.977)	gnorm 9091496.000 (9091496.000)	prob 5.469 (5.4694)	GS 28.672 (28.672)	mem 45.808
Train: [0][130/750]	BT 0.068 (1.281)	DT 0.002 (1.129)	loss 8.288 (8.288)	gnorm 8939434.000 (8939434.000)	prob 5.221 (5.2210)	GS 33.547 (33.547)	mem 45.809
Train: [0][131/750]	BT 0.110 (1.272)	DT 0.002 (1.121)	loss 8.750 (8.750)	gnorm 8620801.000 (8620801.000)	prob 5.072 (5.0717)	GS 37.453 (37.453)	mem 45.811
Train: [0][132/750]	BT 4.250 (1.294)	DT 4.104 (1.143)	loss 9.688 (9.688)	gnorm 9215658.000 (9215658.000)	prob 4.237 (4.2366)	GS 31.766 (31.766)	mem 45.942
Train: [0][133/750]	BT 0.163 (1.286)	DT 0.022 (1.135)	loss 9.013 (9.013)	gnorm 8607143.000 (8607143.000)	prob 4.789 (4.7887)	GS 29.391 (29.391)	mem 45.852
Train: [0][134/750]	BT 5.228 (1.315)	DT 5.091 (1.164)	loss 9.035 (9.035)	gnorm 8830706.000 (8830706.000)	prob 4.767 (4.7668)	GS 32.797 (32.797)	mem 45.835
Train: [0][135/750]	BT 0.136 (1.306)	DT 0.002 (1.156)	loss 9.368 (9.368)	gnorm 8868530.000 (8868530.000)	prob 4.377 (4.3765)	GS 31.625 (31.625)	mem 45.833
Train: [0][136/750]	BT 1.319 (1.306)	DT 1.087 (1.155)	loss 8.879 (8.879)	gnorm 8963086.000 (8963086.000)	prob 5.150 (5.1502)	GS 31.266 (31.266)	mem 45.881
Train: [0][137/750]	BT 0.132 (1.298)	DT 0.002 (1.147)	loss 9.840 (9.840)	gnorm 8459480.000 (8459480.000)	prob 3.861 (3.8607)	GS 27.141 (27.141)	mem 45.882
Train: [0][138/750]	BT 0.130 (1.289)	DT 0.003 (1.138)	loss 8.875 (8.875)	gnorm 8727011.000 (8727011.000)	prob 5.072 (5.0724)	GS 35.766 (35.766)	mem 45.883
Train: [0][139/750]	BT 0.109 (1.281)	DT 0.002 (1.130)	loss 8.278 (8.278)	gnorm 8654879.000 (8654879.000)	prob 5.627 (5.6265)	GS 33.250 (33.250)	mem 45.887
Train: [0][140/750]	BT 0.184 (1.273)	DT 0.023 (1.122)	loss 9.814 (9.814)	gnorm 9910054.000 (9910054.000)	prob 3.776 (3.7755)	GS 29.719 (29.719)	mem 45.920
Train: [0][141/750]	BT 0.154 (1.265)	DT 0.002 (1.114)	loss 9.440 (9.440)	gnorm 8515943.000 (8515943.000)	prob 4.587 (4.5869)	GS 28.969 (28.969)	mem 45.921
Train: [0][142/750]	BT 0.104 (1.257)	DT 0.008 (1.107)	loss 9.603 (9.603)	gnorm 8479633.000 (8479633.000)	prob 4.029 (4.0292)	GS 32.906 (32.906)	mem 45.923
Train: [0][143/750]	BT 0.153 (1.249)	DT 0.002 (1.099)	loss 9.604 (9.604)	gnorm 8170887.500 (8170887.500)	prob 4.511 (4.5107)	GS 33.078 (33.078)	mem 45.924
Train: [0][144/750]	BT 4.603 (1.273)	DT 4.488 (1.122)	loss 9.175 (9.175)	gnorm 8501196.000 (8501196.000)	prob 4.350 (4.3497)	GS 32.000 (32.000)	mem 45.976
Train: [0][145/750]	BT 0.183 (1.265)	DT 0.001 (1.115)	loss 9.261 (9.261)	gnorm 8667186.000 (8667186.000)	prob 4.527 (4.5269)	GS 33.156 (33.156)	mem 45.981
Train: [0][146/750]	BT 8.314 (1.313)	DT 8.227 (1.163)	loss 9.463 (9.463)	gnorm 8400964.000 (8400964.000)	prob 4.298 (4.2981)	GS 37.734 (37.734)	mem 46.149
Train: [0][147/750]	BT 0.080 (1.305)	DT 0.002 (1.155)	loss 8.718 (8.718)	gnorm 8559848.000 (8559848.000)	prob 5.152 (5.1519)	GS 34.312 (34.312)	mem 46.067
Train: [0][148/750]	BT 0.088 (1.297)	DT 0.002 (1.148)	loss 9.085 (9.085)	gnorm 7746320.000 (7746320.000)	prob 4.522 (4.5225)	GS 30.844 (30.844)	mem 46.068
Train: [0][149/750]	BT 0.083 (1.289)	DT 0.001 (1.140)	loss 8.509 (8.509)	gnorm 8169716.000 (8169716.000)	prob 5.886 (5.8863)	GS 28.844 (28.844)	mem 46.070
Train: [0][150/750]	BT 0.089 (1.281)	DT 0.002 (1.132)	loss 9.424 (9.424)	gnorm 7573412.500 (7573412.500)	prob 4.209 (4.2092)	GS 32.844 (32.844)	mem 46.071
Train: [0][151/750]	BT 0.100 (1.273)	DT 0.002 (1.125)	loss 9.393 (9.393)	gnorm 7424418.500 (7424418.500)	prob 4.279 (4.2789)	GS 29.453 (29.453)	mem 46.073
Train: [0][152/750]	BT 0.196 (1.266)	DT 0.024 (1.118)	loss 9.799 (9.799)	gnorm 8109950.500 (8109950.500)	prob 3.871 (3.8706)	GS 30.562 (30.562)	mem 46.085
Train: [0][153/750]	BT 0.202 (1.259)	DT 0.014 (1.110)	loss 9.483 (9.483)	gnorm 8223027.000 (8223027.000)	prob 5.205 (5.2050)	GS 29.812 (29.812)	mem 46.086
Train: [0][154/750]	BT 0.135 (1.251)	DT 0.008 (1.103)	loss 9.704 (9.704)	gnorm 7490211.500 (7490211.500)	prob 4.145 (4.1451)	GS 30.469 (30.469)	mem 46.129
Train: [0][155/750]	BT 0.212 (1.245)	DT 0.004 (1.096)	loss 9.185 (9.185)	gnorm 8059697.500 (8059697.500)	prob 4.388 (4.3876)	GS 35.250 (35.250)	mem 46.139
Train: [0][156/750]	BT 4.673 (1.267)	DT 4.507 (1.118)	loss 8.568 (8.568)	gnorm 8724785.000 (8724785.000)	prob 5.072 (5.0718)	GS 35.922 (35.922)	mem 46.566
Train: [0][157/750]	BT 0.186 (1.260)	DT 0.007 (1.111)	loss 9.356 (9.356)	gnorm 7181589.000 (7181589.000)	prob 4.183 (4.1832)	GS 29.844 (29.844)	mem 46.255
Train: [0][158/750]	BT 6.025 (1.290)	DT 5.908 (1.141)	loss 9.174 (9.174)	gnorm 7881566.000 (7881566.000)	prob 4.753 (4.7531)	GS 33.328 (33.328)	mem 46.392
Train: [0][159/750]	BT 0.101 (1.282)	DT 0.002 (1.134)	loss 8.991 (8.991)	gnorm 7911224.000 (7911224.000)	prob 5.437 (5.4371)	GS 31.594 (31.594)	mem 46.499
Train: [0][160/750]	BT 1.005 (1.281)	DT 0.891 (1.133)	loss 9.411 (9.411)	gnorm 7585834.000 (7585834.000)	prob 4.331 (4.3313)	GS 31.328 (31.328)	mem 46.429
Train: [0][161/750]	BT 0.173 (1.274)	DT 0.002 (1.126)	loss 9.297 (9.297)	gnorm 7452091.000 (7452091.000)	prob 4.870 (4.8704)	GS 24.844 (24.844)	mem 46.397
Train: [0][162/750]	BT 1.410 (1.275)	DT 1.314 (1.127)	loss 10.097 (10.097)	gnorm 7646820.000 (7646820.000)	prob 3.755 (3.7549)	GS 34.625 (34.625)	mem 46.433
Train: [0][163/750]	BT 0.087 (1.267)	DT 0.001 (1.120)	loss 9.016 (9.016)	gnorm 7240044.000 (7240044.000)	prob 4.604 (4.6036)	GS 32.766 (32.766)	mem 46.497
Train: [0][164/750]	BT 0.218 (1.261)	DT 0.065 (1.113)	loss 9.298 (9.298)	gnorm 7984336.500 (7984336.500)	prob 4.281 (4.2806)	GS 33.172 (33.172)	mem 46.510
Train: [0][165/750]	BT 0.153 (1.254)	DT 0.003 (1.107)	loss 10.072 (10.072)	gnorm 7600382.500 (7600382.500)	prob 3.277 (3.2774)	GS 32.547 (32.547)	mem 46.606
Train: [0][166/750]	BT 0.092 (1.247)	DT 0.002 (1.100)	loss 9.442 (9.442)	gnorm 7605991.500 (7605991.500)	prob 4.165 (4.1654)	GS 32.438 (32.438)	mem 46.655
Train: [0][167/750]	BT 0.331 (1.242)	DT 0.052 (1.094)	loss 9.072 (9.072)	gnorm 8200470.000 (8200470.000)	prob 5.049 (5.0490)	GS 36.547 (36.547)	mem 46.463
Train: [0][168/750]	BT 1.233 (1.242)	DT 1.049 (1.094)	loss 9.317 (9.317)	gnorm 7480174.500 (7480174.500)	prob 4.403 (4.4034)	GS 28.547 (28.547)	mem 46.588
Train: [0][169/750]	BT 0.186 (1.235)	DT 0.003 (1.087)	loss 9.784 (9.784)	gnorm 7188552.500 (7188552.500)	prob 4.208 (4.2077)	GS 31.422 (31.422)	mem 46.484
Train: [0][170/750]	BT 8.938 (1.281)	DT 8.821 (1.133)	loss 9.060 (9.060)	gnorm 6780055.000 (6780055.000)	prob 4.442 (4.4417)	GS 33.375 (33.375)	mem 46.634
Train: [0][171/750]	BT 0.142 (1.274)	DT 0.016 (1.126)	loss 9.339 (9.339)	gnorm 7120250.000 (7120250.000)	prob 4.379 (4.3788)	GS 29.875 (29.875)	mem 46.709
Train: [0][172/750]	BT 1.052 (1.273)	DT 0.840 (1.124)	loss 9.410 (9.410)	gnorm 6749119.000 (6749119.000)	prob 3.809 (3.8092)	GS 35.703 (35.703)	mem 47.208
Train: [0][173/750]	BT 0.109 (1.266)	DT 0.002 (1.118)	loss 9.512 (9.512)	gnorm 6927814.500 (6927814.500)	prob 4.287 (4.2874)	GS 27.391 (27.391)	mem 47.116
Train: [0][174/750]	BT 1.391 (1.267)	DT 1.206 (1.118)	loss 9.484 (9.484)	gnorm 6365465.500 (6365465.500)	prob 4.190 (4.1896)	GS 34.609 (34.609)	mem 46.739
Train: [0][175/750]	BT 0.090 (1.260)	DT 0.002 (1.112)	loss 9.942 (9.942)	gnorm 7819115.000 (7819115.000)	prob 3.464 (3.4643)	GS 37.547 (37.547)	mem 46.696
Train: [0][176/750]	BT 1.333 (1.261)	DT 1.212 (1.113)	loss 9.586 (9.586)	gnorm 6337223.000 (6337223.000)	prob 4.058 (4.0580)	GS 34.797 (34.797)	mem 45.873
Train: [0][177/750]	BT 0.077 (1.254)	DT 0.002 (1.106)	loss 9.786 (9.786)	gnorm 6835653.000 (6835653.000)	prob 3.757 (3.7565)	GS 27.578 (27.578)	mem 45.873
Train: [0][178/750]	BT 0.144 (1.248)	DT 0.002 (1.100)	loss 9.876 (9.876)	gnorm 7110617.500 (7110617.500)	prob 3.487 (3.4874)	GS 30.938 (30.938)	mem 45.875
Train: [0][179/750]	BT 0.068 (1.241)	DT 0.002 (1.094)	loss 10.027 (10.027)	gnorm 6423032.000 (6423032.000)	prob 3.432 (3.4323)	GS 28.719 (28.719)	mem 45.875
Train: [0][180/750]	BT 0.156 (1.235)	DT 0.001 (1.088)	loss 9.641 (9.641)	gnorm 6400785.500 (6400785.500)	prob 4.075 (4.0751)	GS 34.250 (34.250)	mem 45.971
Train: [0][181/750]	BT 0.159 (1.229)	DT 0.017 (1.082)	loss 9.831 (9.831)	gnorm 6339538.500 (6339538.500)	prob 4.412 (4.4123)	GS 35.828 (35.828)	mem 45.874
Train: [0][182/750]	BT 6.319 (1.257)	DT 6.232 (1.110)	loss 9.527 (9.527)	gnorm 6158865.000 (6158865.000)	prob 4.565 (4.5654)	GS 34.172 (34.172)	mem 45.822
Train: [0][183/750]	BT 0.201 (1.251)	DT 0.003 (1.104)	loss 9.358 (9.358)	gnorm 6042373.500 (6042373.500)	prob 4.763 (4.7634)	GS 28.031 (28.031)	mem 45.826
Train: [0][184/750]	BT 1.749 (1.254)	DT 1.598 (1.107)	loss 9.686 (9.686)	gnorm 6006074.000 (6006074.000)	prob 4.473 (4.4731)	GS 29.422 (29.422)	mem 44.101
Train: [0][185/750]	BT 0.184 (1.248)	DT 0.010 (1.101)	loss 9.720 (9.720)	gnorm 5969833.000 (5969833.000)	prob 3.934 (3.9336)	GS 29.906 (29.906)	mem 44.027
Train: [0][186/750]	BT 1.603 (1.250)	DT 1.478 (1.103)	loss 9.319 (9.319)	gnorm 6640161.000 (6640161.000)	prob 3.912 (3.9125)	GS 32.125 (32.125)	mem 43.951
Train: [0][187/750]	BT 0.120 (1.244)	DT 0.004 (1.097)	loss 9.447 (9.447)	gnorm 6584459.000 (6584459.000)	prob 4.400 (4.4001)	GS 31.656 (31.656)	mem 43.952
Train: [0][188/750]	BT 3.847 (1.258)	DT 3.758 (1.111)	loss 9.378 (9.378)	gnorm 5947858.500 (5947858.500)	prob 4.379 (4.3790)	GS 35.281 (35.281)	mem 43.983
Train: [0][189/750]	BT 0.120 (1.252)	DT 0.002 (1.105)	loss 9.493 (9.493)	gnorm 6244986.500 (6244986.500)	prob 4.488 (4.4879)	GS 32.469 (32.469)	mem 43.983
Train: [0][190/750]	BT 0.122 (1.246)	DT 0.010 (1.100)	loss 9.844 (9.844)	gnorm 6519638.000 (6519638.000)	prob 4.073 (4.0735)	GS 33.188 (33.188)	mem 44.004
Train: [0][191/750]	BT 0.156 (1.240)	DT 0.003 (1.094)	loss 9.304 (9.304)	gnorm 5509990.000 (5509990.000)	prob 4.812 (4.8123)	GS 30.188 (30.188)	mem 44.049
Train: [0][192/750]	BT 0.235 (1.235)	DT 0.005 (1.088)	loss 9.390 (9.390)	gnorm 6606358.000 (6606358.000)	prob 4.571 (4.5709)	GS 32.281 (32.281)	mem 43.990
Train: [0][193/750]	BT 0.169 (1.229)	DT 0.015 (1.083)	loss 9.306 (9.306)	gnorm 6179959.500 (6179959.500)	prob 4.861 (4.8613)	GS 29.188 (29.188)	mem 43.992
Train: [0][194/750]	BT 4.126 (1.244)	DT 4.012 (1.098)	loss 9.293 (9.293)	gnorm 6112519.500 (6112519.500)	prob 4.612 (4.6124)	GS 32.828 (32.828)	mem 44.185
Train: [0][195/750]	BT 0.092 (1.238)	DT 0.002 (1.092)	loss 9.248 (9.248)	gnorm 6038062.000 (6038062.000)	prob 4.360 (4.3598)	GS 34.406 (34.406)	mem 44.185
Train: [0][196/750]	BT 3.072 (1.248)	DT 2.938 (1.102)	loss 9.749 (9.749)	gnorm 6205047.000 (6205047.000)	prob 3.559 (3.5592)	GS 33.656 (33.656)	mem 43.998
Train: [0][197/750]	BT 0.086 (1.242)	DT 0.003 (1.096)	loss 9.415 (9.415)	gnorm 5878973.500 (5878973.500)	prob 4.873 (4.8728)	GS 29.828 (29.828)	mem 43.999
Train: [0][198/750]	BT 0.792 (1.240)	DT 0.709 (1.094)	loss 8.848 (8.848)	gnorm 6627097.000 (6627097.000)	prob 4.990 (4.9902)	GS 34.141 (34.141)	mem 43.931
Train: [0][199/750]	BT 0.089 (1.234)	DT 0.001 (1.089)	loss 9.549 (9.549)	gnorm 6364564.500 (6364564.500)	prob 3.662 (3.6616)	GS 35.438 (35.438)	mem 43.931
Train: [0][200/750]	BT 5.680 (1.256)	DT 5.532 (1.111)	loss 10.479 (10.479)	gnorm 6459345.500 (6459345.500)	prob 3.016 (3.0159)	GS 34.422 (34.422)	mem 44.067
Train: [0][201/750]	BT 0.222 (1.251)	DT 0.018 (1.105)	loss 9.313 (9.313)	gnorm 5826096.500 (5826096.500)	prob 5.224 (5.2244)	GS 32.062 (32.062)	mem 43.964
Train: [0][202/750]	BT 0.097 (1.245)	DT 0.016 (1.100)	loss 9.736 (9.736)	gnorm 5875841.500 (5875841.500)	prob 4.433 (4.4332)	GS 32.922 (32.922)	mem 43.966
Train: [0][203/750]	BT 0.119 (1.240)	DT 0.001 (1.095)	loss 9.090 (9.090)	gnorm 6001606.500 (6001606.500)	prob 4.868 (4.8681)	GS 36.391 (36.391)	mem 43.969
Train: [0][204/750]	BT 0.189 (1.235)	DT 0.001 (1.089)	loss 9.253 (9.253)	gnorm 6006108.500 (6006108.500)	prob 4.699 (4.6994)	GS 32.328 (32.328)	mem 43.972
Train: [0][205/750]	BT 0.133 (1.229)	DT 0.005 (1.084)	loss 8.943 (8.943)	gnorm 5862167.000 (5862167.000)	prob 5.420 (5.4204)	GS 29.609 (29.609)	mem 43.975
Train: [0][206/750]	BT 2.956 (1.238)	DT 2.859 (1.092)	loss 9.180 (9.180)	gnorm 5836071.000 (5836071.000)	prob 4.500 (4.5004)	GS 32.359 (32.359)	mem 43.991
Train: [0][207/750]	BT 0.175 (1.232)	DT 0.013 (1.087)	loss 9.728 (9.728)	gnorm 5353322.000 (5353322.000)	prob 5.058 (5.0577)	GS 31.844 (31.844)	mem 44.039
Train: [0][208/750]	BT 1.261 (1.233)	DT 1.140 (1.088)	loss 9.455 (9.455)	gnorm 5356370.500 (5356370.500)	prob 4.739 (4.7385)	GS 33.375 (33.375)	mem 44.070
Train: [0][209/750]	BT 0.094 (1.227)	DT 0.001 (1.082)	loss 9.884 (9.884)	gnorm 5981211.500 (5981211.500)	prob 4.251 (4.2511)	GS 34.531 (34.531)	mem 44.107
Train: [0][210/750]	BT 2.056 (1.231)	DT 1.905 (1.086)	loss 8.389 (8.389)	gnorm 5047790.500 (5047790.500)	prob 5.478 (5.4782)	GS 35.484 (35.484)	mem 44.100
Train: [0][211/750]	BT 0.164 (1.226)	DT 0.010 (1.081)	loss 9.616 (9.616)	gnorm 5592391.000 (5592391.000)	prob 4.291 (4.2906)	GS 32.922 (32.922)	mem 44.102
Train: [0][212/750]	BT 2.906 (1.234)	DT 2.789 (1.089)	loss 8.677 (8.677)	gnorm 4892357.000 (4892357.000)	prob 5.333 (5.3331)	GS 29.969 (29.969)	mem 44.168
Train: [0][213/750]	BT 0.145 (1.229)	DT 0.006 (1.084)	loss 9.331 (9.331)	gnorm 5497687.000 (5497687.000)	prob 4.684 (4.6838)	GS 29.516 (29.516)	mem 44.140
Train: [0][214/750]	BT 0.192 (1.224)	DT 0.006 (1.079)	loss 9.666 (9.666)	gnorm 5987653.500 (5987653.500)	prob 3.746 (3.7463)	GS 33.312 (33.312)	mem 44.288
Train: [0][215/750]	BT 0.233 (1.219)	DT 0.005 (1.074)	loss 9.430 (9.430)	gnorm 5249861.000 (5249861.000)	prob 4.454 (4.4543)	GS 29.953 (29.953)	mem 44.325
Train: [0][216/750]	BT 0.191 (1.215)	DT 0.014 (1.069)	loss 9.803 (9.803)	gnorm 5795532.500 (5795532.500)	prob 3.526 (3.5261)	GS 31.734 (31.734)	mem 44.127
Train: [0][217/750]	BT 0.278 (1.210)	DT 0.010 (1.064)	loss 9.741 (9.741)	gnorm 5148849.000 (5148849.000)	prob 4.316 (4.3161)	GS 26.234 (26.234)	mem 44.384
Train: [0][218/750]	BT 3.615 (1.221)	DT 3.297 (1.075)	loss 9.924 (9.924)	gnorm 5918377.000 (5918377.000)	prob 4.248 (4.2481)	GS 33.250 (33.250)	mem 44.226
Train: [0][219/750]	BT 0.130 (1.216)	DT 0.002 (1.070)	loss 9.730 (9.730)	gnorm 5187813.500 (5187813.500)	prob 4.068 (4.0676)	GS 40.703 (40.703)	mem 44.228
Train: [0][220/750]	BT 6.471 (1.240)	DT 6.235 (1.093)	loss 9.665 (9.665)	gnorm 5343709.500 (5343709.500)	prob 4.453 (4.4533)	GS 36.266 (36.266)	mem 44.350
Train: [0][221/750]	BT 0.226 (1.236)	DT 0.018 (1.088)	loss 10.061 (10.061)	gnorm 5773670.000 (5773670.000)	prob 3.548 (3.5483)	GS 29.844 (29.844)	mem 44.252
Train: [0][222/750]	BT 0.136 (1.231)	DT 0.012 (1.083)	loss 10.028 (10.028)	gnorm 5081822.000 (5081822.000)	prob 3.954 (3.9544)	GS 35.547 (35.547)	mem 44.266
Train: [0][223/750]	BT 0.108 (1.226)	DT 0.003 (1.079)	loss 9.422 (9.422)	gnorm 5590394.500 (5590394.500)	prob 4.841 (4.8413)	GS 31.250 (31.250)	mem 44.301
Train: [0][224/750]	BT 3.435 (1.235)	DT 3.202 (1.088)	loss 9.706 (9.706)	gnorm 5524989.500 (5524989.500)	prob 3.845 (3.8452)	GS 34.281 (34.281)	mem 44.279
Train: [0][225/750]	BT 0.127 (1.231)	DT 0.002 (1.083)	loss 10.079 (10.079)	gnorm 5060803.000 (5060803.000)	prob 3.341 (3.3412)	GS 28.375 (28.375)	mem 44.318
Train: [0][226/750]	BT 0.183 (1.226)	DT 0.002 (1.078)	loss 9.712 (9.712)	gnorm 5530745.500 (5530745.500)	prob 3.760 (3.7597)	GS 32.578 (32.578)	mem 44.286
Train: [0][227/750]	BT 0.110 (1.221)	DT 0.003 (1.074)	loss 9.421 (9.421)	gnorm 4880585.500 (4880585.500)	prob 4.330 (4.3304)	GS 32.781 (32.781)	mem 44.289
Train: [0][228/750]	BT 0.136 (1.216)	DT 0.002 (1.069)	loss 9.384 (9.384)	gnorm 5865115.000 (5865115.000)	prob 3.913 (3.9127)	GS 35.000 (35.000)	mem 44.318
Train: [0][229/750]	BT 0.105 (1.211)	DT 0.016 (1.064)	loss 9.142 (9.142)	gnorm 5489422.500 (5489422.500)	prob 5.349 (5.3490)	GS 28.078 (28.078)	mem 44.324
Train: [0][230/750]	BT 3.364 (1.221)	DT 3.202 (1.074)	loss 10.037 (10.037)	gnorm 5689001.000 (5689001.000)	prob 3.452 (3.4521)	GS 32.062 (32.062)	mem 44.353
Train: [0][231/750]	BT 0.165 (1.216)	DT 0.002 (1.069)	loss 9.890 (9.890)	gnorm 5169421.000 (5169421.000)	prob 3.752 (3.7520)	GS 32.344 (32.344)	mem 44.356
Train: [0][232/750]	BT 3.938 (1.228)	DT 3.751 (1.081)	loss 9.561 (9.561)	gnorm 5326927.000 (5326927.000)	prob 4.134 (4.1344)	GS 31.359 (31.359)	mem 44.561
Train: [0][233/750]	BT 0.182 (1.223)	DT 0.006 (1.076)	loss 9.627 (9.627)	gnorm 4796642.000 (4796642.000)	prob 4.507 (4.5071)	GS 27.828 (27.828)	mem 44.717
Train: [0][234/750]	BT 1.933 (1.226)	DT 1.841 (1.079)	loss 9.492 (9.492)	gnorm 4821885.000 (4821885.000)	prob 4.126 (4.1255)	GS 29.766 (29.766)	mem 44.637
Train: [0][235/750]	BT 0.101 (1.222)	DT 0.002 (1.075)	loss 10.031 (10.031)	gnorm 5083849.500 (5083849.500)	prob 3.856 (3.8560)	GS 31.641 (31.641)	mem 44.459
Train: [0][236/750]	BT 2.089 (1.225)	DT 1.940 (1.078)	loss 9.740 (9.740)	gnorm 4634319.500 (4634319.500)	prob 3.881 (3.8814)	GS 34.219 (34.219)	mem 44.468
Train: [0][237/750]	BT 0.196 (1.221)	DT 0.014 (1.074)	loss 9.527 (9.527)	gnorm 4773802.000 (4773802.000)	prob 4.238 (4.2379)	GS 27.109 (27.109)	mem 44.536
Train: [0][238/750]	BT 0.090 (1.216)	DT 0.004 (1.069)	loss 9.796 (9.796)	gnorm 4727631.500 (4727631.500)	prob 3.706 (3.7059)	GS 35.953 (35.953)	mem 44.552
Train: [0][239/750]	BT 0.091 (1.212)	DT 0.003 (1.065)	loss 9.432 (9.432)	gnorm 4937468.500 (4937468.500)	prob 3.954 (3.9544)	GS 34.531 (34.531)	mem 44.642
Train: [0][240/750]	BT 0.093 (1.207)	DT 0.002 (1.060)	loss 9.904 (9.904)	gnorm 4734925.500 (4734925.500)	prob 4.206 (4.2064)	GS 32.719 (32.719)	mem 44.683
Train: [0][241/750]	BT 0.176 (1.203)	DT 0.002 (1.056)	loss 9.663 (9.663)	gnorm 5278749.500 (5278749.500)	prob 4.342 (4.3418)	GS 48.812 (48.812)	mem 44.689
Train: [0][242/750]	BT 3.594 (1.213)	DT 3.437 (1.066)	loss 9.797 (9.797)	gnorm 4992422.500 (4992422.500)	prob 4.255 (4.2548)	GS 36.172 (36.172)	mem 44.646
Train: [0][243/750]	BT 0.113 (1.208)	DT 0.005 (1.062)	loss 9.433 (9.433)	gnorm 4552808.500 (4552808.500)	prob 4.609 (4.6093)	GS 34.219 (34.219)	mem 44.579
Train: [0][244/750]	BT 8.269 (1.237)	DT 8.084 (1.090)	loss 10.470 (10.470)	gnorm 5219341.500 (5219341.500)	prob 3.148 (3.1478)	GS 29.562 (29.562)	mem 44.139
Train: [0][245/750]	BT 0.219 (1.233)	DT 0.017 (1.086)	loss 9.897 (9.897)	gnorm 4696033.500 (4696033.500)	prob 4.123 (4.1227)	GS 31.750 (31.750)	mem 43.900
Train: [0][246/750]	BT 0.365 (1.229)	DT 0.281 (1.083)	loss 9.829 (9.829)	gnorm 4378120.000 (4378120.000)	prob 4.145 (4.1453)	GS 35.297 (35.297)	mem 44.035
Train: [0][247/750]	BT 0.159 (1.225)	DT 0.007 (1.078)	loss 9.877 (9.877)	gnorm 5015476.000 (5015476.000)	prob 4.747 (4.7466)	GS 28.719 (28.719)	mem 44.013
Train: [0][248/750]	BT 0.159 (1.221)	DT 0.002 (1.074)	loss 9.873 (9.873)	gnorm 5206312.000 (5206312.000)	prob 4.095 (4.0950)	GS 31.953 (31.953)	mem 43.941
Train: [0][249/750]	BT 0.149 (1.216)	DT 0.006 (1.070)	loss 9.577 (9.577)	gnorm 4503901.500 (4503901.500)	prob 4.626 (4.6263)	GS 29.391 (29.391)	mem 43.905
Train: [0][250/750]	BT 0.343 (1.213)	DT 0.141 (1.066)	loss 9.399 (9.399)	gnorm 4478248.000 (4478248.000)	prob 4.585 (4.5848)	GS 30.641 (30.641)	mem 43.921
Train: [0][251/750]	BT 0.109 (1.208)	DT 0.002 (1.062)	loss 10.279 (10.279)	gnorm 4493001.000 (4493001.000)	prob 3.105 (3.1050)	GS 28.828 (28.828)	mem 43.921
Train: [0][252/750]	BT 0.117 (1.204)	DT 0.002 (1.058)	loss 10.080 (10.080)	gnorm 4722853.000 (4722853.000)	prob 3.738 (3.7380)	GS 34.266 (34.266)	mem 43.937
Train: [0][253/750]	BT 0.150 (1.200)	DT 0.006 (1.053)	loss 9.635 (9.635)	gnorm 4372778.500 (4372778.500)	prob 3.750 (3.7501)	GS 30.812 (30.812)	mem 43.924
Train: [0][254/750]	BT 9.058 (1.231)	DT 8.845 (1.084)	loss 10.358 (10.358)	gnorm 4406232.500 (4406232.500)	prob 3.441 (3.4412)	GS 35.797 (35.797)	mem 44.073
Train: [0][255/750]	BT 0.291 (1.227)	DT 0.020 (1.080)	loss 10.208 (10.208)	gnorm 4784592.000 (4784592.000)	prob 2.947 (2.9468)	GS 28.547 (28.547)	mem 43.928
Train: [0][256/750]	BT 4.091 (1.238)	DT 3.993 (1.091)	loss 10.100 (10.100)	gnorm 4501274.000 (4501274.000)	prob 3.601 (3.6014)	GS 32.938 (32.938)	mem 43.776
Train: [0][257/750]	BT 0.119 (1.234)	DT 0.007 (1.087)	loss 9.494 (9.494)	gnorm 4441435.500 (4441435.500)	prob 4.297 (4.2970)	GS 28.375 (28.375)	mem 43.781
Train: [0][258/750]	BT 0.099 (1.230)	DT 0.001 (1.083)	loss 9.474 (9.474)	gnorm 5120869.500 (5120869.500)	prob 4.324 (4.3244)	GS 35.812 (35.812)	mem 43.860
Train: [0][259/750]	BT 0.170 (1.225)	DT 0.003 (1.079)	loss 9.284 (9.284)	gnorm 4679899.000 (4679899.000)	prob 4.874 (4.8745)	GS 34.109 (34.109)	mem 43.956
Train: [0][260/750]	BT 0.115 (1.221)	DT 0.007 (1.075)	loss 9.762 (9.762)	gnorm 4614653.000 (4614653.000)	prob 4.142 (4.1421)	GS 37.500 (37.500)	mem 43.915
Train: [0][261/750]	BT 0.090 (1.217)	DT 0.003 (1.070)	loss 9.849 (9.849)	gnorm 4237341.000 (4237341.000)	prob 4.527 (4.5268)	GS 31.531 (31.531)	mem 43.783
Train: [0][262/750]	BT 0.093 (1.213)	DT 0.003 (1.066)	loss 9.315 (9.315)	gnorm 4193467.750 (4193467.750)	prob 4.978 (4.9779)	GS 34.938 (34.938)	mem 43.782
Train: [0][263/750]	BT 0.117 (1.208)	DT 0.013 (1.062)	loss 9.459 (9.459)	gnorm 4179177.750 (4179177.750)	prob 4.854 (4.8540)	GS 35.156 (35.156)	mem 43.799
Train: [0][264/750]	BT 0.095 (1.204)	DT 0.003 (1.058)	loss 9.133 (9.133)	gnorm 4319903.500 (4319903.500)	prob 5.138 (5.1385)	GS 35.031 (35.031)	mem 43.832
Train: [0][265/750]	BT 0.163 (1.200)	DT 0.002 (1.054)	loss 9.398 (9.398)	gnorm 4555955.000 (4555955.000)	prob 4.948 (4.9476)	GS 35.156 (35.156)	mem 44.056
Train: [0][266/750]	BT 12.259 (1.242)	DT 12.126 (1.096)	loss 10.391 (10.391)	gnorm 4347085.000 (4347085.000)	prob 3.713 (3.7129)	GS 33.641 (33.641)	mem 44.114
Train: [0][267/750]	BT 0.094 (1.238)	DT 0.001 (1.092)	loss 9.209 (9.209)	gnorm 4485569.500 (4485569.500)	prob 4.264 (4.2639)	GS 28.625 (28.625)	mem 43.973
Train: [0][268/750]	BT 0.486 (1.235)	DT 0.396 (1.089)	loss 9.428 (9.428)	gnorm 4593221.500 (4593221.500)	prob 4.157 (4.1571)	GS 33.531 (33.531)	mem 43.984
Train: [0][269/750]	BT 0.096 (1.231)	DT 0.001 (1.085)	loss 9.308 (9.308)	gnorm 4280733.500 (4280733.500)	prob 5.140 (5.1398)	GS 32.188 (32.188)	mem 44.011
Train: [0][270/750]	BT 0.111 (1.226)	DT 0.006 (1.081)	loss 9.686 (9.686)	gnorm 4181185.250 (4181185.250)	prob 4.626 (4.6260)	GS 32.719 (32.719)	mem 44.089
Train: [0][271/750]	BT 0.145 (1.222)	DT 0.022 (1.077)	loss 9.642 (9.642)	gnorm 3910163.750 (3910163.750)	prob 4.319 (4.3186)	GS 28.062 (28.062)	mem 43.999
Train: [0][272/750]	BT 0.110 (1.218)	DT 0.010 (1.073)	loss 9.936 (9.936)	gnorm 4332739.500 (4332739.500)	prob 3.882 (3.8824)	GS 32.578 (32.578)	mem 43.967
Train: [0][273/750]	BT 0.084 (1.214)	DT 0.001 (1.069)	loss 10.710 (10.710)	gnorm 5050606.500 (5050606.500)	prob 2.919 (2.9192)	GS 30.766 (30.766)	mem 43.994
Train: [0][274/750]	BT 0.115 (1.210)	DT 0.013 (1.066)	loss 10.426 (10.426)	gnorm 4268973.000 (4268973.000)	prob 2.876 (2.8762)	GS 34.672 (34.672)	mem 44.011
Train: [0][275/750]	BT 0.218 (1.207)	DT 0.022 (1.062)	loss 9.839 (9.839)	gnorm 4206843.000 (4206843.000)	prob 4.159 (4.1586)	GS 28.969 (28.969)	mem 44.012
Train: [0][276/750]	BT 0.163 (1.203)	DT 0.010 (1.058)	loss 9.948 (9.948)	gnorm 4541003.000 (4541003.000)	prob 3.561 (3.5611)	GS 32.734 (32.734)	mem 44.019
Train: [0][277/750]	BT 0.138 (1.199)	DT 0.009 (1.054)	loss 9.420 (9.420)	gnorm 4283436.500 (4283436.500)	prob 4.884 (4.8840)	GS 29.531 (29.531)	mem 44.024
Train: [0][278/750]	BT 7.252 (1.221)	DT 7.017 (1.076)	loss 10.069 (10.069)	gnorm 4128153.750 (4128153.750)	prob 3.156 (3.1559)	GS 29.766 (29.766)	mem 44.219
Train: [0][279/750]	BT 0.157 (1.217)	DT 0.009 (1.072)	loss 9.650 (9.650)	gnorm 4481144.500 (4481144.500)	prob 4.745 (4.7452)	GS 43.359 (43.359)	mem 44.222
Train: [0][280/750]	BT 5.490 (1.232)	DT 5.285 (1.087)	loss 9.885 (9.885)	gnorm 4160395.500 (4160395.500)	prob 3.751 (3.7514)	GS 35.125 (35.125)	mem 44.342
Train: [0][281/750]	BT 0.173 (1.228)	DT 0.018 (1.083)	loss 9.068 (9.068)	gnorm 4392877.500 (4392877.500)	prob 5.041 (5.0408)	GS 35.156 (35.156)	mem 44.348
Train: [0][282/750]	BT 0.092 (1.224)	DT 0.002 (1.079)	loss 9.890 (9.890)	gnorm 4473326.500 (4473326.500)	prob 4.004 (4.0038)	GS 33.984 (33.984)	mem 44.380
Train: [0][283/750]	BT 0.085 (1.220)	DT 0.002 (1.075)	loss 9.732 (9.732)	gnorm 4029255.500 (4029255.500)	prob 4.167 (4.1669)	GS 29.969 (29.969)	mem 44.352
Train: [0][284/750]	BT 0.079 (1.216)	DT 0.001 (1.072)	loss 10.382 (10.382)	gnorm 4024665.500 (4024665.500)	prob 3.663 (3.6633)	GS 30.547 (30.547)	mem 44.386
Train: [0][285/750]	BT 0.101 (1.212)	DT 0.003 (1.068)	loss 10.158 (10.158)	gnorm 4316601.500 (4316601.500)	prob 4.358 (4.3577)	GS 30.016 (30.016)	mem 44.356
Train: [0][286/750]	BT 0.232 (1.209)	DT 0.129 (1.065)	loss 10.289 (10.289)	gnorm 4135724.000 (4135724.000)	prob 3.684 (3.6845)	GS 33.234 (33.234)	mem 44.361
Train: [0][287/750]	BT 0.169 (1.205)	DT 0.002 (1.061)	loss 9.913 (9.913)	gnorm 4586803.000 (4586803.000)	prob 4.770 (4.7698)	GS 32.562 (32.562)	mem 44.366
Train: [0][288/750]	BT 0.089 (1.201)	DT 0.001 (1.057)	loss 9.423 (9.423)	gnorm 4042891.250 (4042891.250)	prob 4.760 (4.7597)	GS 38.156 (38.156)	mem 44.532
Train: [0][289/750]	BT 0.126 (1.198)	DT 0.006 (1.054)	loss 9.335 (9.335)	gnorm 4024969.750 (4024969.750)	prob 4.814 (4.8145)	GS 29.516 (29.516)	mem 44.371
Train: [0][290/750]	BT 3.957 (1.207)	DT 3.781 (1.063)	loss 10.142 (10.142)	gnorm 3836855.750 (3836855.750)	prob 4.281 (4.2812)	GS 31.734 (31.734)	mem 44.453
Train: [0][291/750]	BT 0.090 (1.203)	DT 0.002 (1.059)	loss 9.563 (9.563)	gnorm 4267153.500 (4267153.500)	prob 4.880 (4.8798)	GS 34.406 (34.406)	mem 44.452
Train: [0][292/750]	BT 9.438 (1.232)	DT 9.327 (1.088)	loss 9.183 (9.183)	gnorm 4310599.000 (4310599.000)	prob 4.733 (4.7329)	GS 39.672 (39.672)	mem 44.502
Train: [0][293/750]	BT 0.107 (1.228)	DT 0.006 (1.084)	loss 9.634 (9.634)	gnorm 3813452.500 (3813452.500)	prob 4.331 (4.3315)	GS 25.844 (25.844)	mem 44.505
Train: [0][294/750]	BT 0.083 (1.224)	DT 0.001 (1.080)	loss 10.328 (10.328)	gnorm 4161907.500 (4161907.500)	prob 3.003 (3.0031)	GS 34.578 (34.578)	mem 44.575
Train: [0][295/750]	BT 0.099 (1.220)	DT 0.002 (1.077)	loss 9.183 (9.183)	gnorm 3915068.750 (3915068.750)	prob 4.455 (4.4554)	GS 33.734 (33.734)	mem 44.685
Train: [0][296/750]	BT 0.268 (1.217)	DT 0.002 (1.073)	loss 9.938 (9.938)	gnorm 4148547.500 (4148547.500)	prob 3.077 (3.0769)	GS 31.391 (31.391)	mem 44.586
Train: [0][297/750]	BT 0.125 (1.213)	DT 0.012 (1.069)	loss 9.904 (9.904)	gnorm 3769377.000 (3769377.000)	prob 3.991 (3.9913)	GS 31.406 (31.406)	mem 44.541
Train: [0][298/750]	BT 0.083 (1.209)	DT 0.002 (1.066)	loss 9.690 (9.690)	gnorm 3695564.250 (3695564.250)	prob 3.393 (3.3930)	GS 31.875 (31.875)	mem 44.588
Train: [0][299/750]	BT 0.216 (1.206)	DT 0.001 (1.062)	loss 10.500 (10.500)	gnorm 4532071.000 (4532071.000)	prob 3.252 (3.2520)	GS 31.156 (31.156)	mem 44.761
Train: [0][300/750]	BT 0.082 (1.202)	DT 0.002 (1.059)	loss 10.190 (10.190)	gnorm 4324796.500 (4324796.500)	prob 3.195 (3.1947)	GS 36.562 (36.562)	mem 44.652
Train: [0][301/750]	BT 0.125 (1.199)	DT 0.002 (1.055)	loss 9.681 (9.681)	gnorm 3890676.500 (3890676.500)	prob 3.872 (3.8723)	GS 31.219 (31.219)	mem 44.595
Train: [0][302/750]	BT 3.959 (1.208)	DT 3.723 (1.064)	loss 9.417 (9.417)	gnorm 3813206.250 (3813206.250)	prob 4.155 (4.1551)	GS 32.812 (32.812)	mem 43.788
Train: [0][303/750]	BT 0.235 (1.205)	DT 0.019 (1.061)	loss 9.933 (9.933)	gnorm 3968874.500 (3968874.500)	prob 3.833 (3.8332)	GS 32.516 (32.516)	mem 43.717
Train: [0][304/750]	BT 7.825 (1.226)	DT 7.699 (1.083)	loss 9.611 (9.611)	gnorm 4050510.250 (4050510.250)	prob 4.194 (4.1944)	GS 32.188 (32.188)	mem 43.769
Train: [0][305/750]	BT 0.140 (1.223)	DT 0.002 (1.079)	loss 10.870 (10.870)	gnorm 4474346.500 (4474346.500)	prob 3.700 (3.7001)	GS 31.266 (31.266)	mem 43.773
Train: [0][306/750]	BT 0.091 (1.219)	DT 0.003 (1.075)	loss 10.489 (10.489)	gnorm 4139703.750 (4139703.750)	prob 3.207 (3.2072)	GS 34.594 (34.594)	mem 43.773
Train: [0][307/750]	BT 0.182 (1.216)	DT 0.003 (1.072)	loss 9.865 (9.865)	gnorm 4067864.750 (4067864.750)	prob 3.693 (3.6929)	GS 29.516 (29.516)	mem 43.774
Train: [0][308/750]	BT 0.205 (1.213)	DT 0.042 (1.069)	loss 9.630 (9.630)	gnorm 3883744.500 (3883744.500)	prob 3.949 (3.9488)	GS 35.078 (35.078)	mem 43.775
Train: [0][309/750]	BT 0.115 (1.209)	DT 0.001 (1.065)	loss 9.953 (9.953)	gnorm 4201487.000 (4201487.000)	prob 3.392 (3.3920)	GS 32.656 (32.656)	mem 43.789
Train: [0][310/750]	BT 1.251 (1.209)	DT 1.148 (1.065)	loss 9.710 (9.710)	gnorm 3987205.500 (3987205.500)	prob 4.403 (4.4035)	GS 35.250 (35.250)	mem 43.819
Train: [0][311/750]	BT 0.092 (1.206)	DT 0.003 (1.062)	loss 10.336 (10.336)	gnorm 4021486.500 (4021486.500)	prob 3.126 (3.1261)	GS 30.844 (30.844)	mem 43.817
Train: [0][312/750]	BT 0.141 (1.202)	DT 0.039 (1.059)	loss 9.526 (9.526)	gnorm 4011673.250 (4011673.250)	prob 3.863 (3.8630)	GS 34.391 (34.391)	mem 43.825
Train: [0][313/750]	BT 0.089 (1.199)	DT 0.003 (1.055)	loss 9.990 (9.990)	gnorm 3746390.250 (3746390.250)	prob 4.233 (4.2333)	GS 30.000 (30.000)	mem 43.830
Train: [0][314/750]	BT 0.728 (1.197)	DT 0.478 (1.054)	loss 9.723 (9.723)	gnorm 3623655.500 (3623655.500)	prob 3.446 (3.4460)	GS 35.734 (35.734)	mem 43.839
Train: [0][315/750]	BT 0.122 (1.194)	DT 0.002 (1.050)	loss 9.971 (9.971)	gnorm 3956006.250 (3956006.250)	prob 3.969 (3.9685)	GS 31.141 (31.141)	mem 43.840
Train: [0][316/750]	BT 9.767 (1.221)	DT 9.661 (1.077)	loss 10.475 (10.475)	gnorm 4043615.250 (4043615.250)	prob 2.481 (2.4814)	GS 35.125 (35.125)	mem 43.885
Train: [0][317/750]	BT 0.177 (1.217)	DT 0.022 (1.074)	loss 8.934 (8.934)	gnorm 3914044.250 (3914044.250)	prob 5.188 (5.1877)	GS 30.484 (30.484)	mem 44.018
Train: [0][318/750]	BT 0.114 (1.214)	DT 0.012 (1.071)	loss 10.510 (10.510)	gnorm 3821828.250 (3821828.250)	prob 2.956 (2.9565)	GS 33.078 (33.078)	mem 44.153
Train: [0][319/750]	BT 0.157 (1.211)	DT 0.010 (1.067)	loss 9.493 (9.493)	gnorm 3575423.500 (3575423.500)	prob 4.731 (4.7314)	GS 30.438 (30.438)	mem 44.178
Train: [0][320/750]	BT 0.150 (1.207)	DT 0.002 (1.064)	loss 9.901 (9.901)	gnorm 3560609.250 (3560609.250)	prob 3.906 (3.9058)	GS 28.922 (28.922)	mem 43.861
Train: [0][321/750]	BT 0.112 (1.204)	DT 0.002 (1.061)	loss 9.680 (9.680)	gnorm 3932689.500 (3932689.500)	prob 4.280 (4.2803)	GS 29.984 (29.984)	mem 43.864
Train: [0][322/750]	BT 0.287 (1.201)	DT 0.109 (1.058)	loss 9.942 (9.942)	gnorm 3563488.250 (3563488.250)	prob 3.664 (3.6639)	GS 31.469 (31.469)	mem 44.008
Train: [0][323/750]	BT 0.172 (1.198)	DT 0.008 (1.055)	loss 9.684 (9.684)	gnorm 3650144.500 (3650144.500)	prob 4.527 (4.5266)	GS 27.219 (27.219)	mem 43.920
Train: [0][324/750]	BT 0.148 (1.195)	DT 0.002 (1.051)	loss 9.772 (9.772)	gnorm 3445147.250 (3445147.250)	prob 3.881 (3.8813)	GS 38.641 (38.641)	mem 43.989
Train: [0][325/750]	BT 0.130 (1.191)	DT 0.003 (1.048)	loss 9.739 (9.739)	gnorm 3508216.750 (3508216.750)	prob 3.679 (3.6785)	GS 28.297 (28.297)	mem 43.870
Train: [0][326/750]	BT 0.192 (1.188)	DT 0.007 (1.045)	loss 9.995 (9.995)	gnorm 3742942.750 (3742942.750)	prob 3.686 (3.6861)	GS 33.547 (33.547)	mem 43.973
Train: [0][327/750]	BT 0.165 (1.185)	DT 0.015 (1.042)	loss 10.620 (10.620)	gnorm 3616417.250 (3616417.250)	prob 3.112 (3.1121)	GS 29.203 (29.203)	mem 43.887
Train: [0][328/750]	BT 12.312 (1.219)	DT 12.234 (1.076)	loss 10.406 (10.406)	gnorm 3648607.000 (3648607.000)	prob 2.839 (2.8394)	GS 33.234 (33.234)	mem 44.094
Train: [0][329/750]	BT 0.086 (1.216)	DT 0.002 (1.073)	loss 10.017 (10.017)	gnorm 3775491.500 (3775491.500)	prob 3.851 (3.8506)	GS 31.172 (31.172)	mem 44.099
Train: [0][330/750]	BT 0.191 (1.213)	DT 0.003 (1.069)	loss 9.693 (9.693)	gnorm 3581204.500 (3581204.500)	prob 3.926 (3.9256)	GS 34.578 (34.578)	mem 44.103
Train: [0][331/750]	BT 0.194 (1.210)	DT 0.036 (1.066)	loss 10.010 (10.010)	gnorm 3473838.500 (3473838.500)	prob 3.849 (3.8492)	GS 32.219 (32.219)	mem 44.108
Train: [0][332/750]	BT 0.078 (1.206)	DT 0.002 (1.063)	loss 10.076 (10.076)	gnorm 3690417.500 (3690417.500)	prob 3.307 (3.3067)	GS 33.516 (33.516)	mem 44.109
Train: [0][333/750]	BT 0.094 (1.203)	DT 0.016 (1.060)	loss 9.894 (9.894)	gnorm 3416640.250 (3416640.250)	prob 3.943 (3.9428)	GS 27.562 (27.562)	mem 44.111
Train: [0][334/750]	BT 1.529 (1.204)	DT 1.425 (1.061)	loss 10.332 (10.332)	gnorm 4035397.500 (4035397.500)	prob 3.019 (3.0195)	GS 35.531 (35.531)	mem 44.217
Train: [0][335/750]	BT 0.082 (1.200)	DT 0.001 (1.058)	loss 10.272 (10.272)	gnorm 3557244.500 (3557244.500)	prob 3.141 (3.1414)	GS 30.578 (30.578)	mem 44.278
Train: [0][336/750]	BT 0.089 (1.197)	DT 0.002 (1.055)	loss 10.285 (10.285)	gnorm 4039205.500 (4039205.500)	prob 2.710 (2.7101)	GS 36.609 (36.609)	mem 44.376
Train: [0][337/750]	BT 0.151 (1.194)	DT 0.002 (1.052)	loss 9.721 (9.721)	gnorm 3446542.500 (3446542.500)	prob 4.019 (4.0193)	GS 30.922 (30.922)	mem 44.415
Train: [0][338/750]	BT 0.118 (1.191)	DT 0.015 (1.049)	loss 10.191 (10.191)	gnorm 3813714.000 (3813714.000)	prob 3.163 (3.1628)	GS 34.375 (34.375)	mem 44.218
Train: [0][339/750]	BT 0.113 (1.188)	DT 0.003 (1.045)	loss 10.166 (10.166)	gnorm 3469955.500 (3469955.500)	prob 2.783 (2.7827)	GS 29.312 (29.312)	mem 44.248
Train: [0][340/750]	BT 12.224 (1.220)	DT 12.091 (1.078)	loss 9.304 (9.304)	gnorm 3233980.500 (3233980.500)	prob 4.713 (4.7131)	GS 31.016 (31.016)	mem 44.275
Train: [0][341/750]	BT 0.083 (1.217)	DT 0.002 (1.075)	loss 10.756 (10.756)	gnorm 3412920.250 (3412920.250)	prob 2.689 (2.6891)	GS 30.250 (30.250)	mem 44.276
Train: [0][342/750]	BT 0.103 (1.213)	DT 0.001 (1.072)	loss 9.679 (9.679)	gnorm 3180265.500 (3180265.500)	prob 3.582 (3.5824)	GS 30.938 (30.938)	mem 44.278
Train: [0][343/750]	BT 0.173 (1.210)	DT 0.004 (1.069)	loss 9.786 (9.786)	gnorm 3458688.250 (3458688.250)	prob 3.840 (3.8397)	GS 30.125 (30.125)	mem 44.287
Train: [0][344/750]	BT 0.172 (1.207)	DT 0.002 (1.065)	loss 10.404 (10.404)	gnorm 3962361.000 (3962361.000)	prob 2.653 (2.6531)	GS 33.266 (33.266)	mem 44.316
Train: [0][345/750]	BT 0.177 (1.204)	DT 0.023 (1.062)	loss 10.311 (10.311)	gnorm 4144091.250 (4144091.250)	prob 2.999 (2.9992)	GS 35.453 (35.453)	mem 44.400
Train: [0][346/750]	BT 0.170 (1.201)	DT 0.020 (1.059)	loss 9.510 (9.510)	gnorm 3445796.750 (3445796.750)	prob 3.845 (3.8447)	GS 30.891 (30.891)	mem 44.336
Train: [0][347/750]	BT 0.102 (1.198)	DT 0.001 (1.056)	loss 9.944 (9.944)	gnorm 3057692.500 (3057692.500)	prob 4.169 (4.1687)	GS 26.656 (26.656)	mem 44.336
Train: [0][348/750]	BT 0.132 (1.195)	DT 0.008 (1.053)	loss 9.101 (9.101)	gnorm 3060933.500 (3060933.500)	prob 4.717 (4.7167)	GS 33.922 (33.922)	mem 44.339
Train: [0][349/750]	BT 0.158 (1.192)	DT 0.004 (1.050)	loss 9.838 (9.838)	gnorm 3254768.500 (3254768.500)	prob 3.710 (3.7095)	GS 31.406 (31.406)	mem 44.256
Train: [0][350/750]	BT 0.160 (1.189)	DT 0.023 (1.047)	loss 9.936 (9.936)	gnorm 3413620.500 (3413620.500)	prob 3.707 (3.7071)	GS 33.719 (33.719)	mem 44.260
Train: [0][351/750]	BT 0.085 (1.186)	DT 0.003 (1.044)	loss 9.940 (9.940)	gnorm 3321496.500 (3321496.500)	prob 4.274 (4.2745)	GS 32.734 (32.734)	mem 44.264
Train: [0][352/750]	BT 10.554 (1.213)	DT 10.462 (1.071)	loss 10.767 (10.767)	gnorm 3792350.000 (3792350.000)	prob 2.761 (2.7608)	GS 35.312 (35.312)	mem 44.472
Train: [0][353/750]	BT 0.085 (1.210)	DT 0.003 (1.068)	loss 9.773 (9.773)	gnorm 3435374.500 (3435374.500)	prob 4.812 (4.8118)	GS 31.516 (31.516)	mem 44.473
Train: [0][354/750]	BT 0.089 (1.206)	DT 0.003 (1.065)	loss 10.052 (10.052)	gnorm 3600904.000 (3600904.000)	prob 4.217 (4.2166)	GS 33.422 (33.422)	mem 44.478
Train: [0][355/750]	BT 0.137 (1.203)	DT 0.018 (1.062)	loss 9.768 (9.768)	gnorm 3236569.250 (3236569.250)	prob 4.420 (4.4201)	GS 33.781 (33.781)	mem 44.596
Train: [0][356/750]	BT 0.145 (1.200)	DT 0.023 (1.059)	loss 9.943 (9.943)	gnorm 3245881.750 (3245881.750)	prob 4.082 (4.0825)	GS 31.297 (31.297)	mem 44.494
Train: [0][357/750]	BT 0.137 (1.197)	DT 0.017 (1.056)	loss 10.468 (10.468)	gnorm 3321685.000 (3321685.000)	prob 3.349 (3.3487)	GS 38.906 (38.906)	mem 44.499
Train: [0][358/750]	BT 2.954 (1.202)	DT 2.858 (1.061)	loss 9.894 (9.894)	gnorm 3455313.500 (3455313.500)	prob 3.580 (3.5801)	GS 36.984 (36.984)	mem 44.637
Train: [0][359/750]	BT 0.086 (1.199)	DT 0.002 (1.058)	loss 9.909 (9.909)	gnorm 3048473.500 (3048473.500)	prob 3.908 (3.9078)	GS 29.750 (29.750)	mem 44.568
Train: [0][360/750]	BT 0.122 (1.196)	DT 0.010 (1.055)	loss 10.032 (10.032)	gnorm 3117842.500 (3117842.500)	prob 3.378 (3.3778)	GS 32.312 (32.312)	mem 44.571
Train: [0][361/750]	BT 0.139 (1.193)	DT 0.003 (1.053)	loss 9.880 (9.880)	gnorm 3020581.000 (3020581.000)	prob 4.244 (4.2442)	GS 36.062 (36.062)	mem 44.575
Train: [0][362/750]	BT 0.266 (1.191)	DT 0.153 (1.050)	loss 10.252 (10.252)	gnorm 3176604.500 (3176604.500)	prob 3.073 (3.0732)	GS 35.141 (35.141)	mem 44.580
Train: [0][363/750]	BT 0.102 (1.188)	DT 0.001 (1.047)	loss 9.908 (9.908)	gnorm 3380302.750 (3380302.750)	prob 3.704 (3.7043)	GS 31.500 (31.500)	mem 44.615
Train: [0][364/750]	BT 10.400 (1.213)	DT 10.319 (1.073)	loss 10.544 (10.544)	gnorm 3728917.000 (3728917.000)	prob 2.658 (2.6578)	GS 34.812 (34.812)	mem 43.963
Train: [0][365/750]	BT 0.149 (1.210)	DT 0.002 (1.070)	loss 10.371 (10.371)	gnorm 3344267.500 (3344267.500)	prob 4.255 (4.2548)	GS 31.234 (31.234)	mem 43.989
Train: [0][366/750]	BT 0.124 (1.207)	DT 0.002 (1.067)	loss 9.561 (9.561)	gnorm 2926442.250 (2926442.250)	prob 3.955 (3.9548)	GS 34.953 (34.953)	mem 44.029
Train: [0][367/750]	BT 0.252 (1.205)	DT 0.044 (1.064)	loss 10.689 (10.689)	gnorm 3235493.500 (3235493.500)	prob 3.082 (3.0819)	GS 32.172 (32.172)	mem 43.965
Train: [0][368/750]	BT 0.230 (1.202)	DT 0.002 (1.061)	loss 9.693 (9.693)	gnorm 3337185.000 (3337185.000)	prob 3.594 (3.5937)	GS 31.594 (31.594)	mem 43.989
Train: [0][369/750]	BT 0.225 (1.199)	DT 0.006 (1.058)	loss 10.542 (10.542)	gnorm 3418328.750 (3418328.750)	prob 3.702 (3.7017)	GS 34.562 (34.562)	mem 44.009
Train: [0][370/750]	BT 0.189 (1.197)	DT 0.030 (1.056)	loss 10.181 (10.181)	gnorm 3376822.250 (3376822.250)	prob 3.213 (3.2126)	GS 33.219 (33.219)	mem 43.968
Train: [0][371/750]	BT 0.111 (1.194)	DT 0.002 (1.053)	loss 9.693 (9.693)	gnorm 3160626.250 (3160626.250)	prob 3.950 (3.9504)	GS 30.719 (30.719)	mem 43.971
Train: [0][372/750]	BT 0.153 (1.191)	DT 0.010 (1.050)	loss 10.548 (10.548)	gnorm 3502886.750 (3502886.750)	prob 2.806 (2.8057)	GS 33.250 (33.250)	mem 44.062
Train: [0][373/750]	BT 0.234 (1.188)	DT 0.004 (1.047)	loss 10.444 (10.444)	gnorm 3072966.250 (3072966.250)	prob 2.837 (2.8375)	GS 32.125 (32.125)	mem 44.106
Train: [0][374/750]	BT 4.670 (1.198)	DT 4.453 (1.056)	loss 10.792 (10.792)	gnorm 3164078.500 (3164078.500)	prob 2.218 (2.2180)	GS 34.141 (34.141)	mem 43.935
Train: [0][375/750]	BT 0.179 (1.195)	DT 0.012 (1.053)	loss 9.858 (9.858)	gnorm 3016138.750 (3016138.750)	prob 3.208 (3.2085)	GS 33.016 (33.016)	mem 44.083
Train: [0][376/750]	BT 9.105 (1.216)	DT 8.976 (1.074)	loss 10.239 (10.239)	gnorm 3172379.250 (3172379.250)	prob 2.366 (2.3661)	GS 36.906 (36.906)	mem 44.051
Train: [0][377/750]	BT 0.090 (1.213)	DT 0.002 (1.072)	loss 10.658 (10.658)	gnorm 2868583.750 (2868583.750)	prob 3.037 (3.0368)	GS 31.234 (31.234)	mem 44.052
Train: [0][378/750]	BT 0.094 (1.210)	DT 0.002 (1.069)	loss 9.732 (9.732)	gnorm 3229516.750 (3229516.750)	prob 4.552 (4.5516)	GS 32.016 (32.016)	mem 44.053
Train: [0][379/750]	BT 0.089 (1.207)	DT 0.001 (1.066)	loss 9.972 (9.972)	gnorm 2911204.500 (2911204.500)	prob 2.955 (2.9554)	GS 30.625 (30.625)	mem 44.055
Train: [0][380/750]	BT 0.088 (1.204)	DT 0.001 (1.063)	loss 10.417 (10.417)	gnorm 3250339.000 (3250339.000)	prob 3.677 (3.6771)	GS 34.859 (34.859)	mem 44.057
Train: [0][381/750]	BT 0.084 (1.201)	DT 0.002 (1.060)	loss 10.048 (10.048)	gnorm 3032434.250 (3032434.250)	prob 3.594 (3.5939)	GS 35.047 (35.047)	mem 44.059
Train: [0][382/750]	BT 0.084 (1.198)	DT 0.002 (1.058)	loss 9.871 (9.871)	gnorm 2919552.750 (2919552.750)	prob 3.443 (3.4435)	GS 30.953 (30.953)	mem 44.060
Train: [0][383/750]	BT 0.087 (1.195)	DT 0.001 (1.055)	loss 10.501 (10.501)	gnorm 3119772.750 (3119772.750)	prob 3.142 (3.1424)	GS 30.344 (30.344)	mem 44.062
Train: [0][384/750]	BT 0.125 (1.192)	DT 0.002 (1.052)	loss 10.018 (10.018)	gnorm 3220965.750 (3220965.750)	prob 3.389 (3.3892)	GS 37.297 (37.297)	mem 44.062
Train: [0][385/750]	BT 0.262 (1.190)	DT 0.002 (1.049)	loss 10.337 (10.337)	gnorm 3024813.500 (3024813.500)	prob 3.054 (3.0543)	GS 32.562 (32.562)	mem 44.078
Train: [0][386/750]	BT 1.091 (1.190)	DT 0.975 (1.049)	loss 10.386 (10.386)	gnorm 3327252.000 (3327252.000)	prob 3.053 (3.0533)	GS 29.359 (29.359)	mem 44.085
Train: [0][387/750]	BT 0.177 (1.187)	DT 0.009 (1.046)	loss 9.578 (9.578)	gnorm 2871998.000 (2871998.000)	prob 3.580 (3.5798)	GS 30.719 (30.719)	mem 44.140
Train: [0][388/750]	BT 10.969 (1.212)	DT 10.851 (1.072)	loss 10.334 (10.334)	gnorm 3149855.750 (3149855.750)	prob 2.763 (2.7626)	GS 31.828 (31.828)	mem 44.113
Train: [0][389/750]	BT 0.085 (1.210)	DT 0.002 (1.069)	loss 9.919 (9.919)	gnorm 2882579.000 (2882579.000)	prob 3.786 (3.7865)	GS 28.016 (28.016)	mem 44.116
Train: [0][390/750]	BT 0.098 (1.207)	DT 0.001 (1.066)	loss 9.693 (9.693)	gnorm 3003148.500 (3003148.500)	prob 3.641 (3.6408)	GS 34.781 (34.781)	mem 44.129
Train: [0][391/750]	BT 0.156 (1.204)	DT 0.010 (1.064)	loss 10.362 (10.362)	gnorm 2759739.000 (2759739.000)	prob 3.321 (3.3211)	GS 27.031 (27.031)	mem 44.135
Train: [0][392/750]	BT 0.098 (1.201)	DT 0.002 (1.061)	loss 9.319 (9.319)	gnorm 3020823.000 (3020823.000)	prob 4.297 (4.2967)	GS 34.438 (34.438)	mem 44.193
Train: [0][393/750]	BT 0.099 (1.198)	DT 0.002 (1.058)	loss 10.842 (10.842)	gnorm 3238900.500 (3238900.500)	prob 2.203 (2.2028)	GS 31.375 (31.375)	mem 44.137
Train: [0][394/750]	BT 0.107 (1.196)	DT 0.001 (1.055)	loss 9.949 (9.949)	gnorm 2749966.500 (2749966.500)	prob 2.961 (2.9606)	GS 31.484 (31.484)	mem 44.138
Train: [0][395/750]	BT 0.176 (1.193)	DT 0.009 (1.053)	loss 10.453 (10.453)	gnorm 3303735.250 (3303735.250)	prob 2.999 (2.9990)	GS 35.734 (35.734)	mem 44.140
Train: [0][396/750]	BT 0.140 (1.190)	DT 0.002 (1.050)	loss 10.601 (10.601)	gnorm 3396897.750 (3396897.750)	prob 2.187 (2.1873)	GS 32.453 (32.453)	mem 44.194
Train: [0][397/750]	BT 0.152 (1.188)	DT 0.010 (1.048)	loss 10.260 (10.260)	gnorm 3287442.000 (3287442.000)	prob 2.959 (2.9592)	GS 31.125 (31.125)	mem 44.195
Train: [0][398/750]	BT 0.721 (1.187)	DT 0.592 (1.046)	loss 10.157 (10.157)	gnorm 2874478.000 (2874478.000)	prob 3.204 (3.2037)	GS 36.125 (36.125)	mem 44.163
Train: [0][399/750]	BT 0.201 (1.184)	DT 0.002 (1.044)	loss 10.269 (10.269)	gnorm 3129760.750 (3129760.750)	prob 2.977 (2.9769)	GS 25.719 (25.719)	mem 44.170
Train: [0][400/750]	BT 14.019 (1.216)	DT 13.883 (1.076)	loss 10.221 (10.221)	gnorm 2960868.750 (2960868.750)	prob 3.246 (3.2458)	GS 33.359 (33.359)	mem 44.379
Train: [0][401/750]	BT 0.085 (1.213)	DT 0.005 (1.073)	loss 10.182 (10.182)	gnorm 3024168.000 (3024168.000)	prob 2.933 (2.9330)	GS 29.812 (29.812)	mem 44.382
Train: [0][402/750]	BT 0.095 (1.211)	DT 0.001 (1.071)	loss 10.876 (10.876)	gnorm 2867697.000 (2867697.000)	prob 2.700 (2.7005)	GS 33.062 (33.062)	mem 44.384
Train: [0][403/750]	BT 0.115 (1.208)	DT 0.004 (1.068)	loss 9.956 (9.956)	gnorm 2722703.250 (2722703.250)	prob 3.268 (3.2685)	GS 29.266 (29.266)	mem 44.388
Train: [0][404/750]	BT 0.130 (1.205)	DT 0.003 (1.065)	loss 10.701 (10.701)	gnorm 3248052.500 (3248052.500)	prob 2.282 (2.2818)	GS 34.422 (34.422)	mem 44.390
Train: [0][405/750]	BT 0.135 (1.203)	DT 0.002 (1.063)	loss 9.785 (9.785)	gnorm 2842304.750 (2842304.750)	prob 3.745 (3.7452)	GS 27.703 (27.703)	mem 44.393
Train: [0][406/750]	BT 0.110 (1.200)	DT 0.002 (1.060)	loss 10.255 (10.255)	gnorm 2903569.750 (2903569.750)	prob 3.073 (3.0732)	GS 37.484 (37.484)	mem 44.404
Train: [0][407/750]	BT 0.128 (1.197)	DT 0.008 (1.057)	loss 10.748 (10.748)	gnorm 3100224.750 (3100224.750)	prob 2.318 (2.3185)	GS 33.422 (33.422)	mem 44.496
Train: [0][408/750]	BT 0.113 (1.195)	DT 0.002 (1.055)	loss 10.181 (10.181)	gnorm 2779286.750 (2779286.750)	prob 3.059 (3.0588)	GS 31.438 (31.438)	mem 44.420
Train: [0][409/750]	BT 0.071 (1.192)	DT 0.002 (1.052)	loss 9.882 (9.882)	gnorm 2924470.750 (2924470.750)	prob 3.201 (3.2012)	GS 31.250 (31.250)	mem 44.423
Train: [0][410/750]	BT 0.167 (1.189)	DT 0.002 (1.050)	loss 9.804 (9.804)	gnorm 2654732.000 (2654732.000)	prob 3.586 (3.5863)	GS 34.969 (34.969)	mem 44.459
Train: [0][411/750]	BT 0.098 (1.187)	DT 0.002 (1.047)	loss 10.641 (10.641)	gnorm 3227420.250 (3227420.250)	prob 3.080 (3.0798)	GS 31.109 (31.109)	mem 44.547
Train: [0][412/750]	BT 12.614 (1.214)	DT 12.538 (1.075)	loss 9.975 (9.975)	gnorm 3036330.000 (3036330.000)	prob 3.575 (3.5751)	GS 30.719 (30.719)	mem 44.635
Train: [0][413/750]	BT 0.100 (1.212)	DT 0.002 (1.072)	loss 9.623 (9.623)	gnorm 2812370.750 (2812370.750)	prob 4.344 (4.3438)	GS 29.141 (29.141)	mem 44.636
Train: [0][414/750]	BT 0.089 (1.209)	DT 0.009 (1.070)	loss 10.307 (10.307)	gnorm 2880985.250 (2880985.250)	prob 3.428 (3.4279)	GS 35.219 (35.219)	mem 44.638
Train: [0][415/750]	BT 0.077 (1.206)	DT 0.001 (1.067)	loss 9.546 (9.546)	gnorm 2834227.000 (2834227.000)	prob 4.525 (4.5253)	GS 31.281 (31.281)	mem 44.638
Train: [0][416/750]	BT 0.079 (1.204)	DT 0.002 (1.065)	loss 10.446 (10.446)	gnorm 2907440.750 (2907440.750)	prob 3.299 (3.2994)	GS 32.750 (32.750)	mem 44.640
Train: [0][417/750]	BT 0.131 (1.201)	DT 0.026 (1.062)	loss 10.161 (10.161)	gnorm 3075034.000 (3075034.000)	prob 3.738 (3.7381)	GS 34.156 (34.156)	mem 44.642
Train: [0][418/750]	BT 0.220 (1.199)	DT 0.003 (1.060)	loss 9.788 (9.788)	gnorm 2885026.000 (2885026.000)	prob 3.452 (3.4521)	GS 33.844 (33.844)	mem 44.649
Train: [0][419/750]	BT 0.123 (1.196)	DT 0.002 (1.057)	loss 10.017 (10.017)	gnorm 3105576.000 (3105576.000)	prob 4.399 (4.3993)	GS 29.266 (29.266)	mem 44.651
Train: [0][420/750]	BT 0.134 (1.194)	DT 0.001 (1.055)	loss 9.716 (9.716)	gnorm 2928916.000 (2928916.000)	prob 3.462 (3.4624)	GS 36.047 (36.047)	mem 44.652
Train: [0][421/750]	BT 0.231 (1.191)	DT 0.029 (1.052)	loss 9.966 (9.966)	gnorm 2912709.000 (2912709.000)	prob 3.590 (3.5897)	GS 30.844 (30.844)	mem 44.655
Train: [0][422/750]	BT 0.208 (1.189)	DT 0.007 (1.050)	loss 9.888 (9.888)	gnorm 2870511.000 (2870511.000)	prob 3.440 (3.4401)	GS 35.250 (35.250)	mem 44.655
Train: [0][423/750]	BT 0.166 (1.186)	DT 0.008 (1.047)	loss 10.633 (10.633)	gnorm 2975450.750 (2975450.750)	prob 2.182 (2.1822)	GS 34.328 (34.328)	mem 44.380
Train: [0][424/750]	BT 9.259 (1.206)	DT 9.152 (1.066)	loss 10.336 (10.336)	gnorm 2843818.000 (2843818.000)	prob 3.049 (3.0492)	GS 33.406 (33.406)	mem 43.998
Train: [0][425/750]	BT 0.090 (1.203)	DT 0.002 (1.064)	loss 10.164 (10.164)	gnorm 2939048.000 (2939048.000)	prob 3.289 (3.2895)	GS 30.844 (30.844)	mem 44.115
Train: [0][426/750]	BT 0.158 (1.200)	DT 0.002 (1.061)	loss 10.656 (10.656)	gnorm 3478082.000 (3478082.000)	prob 2.550 (2.5499)	GS 35.312 (35.312)	mem 44.090
Train: [0][427/750]	BT 0.178 (1.198)	DT 0.018 (1.059)	loss 9.932 (9.932)	gnorm 2614945.500 (2614945.500)	prob 3.446 (3.4460)	GS 35.000 (35.000)	mem 44.001
Train: [0][428/750]	BT 0.147 (1.196)	DT 0.003 (1.057)	loss 10.349 (10.349)	gnorm 2972585.000 (2972585.000)	prob 3.044 (3.0435)	GS 36.922 (36.922)	mem 44.018
Train: [0][429/750]	BT 0.089 (1.193)	DT 0.002 (1.054)	loss 10.287 (10.287)	gnorm 2970776.250 (2970776.250)	prob 2.778 (2.7775)	GS 31.438 (31.438)	mem 44.100
Train: [0][430/750]	BT 0.135 (1.191)	DT 0.002 (1.052)	loss 10.886 (10.886)	gnorm 3329392.500 (3329392.500)	prob 1.853 (1.8533)	GS 32.547 (32.547)	mem 44.140
Train: [0][431/750]	BT 0.093 (1.188)	DT 0.002 (1.049)	loss 10.222 (10.222)	gnorm 2994180.500 (2994180.500)	prob 3.131 (3.1307)	GS 30.812 (30.812)	mem 44.151
Train: [0][432/750]	BT 0.106 (1.186)	DT 0.002 (1.047)	loss 10.714 (10.714)	gnorm 3270586.000 (3270586.000)	prob 2.144 (2.1445)	GS 33.438 (33.438)	mem 44.060
Train: [0][433/750]	BT 0.164 (1.183)	DT 0.010 (1.044)	loss 9.792 (9.792)	gnorm 2453812.000 (2453812.000)	prob 4.119 (4.1191)	GS 32.672 (32.672)	mem 44.092
Train: [0][434/750]	BT 0.140 (1.181)	DT 0.037 (1.042)	loss 10.459 (10.459)	gnorm 2690920.000 (2690920.000)	prob 3.391 (3.3909)	GS 32.328 (32.328)	mem 44.282
Train: [0][435/750]	BT 0.171 (1.178)	DT 0.002 (1.040)	loss 9.215 (9.215)	gnorm 2552250.750 (2552250.750)	prob 4.708 (4.7083)	GS 34.062 (34.062)	mem 44.247
Train: [0][436/750]	BT 11.782 (1.203)	DT 11.583 (1.064)	loss 10.272 (10.272)	gnorm 2644637.000 (2644637.000)	prob 3.345 (3.3455)	GS 32.953 (32.953)	mem 44.036
Train: [0][437/750]	BT 0.091 (1.200)	DT 0.002 (1.061)	loss 10.504 (10.504)	gnorm 2917383.250 (2917383.250)	prob 3.558 (3.5584)	GS 32.578 (32.578)	mem 44.060
Train: [0][438/750]	BT 0.090 (1.198)	DT 0.002 (1.059)	loss 10.586 (10.586)	gnorm 2635069.750 (2635069.750)	prob 3.063 (3.0629)	GS 32.594 (32.594)	mem 44.039
Train: [0][439/750]	BT 0.078 (1.195)	DT 0.003 (1.057)	loss 9.414 (9.414)	gnorm 2649850.000 (2649850.000)	prob 4.952 (4.9522)	GS 33.750 (33.750)	mem 44.041
Train: [0][440/750]	BT 0.143 (1.193)	DT 0.001 (1.054)	loss 10.811 (10.811)	gnorm 2805545.250 (2805545.250)	prob 2.490 (2.4904)	GS 34.781 (34.781)	mem 44.084
Train: [0][441/750]	BT 0.091 (1.190)	DT 0.002 (1.052)	loss 10.345 (10.345)	gnorm 2803957.750 (2803957.750)	prob 3.350 (3.3496)	GS 34.734 (34.734)	mem 44.089
Train: [0][442/750]	BT 0.082 (1.188)	DT 0.001 (1.049)	loss 10.597 (10.597)	gnorm 2567695.500 (2567695.500)	prob 2.666 (2.6657)	GS 35.125 (35.125)	mem 44.099
Train: [0][443/750]	BT 0.189 (1.185)	DT 0.002 (1.047)	loss 10.734 (10.734)	gnorm 2652342.250 (2652342.250)	prob 3.178 (3.1780)	GS 29.781 (29.781)	mem 44.100
Train: [0][444/750]	BT 0.071 (1.183)	DT 0.002 (1.045)	loss 10.177 (10.177)	gnorm 3012507.250 (3012507.250)	prob 3.377 (3.3770)	GS 35.828 (35.828)	mem 44.101
Train: [0][445/750]	BT 0.147 (1.181)	DT 0.002 (1.042)	loss 9.513 (9.513)	gnorm 2736493.250 (2736493.250)	prob 4.501 (4.5013)	GS 36.766 (36.766)	mem 44.118
Train: [0][446/750]	BT 0.374 (1.179)	DT 0.275 (1.041)	loss 10.324 (10.324)	gnorm 2566614.250 (2566614.250)	prob 3.008 (3.0077)	GS 34.156 (34.156)	mem 44.196
Train: [0][447/750]	BT 0.142 (1.176)	DT 0.002 (1.038)	loss 10.337 (10.337)	gnorm 2492937.750 (2492937.750)	prob 3.648 (3.6481)	GS 32.062 (32.062)	mem 44.107
Train: [0][448/750]	BT 10.352 (1.197)	DT 10.194 (1.059)	loss 10.674 (10.674)	gnorm 2977580.000 (2977580.000)	prob 2.344 (2.3442)	GS 34.734 (34.734)	mem 44.249
Train: [0][449/750]	BT 0.172 (1.195)	DT 0.016 (1.056)	loss 9.797 (9.797)	gnorm 2549968.750 (2549968.750)	prob 3.270 (3.2699)	GS 33.266 (33.266)	mem 44.119
Train: [0][450/750]	BT 0.085 (1.192)	DT 0.001 (1.054)	loss 10.265 (10.265)	gnorm 2659447.250 (2659447.250)	prob 2.481 (2.4805)	GS 33.375 (33.375)	mem 44.122
Train: [0][451/750]	BT 0.105 (1.190)	DT 0.002 (1.052)	loss 10.457 (10.457)	gnorm 2719217.750 (2719217.750)	prob 2.641 (2.6412)	GS 31.922 (31.922)	mem 44.125
Train: [0][452/750]	BT 0.167 (1.188)	DT 0.008 (1.049)	loss 10.267 (10.267)	gnorm 2929373.750 (2929373.750)	prob 2.796 (2.7961)	GS 35.281 (35.281)	mem 44.128
Train: [0][453/750]	BT 0.110 (1.185)	DT 0.003 (1.047)	loss 10.606 (10.606)	gnorm 2748727.500 (2748727.500)	prob 2.498 (2.4983)	GS 30.188 (30.188)	mem 44.167
Train: [0][454/750]	BT 0.209 (1.183)	DT 0.024 (1.045)	loss 10.009 (10.009)	gnorm 2413350.000 (2413350.000)	prob 3.252 (3.2519)	GS 35.109 (35.109)	mem 44.301
Train: [0][455/750]	BT 0.195 (1.181)	DT 0.030 (1.043)	loss 10.448 (10.448)	gnorm 2562737.750 (2562737.750)	prob 2.544 (2.5444)	GS 25.000 (25.000)	mem 44.138
Train: [0][456/750]	BT 0.152 (1.179)	DT 0.012 (1.040)	loss 10.384 (10.384)	gnorm 2797179.250 (2797179.250)	prob 2.654 (2.6544)	GS 28.922 (28.922)	mem 44.140
Train: [0][457/750]	BT 0.218 (1.176)	DT 0.067 (1.038)	loss 10.420 (10.420)	gnorm 2670342.500 (2670342.500)	prob 3.083 (3.0825)	GS 29.141 (29.141)	mem 44.153
Train: [0][458/750]	BT 3.046 (1.181)	DT 2.855 (1.042)	loss 10.846 (10.846)	gnorm 2505176.500 (2505176.500)	prob 2.230 (2.2296)	GS 34.438 (34.438)	mem 44.190
Train: [0][459/750]	BT 0.132 (1.178)	DT 0.002 (1.040)	loss 10.166 (10.166)	gnorm 2542662.000 (2542662.000)	prob 3.416 (3.4161)	GS 31.031 (31.031)	mem 44.192
Train: [0][460/750]	BT 12.068 (1.202)	DT 11.982 (1.064)	loss 10.643 (10.643)	gnorm 2712670.250 (2712670.250)	prob 2.555 (2.5549)	GS 36.516 (36.516)	mem 44.304
Train: [0][461/750]	BT 0.106 (1.200)	DT 0.002 (1.061)	loss 9.955 (9.955)	gnorm 2324489.250 (2324489.250)	prob 3.888 (3.8884)	GS 27.250 (27.250)	mem 44.306
Train: [0][462/750]	BT 0.138 (1.197)	DT 0.005 (1.059)	loss 10.166 (10.166)	gnorm 2719761.750 (2719761.750)	prob 3.101 (3.1013)	GS 37.406 (37.406)	mem 44.308
Train: [0][463/750]	BT 0.068 (1.195)	DT 0.002 (1.057)	loss 10.605 (10.605)	gnorm 2657447.750 (2657447.750)	prob 3.545 (3.5455)	GS 30.000 (30.000)	mem 44.330
Train: [0][464/750]	BT 0.223 (1.193)	DT 0.002 (1.055)	loss 10.543 (10.543)	gnorm 2609382.250 (2609382.250)	prob 2.589 (2.5887)	GS 30.578 (30.578)	mem 44.408
Train: [0][465/750]	BT 0.166 (1.191)	DT 0.005 (1.052)	loss 10.185 (10.185)	gnorm 2513441.000 (2513441.000)	prob 3.205 (3.2045)	GS 36.391 (36.391)	mem 44.377
Train: [0][466/750]	BT 0.089 (1.188)	DT 0.003 (1.050)	loss 10.237 (10.237)	gnorm 2505490.500 (2505490.500)	prob 3.125 (3.1247)	GS 35.500 (35.500)	mem 44.379
Train: [0][467/750]	BT 0.089 (1.186)	DT 0.003 (1.048)	loss 10.757 (10.757)	gnorm 2556209.500 (2556209.500)	prob 2.713 (2.7133)	GS 31.531 (31.531)	mem 44.392
Train: [0][468/750]	BT 0.086 (1.183)	DT 0.003 (1.046)	loss 10.577 (10.577)	gnorm 2536938.250 (2536938.250)	prob 2.594 (2.5937)	GS 29.500 (29.500)	mem 44.393
Train: [0][469/750]	BT 0.097 (1.181)	DT 0.001 (1.043)	loss 10.301 (10.301)	gnorm 2390375.500 (2390375.500)	prob 3.372 (3.3719)	GS 31.078 (31.078)	mem 44.396
Train: [0][470/750]	BT 0.183 (1.179)	DT 0.016 (1.041)	loss 10.286 (10.286)	gnorm 2327030.750 (2327030.750)	prob 2.846 (2.8460)	GS 30.625 (30.625)	mem 44.399
Train: [0][471/750]	BT 0.181 (1.177)	DT 0.006 (1.039)	loss 10.521 (10.521)	gnorm 2679821.500 (2679821.500)	prob 3.236 (3.2362)	GS 33.109 (33.109)	mem 44.404
Train: [0][472/750]	BT 13.001 (1.202)	DT 12.928 (1.064)	loss 10.674 (10.674)	gnorm 2355969.250 (2355969.250)	prob 2.145 (2.1450)	GS 34.375 (34.375)	mem 44.452
Train: [0][473/750]	BT 0.129 (1.200)	DT 0.002 (1.062)	loss 9.898 (9.898)	gnorm 2293969.000 (2293969.000)	prob 4.200 (4.2005)	GS 30.797 (30.797)	mem 44.456
Train: [0][474/750]	BT 0.096 (1.197)	DT 0.002 (1.060)	loss 9.992 (9.992)	gnorm 2472241.000 (2472241.000)	prob 3.721 (3.7207)	GS 34.328 (34.328)	mem 44.459
Train: [0][475/750]	BT 0.099 (1.195)	DT 0.005 (1.058)	loss 10.075 (10.075)	gnorm 2290618.250 (2290618.250)	prob 3.564 (3.5642)	GS 38.000 (38.000)	mem 44.462
Train: [0][476/750]	BT 0.153 (1.193)	DT 0.012 (1.055)	loss 10.082 (10.082)	gnorm 2373811.750 (2373811.750)	prob 3.072 (3.0722)	GS 33.484 (33.484)	mem 44.478
Train: [0][477/750]	BT 0.122 (1.191)	DT 0.002 (1.053)	loss 10.544 (10.544)	gnorm 2512381.750 (2512381.750)	prob 3.818 (3.8176)	GS 30.266 (30.266)	mem 44.479
Train: [0][478/750]	BT 0.121 (1.188)	DT 0.003 (1.051)	loss 10.798 (10.798)	gnorm 2718596.000 (2718596.000)	prob 2.676 (2.6759)	GS 30.297 (30.297)	mem 44.531
Train: [0][479/750]	BT 0.111 (1.186)	DT 0.004 (1.049)	loss 10.367 (10.367)	gnorm 2499920.750 (2499920.750)	prob 4.044 (4.0441)	GS 27.859 (27.859)	mem 44.538
Train: [0][480/750]	BT 0.104 (1.184)	DT 0.001 (1.047)	loss 10.467 (10.467)	gnorm 2594050.500 (2594050.500)	prob 2.802 (2.8025)	GS 34.062 (34.062)	mem 44.537
Train: [0][481/750]	BT 0.284 (1.182)	DT 0.061 (1.044)	loss 10.051 (10.051)	gnorm 2463526.500 (2463526.500)	prob 3.928 (3.9281)	GS 37.562 (37.562)	mem 44.478
Train: [0][482/750]	BT 0.297 (1.180)	DT 0.084 (1.042)	loss 10.358 (10.358)	gnorm 2712710.250 (2712710.250)	prob 3.155 (3.1554)	GS 37.828 (37.828)	mem 44.479
Train: [0][483/750]	BT 0.108 (1.178)	DT 0.010 (1.040)	loss 10.138 (10.138)	gnorm 2446359.000 (2446359.000)	prob 3.435 (3.4346)	GS 34.469 (34.469)	mem 44.487
Train: [0][484/750]	BT 11.980 (1.200)	DT 11.879 (1.063)	loss 10.248 (10.248)	gnorm 2432162.500 (2432162.500)	prob 4.151 (4.1508)	GS 37.062 (37.062)	mem 43.952
Train: [0][485/750]	BT 0.097 (1.198)	DT 0.001 (1.061)	loss 9.485 (9.485)	gnorm 2549898.000 (2549898.000)	prob 4.208 (4.2076)	GS 34.078 (34.078)	mem 43.909
Train: [0][486/750]	BT 0.074 (1.196)	DT 0.004 (1.058)	loss 10.263 (10.263)	gnorm 2243515.500 (2243515.500)	prob 3.590 (3.5899)	GS 32.219 (32.219)	mem 43.942
Train: [0][487/750]	BT 0.098 (1.193)	DT 0.003 (1.056)	loss 9.853 (9.853)	gnorm 2403595.000 (2403595.000)	prob 3.282 (3.2824)	GS 32.969 (32.969)	mem 43.912
Train: [0][488/750]	BT 0.090 (1.191)	DT 0.003 (1.054)	loss 10.603 (10.603)	gnorm 2785927.000 (2785927.000)	prob 3.058 (3.0576)	GS 34.172 (34.172)	mem 43.913
Train: [0][489/750]	BT 0.120 (1.189)	DT 0.003 (1.052)	loss 10.162 (10.162)	gnorm 2425969.250 (2425969.250)	prob 3.568 (3.5684)	GS 33.656 (33.656)	mem 43.913
Train: [0][490/750]	BT 0.169 (1.187)	DT 0.004 (1.050)	loss 10.848 (10.848)	gnorm 2817178.750 (2817178.750)	prob 2.360 (2.3603)	GS 31.625 (31.625)	mem 43.913
Train: [0][491/750]	BT 0.119 (1.185)	DT 0.012 (1.048)	loss 10.287 (10.287)	gnorm 2343036.250 (2343036.250)	prob 3.482 (3.4825)	GS 26.094 (26.094)	mem 43.915
Train: [0][492/750]	BT 0.091 (1.183)	DT 0.001 (1.046)	loss 10.075 (10.075)	gnorm 2186815.000 (2186815.000)	prob 3.174 (3.1738)	GS 30.297 (30.297)	mem 43.915
Train: [0][493/750]	BT 0.096 (1.180)	DT 0.002 (1.043)	loss 10.082 (10.082)	gnorm 2569580.750 (2569580.750)	prob 3.354 (3.3544)	GS 34.078 (34.078)	mem 43.848
Train: [0][494/750]	BT 0.108 (1.178)	DT 0.012 (1.041)	loss 11.047 (11.047)	gnorm 2536946.250 (2536946.250)	prob 2.086 (2.0862)	GS 33.172 (33.172)	mem 43.850
Train: [0][495/750]	BT 0.129 (1.176)	DT 0.012 (1.039)	loss 10.604 (10.604)	gnorm 2712832.750 (2712832.750)	prob 2.747 (2.7466)	GS 28.938 (28.938)	mem 43.858
Train: [0][496/750]	BT 11.149 (1.196)	DT 11.031 (1.059)	loss 10.097 (10.097)	gnorm 2676117.250 (2676117.250)	prob 3.055 (3.0551)	GS 34.625 (34.625)	mem 44.063
Train: [0][497/750]	BT 0.085 (1.194)	DT 0.001 (1.057)	loss 10.415 (10.415)	gnorm 2479037.250 (2479037.250)	prob 3.398 (3.3982)	GS 31.906 (31.906)	mem 44.135
Train: [0][498/750]	BT 0.114 (1.192)	DT 0.002 (1.055)	loss 10.045 (10.045)	gnorm 2383169.000 (2383169.000)	prob 3.430 (3.4303)	GS 33.547 (33.547)	mem 44.043
Train: [0][499/750]	BT 0.193 (1.190)	DT 0.002 (1.053)	loss 10.429 (10.429)	gnorm 2364819.000 (2364819.000)	prob 3.202 (3.2021)	GS 40.875 (40.875)	mem 44.045
Train: [0][500/750]	BT 0.152 (1.188)	DT 0.002 (1.051)	loss 10.701 (10.701)	gnorm 2215360.750 (2215360.750)	prob 2.705 (2.7051)	GS 35.891 (35.891)	mem 44.046
Train: [0][501/750]	BT 0.175 (1.186)	DT 0.010 (1.049)	loss 9.967 (9.967)	gnorm 2134886.000 (2134886.000)	prob 3.263 (3.2629)	GS 27.047 (27.047)	mem 44.047
Train: [0][502/750]	BT 0.152 (1.184)	DT 0.002 (1.047)	loss 10.476 (10.476)	gnorm 2460199.500 (2460199.500)	prob 3.014 (3.0135)	GS 33.703 (33.703)	mem 44.046
Train: [0][503/750]	BT 0.086 (1.181)	DT 0.010 (1.045)	loss 10.732 (10.732)	gnorm 2285792.500 (2285792.500)	prob 2.461 (2.4614)	GS 28.719 (28.719)	mem 44.048
Train: [0][504/750]	BT 0.107 (1.179)	DT 0.001 (1.043)	loss 10.135 (10.135)	gnorm 2303632.500 (2303632.500)	prob 2.598 (2.5980)	GS 33.172 (33.172)	mem 44.049
Train: [0][505/750]	BT 0.107 (1.177)	DT 0.005 (1.041)	loss 10.317 (10.317)	gnorm 2421832.250 (2421832.250)	prob 2.394 (2.3942)	GS 27.938 (27.938)	mem 44.050
Train: [0][506/750]	BT 0.722 (1.176)	DT 0.554 (1.040)	loss 10.096 (10.096)	gnorm 2446881.500 (2446881.500)	prob 2.299 (2.2992)	GS 35.188 (35.188)	mem 44.058
Train: [0][507/750]	BT 0.175 (1.174)	DT 0.003 (1.038)	loss 10.231 (10.231)	gnorm 2326185.250 (2326185.250)	prob 3.623 (3.6232)	GS 29.484 (29.484)	mem 44.063
Train: [0][508/750]	BT 9.808 (1.191)	DT 9.592 (1.054)	loss 10.490 (10.490)	gnorm 2685961.250 (2685961.250)	prob 2.386 (2.3856)	GS 37.953 (37.953)	mem 44.165
Train: [0][509/750]	BT 0.131 (1.189)	DT 0.002 (1.052)	loss 10.284 (10.284)	gnorm 2234117.500 (2234117.500)	prob 3.160 (3.1605)	GS 30.016 (30.016)	mem 44.166
Train: [0][510/750]	BT 0.096 (1.187)	DT 0.002 (1.050)	loss 10.388 (10.388)	gnorm 2403774.500 (2403774.500)	prob 2.836 (2.8365)	GS 31.328 (31.328)	mem 44.089
Train: [0][511/750]	BT 0.090 (1.185)	DT 0.002 (1.048)	loss 10.195 (10.195)	gnorm 2391889.500 (2391889.500)	prob 3.431 (3.4315)	GS 29.969 (29.969)	mem 44.091
Train: [0][512/750]	BT 0.097 (1.183)	DT 0.001 (1.046)	loss 10.527 (10.527)	gnorm 2369939.500 (2369939.500)	prob 2.202 (2.2021)	GS 31.672 (31.672)	mem 44.089
Train: [0][513/750]	BT 0.107 (1.181)	DT 0.008 (1.044)	loss 10.455 (10.455)	gnorm 2329556.750 (2329556.750)	prob 2.664 (2.6637)	GS 32.031 (32.031)	mem 44.024
Train: [0][514/750]	BT 0.322 (1.179)	DT 0.139 (1.042)	loss 10.076 (10.076)	gnorm 2352853.250 (2352853.250)	prob 3.191 (3.1905)	GS 34.188 (34.188)	mem 43.994
Train: [0][515/750]	BT 0.103 (1.177)	DT 0.002 (1.040)	loss 10.567 (10.567)	gnorm 2611296.250 (2611296.250)	prob 2.378 (2.3783)	GS 32.750 (32.750)	mem 43.971
Train: [0][516/750]	BT 0.202 (1.175)	DT 0.005 (1.038)	loss 10.263 (10.263)	gnorm 2359434.000 (2359434.000)	prob 2.115 (2.1147)	GS 32.609 (32.609)	mem 44.097
Train: [0][517/750]	BT 0.119 (1.173)	DT 0.013 (1.036)	loss 10.625 (10.625)	gnorm 2261879.000 (2261879.000)	prob 1.547 (1.5472)	GS 35.031 (35.031)	mem 44.107
Train: [0][518/750]	BT 8.727 (1.188)	DT 8.589 (1.051)	loss 10.803 (10.803)	gnorm 2570218.250 (2570218.250)	prob 1.117 (1.1167)	GS 37.125 (37.125)	mem 44.167
Train: [0][519/750]	BT 0.155 (1.186)	DT 0.002 (1.049)	loss 10.682 (10.682)	gnorm 2404563.500 (2404563.500)	prob 1.633 (1.6326)	GS 30.547 (30.547)	mem 44.104
Train: [0][520/750]	BT 4.058 (1.191)	DT 3.855 (1.054)	loss 9.782 (9.782)	gnorm 2154388.750 (2154388.750)	prob 2.020 (2.0202)	GS 33.828 (33.828)	mem 44.131
Train: [0][521/750]	BT 0.146 (1.189)	DT 0.010 (1.052)	loss 10.434 (10.434)	gnorm 2351833.500 (2351833.500)	prob 2.235 (2.2351)	GS 34.734 (34.734)	mem 44.057
Train: [0][522/750]	BT 0.124 (1.187)	DT 0.004 (1.050)	loss 10.449 (10.449)	gnorm 2375288.000 (2375288.000)	prob 1.187 (1.1867)	GS 36.344 (36.344)	mem 43.993
Train: [0][523/750]	BT 0.121 (1.185)	DT 0.003 (1.048)	loss 10.478 (10.478)	gnorm 2229703.250 (2229703.250)	prob 0.981 (0.9811)	GS 33.406 (33.406)	mem 44.027
Train: [0][524/750]	BT 0.089 (1.183)	DT 0.002 (1.046)	loss 10.213 (10.213)	gnorm 2612814.000 (2612814.000)	prob 1.732 (1.7323)	GS 32.562 (32.562)	mem 44.047
Train: [0][525/750]	BT 0.093 (1.181)	DT 0.003 (1.044)	loss 10.126 (10.126)	gnorm 2416272.000 (2416272.000)	prob 1.719 (1.7188)	GS 31.828 (31.828)	mem 44.092
Train: [0][526/750]	BT 0.259 (1.179)	DT 0.002 (1.042)	loss 10.020 (10.020)	gnorm 2291637.500 (2291637.500)	prob 1.759 (1.7593)	GS 35.422 (35.422)	mem 44.027
Train: [0][527/750]	BT 0.110 (1.177)	DT 0.009 (1.040)	loss 10.320 (10.320)	gnorm 2211204.250 (2211204.250)	prob 1.424 (1.4237)	GS 32.594 (32.594)	mem 44.027
Train: [0][528/750]	BT 0.170 (1.175)	DT 0.015 (1.038)	loss 11.004 (11.004)	gnorm 2460220.500 (2460220.500)	prob 0.449 (0.4488)	GS 31.781 (31.781)	mem 44.027
Train: [0][529/750]	BT 0.160 (1.173)	DT 0.002 (1.037)	loss 10.229 (10.229)	gnorm 2267948.750 (2267948.750)	prob 2.064 (2.0644)	GS 29.047 (29.047)	mem 44.028
Train: [0][530/750]	BT 9.646 (1.189)	DT 9.564 (1.053)	loss 10.174 (10.174)	gnorm 2455753.750 (2455753.750)	prob 1.367 (1.3671)	GS 30.953 (30.953)	mem 44.108
Train: [0][531/750]	BT 0.213 (1.187)	DT 0.002 (1.051)	loss 10.950 (10.950)	gnorm 2270210.750 (2270210.750)	prob 0.807 (0.8070)	GS 31.844 (31.844)	mem 44.108
Train: [0][532/750]	BT 2.989 (1.191)	DT 2.849 (1.054)	loss 10.323 (10.323)	gnorm 2321097.000 (2321097.000)	prob 1.071 (1.0710)	GS 35.828 (35.828)	mem 44.045
Train: [0][533/750]	BT 0.073 (1.189)	DT 0.001 (1.052)	loss 10.304 (10.304)	gnorm 2578903.500 (2578903.500)	prob 1.561 (1.5611)	GS 32.453 (32.453)	mem 44.049
Train: [0][534/750]	BT 0.156 (1.187)	DT 0.002 (1.050)	loss 10.888 (10.888)	gnorm 2287287.000 (2287287.000)	prob 0.278 (0.2776)	GS 36.812 (36.812)	mem 44.121
Train: [0][535/750]	BT 0.261 (1.185)	DT 0.001 (1.048)	loss 10.535 (10.535)	gnorm 2059347.125 (2059347.125)	prob 1.096 (1.0963)	GS 31.000 (31.000)	mem 44.271
Train: [0][536/750]	BT 0.219 (1.183)	DT 0.020 (1.046)	loss 9.587 (9.587)	gnorm 2088039.375 (2088039.375)	prob 1.869 (1.8689)	GS 33.844 (33.844)	mem 44.047
Train: [0][537/750]	BT 0.117 (1.181)	DT 0.002 (1.044)	loss 10.227 (10.227)	gnorm 2326705.500 (2326705.500)	prob 1.786 (1.7855)	GS 29.656 (29.656)	mem 44.046
Train: [0][538/750]	BT 0.135 (1.179)	DT 0.012 (1.042)	loss 10.804 (10.804)	gnorm 2361503.250 (2361503.250)	prob 0.489 (0.4890)	GS 32.359 (32.359)	mem 44.074
Train: [0][539/750]	BT 0.236 (1.177)	DT 0.026 (1.040)	loss 10.657 (10.657)	gnorm 2513307.750 (2513307.750)	prob 1.997 (1.9973)	GS 33.172 (33.172)	mem 44.097
Train: [0][540/750]	BT 0.100 (1.176)	DT 0.002 (1.039)	loss 10.690 (10.690)	gnorm 2505974.750 (2505974.750)	prob 0.990 (0.9897)	GS 34.281 (34.281)	mem 44.046
Train: [0][541/750]	BT 0.122 (1.174)	DT 0.002 (1.037)	loss 10.864 (10.864)	gnorm 2219969.000 (2219969.000)	prob 0.968 (0.9683)	GS 34.969 (34.969)	mem 44.046
Train: [0][542/750]	BT 8.292 (1.187)	DT 8.071 (1.050)	loss 10.775 (10.775)	gnorm 2561895.000 (2561895.000)	prob 0.605 (0.6047)	GS 30.797 (30.797)	mem 44.016
Train: [0][543/750]	BT 0.178 (1.185)	DT 0.005 (1.048)	loss 10.601 (10.601)	gnorm 2433126.000 (2433126.000)	prob 1.693 (1.6928)	GS 34.531 (34.531)	mem 44.072
Train: [0][544/750]	BT 4.344 (1.191)	DT 4.218 (1.053)	loss 10.307 (10.307)	gnorm 2473409.750 (2473409.750)	prob 1.212 (1.2117)	GS 36.750 (36.750)	mem 43.973
Train: [0][545/750]	BT 0.166 (1.189)	DT 0.003 (1.052)	loss 10.705 (10.705)	gnorm 2319217.500 (2319217.500)	prob 0.334 (0.3336)	GS 32.234 (32.234)	mem 43.977
Train: [0][546/750]	BT 0.100 (1.187)	DT 0.004 (1.050)	loss 10.058 (10.058)	gnorm 2079981.000 (2079981.000)	prob 1.979 (1.9791)	GS 31.109 (31.109)	mem 43.977
Train: [0][547/750]	BT 0.105 (1.185)	DT 0.002 (1.048)	loss 10.279 (10.279)	gnorm 2348790.500 (2348790.500)	prob 1.296 (1.2964)	GS 29.484 (29.484)	mem 43.977
Train: [0][548/750]	BT 0.126 (1.183)	DT 0.002 (1.046)	loss 10.839 (10.839)	gnorm 2562550.000 (2562550.000)	prob 0.789 (0.7892)	GS 35.344 (35.344)	mem 43.977
Train: [0][549/750]	BT 0.169 (1.181)	DT 0.038 (1.044)	loss 10.997 (10.997)	gnorm 2144046.250 (2144046.250)	prob 1.040 (1.0403)	GS 28.828 (28.828)	mem 44.046
Train: [0][550/750]	BT 0.146 (1.179)	DT 0.001 (1.042)	loss 9.976 (9.976)	gnorm 2365921.500 (2365921.500)	prob 1.369 (1.3693)	GS 33.766 (33.766)	mem 43.977
Train: [0][551/750]	BT 0.101 (1.177)	DT 0.002 (1.040)	loss 10.206 (10.206)	gnorm 2036016.500 (2036016.500)	prob 1.286 (1.2858)	GS 29.484 (29.484)	mem 44.090
Train: [0][552/750]	BT 0.276 (1.176)	DT 0.014 (1.038)	loss 10.628 (10.628)	gnorm 1986982.500 (1986982.500)	prob 0.309 (0.3088)	GS 31.797 (31.797)	mem 44.052
Train: [0][553/750]	BT 0.122 (1.174)	DT 0.002 (1.036)	loss 11.008 (11.008)	gnorm 2260426.250 (2260426.250)	prob 0.380 (0.3804)	GS 33.938 (33.938)	mem 43.989
Train: [0][554/750]	BT 10.636 (1.191)	DT 10.493 (1.054)	loss 11.091 (11.091)	gnorm 2277228.000 (2277228.000)	prob -0.661 (-0.6614)	GS 35.906 (35.906)	mem 44.044
Train: [0][555/750]	BT 0.105 (1.189)	DT 0.002 (1.052)	loss 10.761 (10.761)	gnorm 2218244.750 (2218244.750)	prob -0.199 (-0.1989)	GS 29.609 (29.609)	mem 44.044
Train: [0][556/750]	BT 1.520 (1.189)	DT 1.430 (1.052)	loss 10.536 (10.536)	gnorm 2307227.250 (2307227.250)	prob -0.466 (-0.4662)	GS 36.797 (36.797)	mem 44.121
Train: [0][557/750]	BT 0.099 (1.187)	DT 0.003 (1.050)	loss 10.578 (10.578)	gnorm 2454423.750 (2454423.750)	prob -0.066 (-0.0665)	GS 28.875 (28.875)	mem 44.001
Train: [0][558/750]	BT 0.170 (1.186)	DT 0.003 (1.049)	loss 9.480 (9.480)	gnorm 2243319.500 (2243319.500)	prob 0.620 (0.6203)	GS 32.094 (32.094)	mem 44.003
Train: [0][559/750]	BT 0.107 (1.184)	DT 0.002 (1.047)	loss 10.442 (10.442)	gnorm 2109176.500 (2109176.500)	prob 0.697 (0.6970)	GS 33.922 (33.922)	mem 44.098
Train: [0][560/750]	BT 0.147 (1.182)	DT 0.026 (1.045)	loss 10.717 (10.717)	gnorm 2111231.500 (2111231.500)	prob 0.419 (0.4187)	GS 34.094 (34.094)	mem 44.077
Train: [0][561/750]	BT 0.114 (1.180)	DT 0.012 (1.043)	loss 10.330 (10.330)	gnorm 2143769.750 (2143769.750)	prob 1.372 (1.3715)	GS 30.297 (30.297)	mem 44.021
Train: [0][562/750]	BT 0.086 (1.178)	DT 0.002 (1.041)	loss 10.086 (10.086)	gnorm 2128061.250 (2128061.250)	prob 1.455 (1.4553)	GS 35.641 (35.641)	mem 44.070
Train: [0][563/750]	BT 0.122 (1.176)	DT 0.002 (1.039)	loss 10.377 (10.377)	gnorm 2155666.000 (2155666.000)	prob 1.469 (1.4691)	GS 31.016 (31.016)	mem 44.203
Train: [0][564/750]	BT 0.160 (1.174)	DT 0.003 (1.037)	loss 10.340 (10.340)	gnorm 2072098.125 (2072098.125)	prob 1.589 (1.5888)	GS 34.578 (34.578)	mem 44.325
Train: [0][565/750]	BT 0.156 (1.172)	DT 0.027 (1.036)	loss 9.339 (9.339)	gnorm 2173428.500 (2173428.500)	prob 2.702 (2.7022)	GS 33.406 (33.406)	mem 44.024
Train: [0][566/750]	BT 8.375 (1.185)	DT 8.224 (1.048)	loss 9.684 (9.684)	gnorm 1854716.250 (1854716.250)	prob 2.729 (2.7290)	GS 33.609 (33.609)	mem 43.967
Train: [0][567/750]	BT 0.086 (1.183)	DT 0.003 (1.047)	loss 10.218 (10.218)	gnorm 1976544.125 (1976544.125)	prob 2.624 (2.6238)	GS 29.875 (29.875)	mem 43.966
Train: [0][568/750]	BT 2.768 (1.186)	DT 2.642 (1.049)	loss 10.171 (10.171)	gnorm 2105785.500 (2105785.500)	prob 2.737 (2.7367)	GS 38.109 (38.109)	mem 43.874
Train: [0][569/750]	BT 0.151 (1.184)	DT 0.013 (1.048)	loss 10.668 (10.668)	gnorm 1975868.750 (1975868.750)	prob 1.695 (1.6952)	GS 35.875 (35.875)	mem 43.878
Train: [0][570/750]	BT 0.130 (1.182)	DT 0.012 (1.046)	loss 10.893 (10.893)	gnorm 2175070.000 (2175070.000)	prob 2.193 (2.1932)	GS 35.656 (35.656)	mem 43.878
Train: [0][571/750]	BT 0.092 (1.180)	DT 0.003 (1.044)	loss 10.547 (10.547)	gnorm 2125479.500 (2125479.500)	prob 2.989 (2.9895)	GS 27.984 (27.984)	mem 43.894
Train: [0][572/750]	BT 0.107 (1.179)	DT 0.001 (1.042)	loss 10.225 (10.225)	gnorm 2235639.000 (2235639.000)	prob 2.456 (2.4563)	GS 33.266 (33.266)	mem 43.919
Train: [0][573/750]	BT 0.076 (1.177)	DT 0.002 (1.040)	loss 10.067 (10.067)	gnorm 1967035.125 (1967035.125)	prob 2.992 (2.9921)	GS 33.422 (33.422)	mem 43.951
Train: [0][574/750]	BT 0.125 (1.175)	DT 0.002 (1.038)	loss 11.238 (11.238)	gnorm 2337333.500 (2337333.500)	prob 1.511 (1.5105)	GS 34.828 (34.828)	mem 43.890
Train: [0][575/750]	BT 0.096 (1.173)	DT 0.001 (1.037)	loss 10.284 (10.284)	gnorm 1953792.375 (1953792.375)	prob 2.256 (2.2563)	GS 27.484 (27.484)	mem 43.891
Train: [0][576/750]	BT 0.140 (1.171)	DT 0.001 (1.035)	loss 10.099 (10.099)	gnorm 1943683.625 (1943683.625)	prob 2.628 (2.6278)	GS 32.250 (32.250)	mem 44.089
Train: [0][577/750]	BT 0.223 (1.170)	DT 0.028 (1.033)	loss 10.807 (10.807)	gnorm 2221184.000 (2221184.000)	prob 2.273 (2.2733)	GS 31.703 (31.703)	mem 43.971
Train: [0][578/750]	BT 10.096 (1.185)	DT 9.864 (1.048)	loss 10.571 (10.571)	gnorm 1921124.375 (1921124.375)	prob 2.048 (2.0480)	GS 31.906 (31.906)	mem 43.774
Train: [0][579/750]	BT 0.284 (1.183)	DT 0.003 (1.047)	loss 10.205 (10.205)	gnorm 2077430.750 (2077430.750)	prob 4.019 (4.0185)	GS 30.531 (30.531)	mem 43.685
Train: [0][580/750]	BT 2.335 (1.185)	DT 2.215 (1.049)	loss 10.784 (10.784)	gnorm 2251339.750 (2251339.750)	prob 2.190 (2.1897)	GS 32.328 (32.328)	mem 43.571
Train: [0][581/750]	BT 0.144 (1.184)	DT 0.002 (1.047)	loss 10.234 (10.234)	gnorm 2008130.500 (2008130.500)	prob 2.443 (2.4427)	GS 31.750 (31.750)	mem 43.571
Train: [0][582/750]	BT 0.147 (1.182)	DT 0.005 (1.045)	loss 10.429 (10.429)	gnorm 2046157.875 (2046157.875)	prob 2.039 (2.0392)	GS 28.781 (28.781)	mem 43.589
Train: [0][583/750]	BT 0.092 (1.180)	DT 0.010 (1.043)	loss 10.233 (10.233)	gnorm 1952732.875 (1952732.875)	prob 1.690 (1.6897)	GS 33.359 (33.359)	mem 43.590
Train: [0][584/750]	BT 0.126 (1.178)	DT 0.001 (1.041)	loss 10.542 (10.542)	gnorm 2045275.750 (2045275.750)	prob 1.004 (1.0036)	GS 31.188 (31.188)	mem 43.589
Train: [0][585/750]	BT 0.089 (1.176)	DT 0.004 (1.040)	loss 10.438 (10.438)	gnorm 2169844.750 (2169844.750)	prob 1.690 (1.6904)	GS 29.594 (29.594)	mem 43.590
Train: [0][586/750]	BT 0.088 (1.174)	DT 0.003 (1.038)	loss 10.073 (10.073)	gnorm 1805285.375 (1805285.375)	prob 1.527 (1.5273)	GS 34.172 (34.172)	mem 43.590
Train: [0][587/750]	BT 0.105 (1.173)	DT 0.015 (1.036)	loss 11.241 (11.241)	gnorm 2080766.250 (2080766.250)	prob 1.610 (1.6098)	GS 25.328 (25.328)	mem 43.597
Train: [0][588/750]	BT 0.106 (1.171)	DT 0.003 (1.034)	loss 10.245 (10.245)	gnorm 2043435.250 (2043435.250)	prob 1.644 (1.6436)	GS 31.422 (31.422)	mem 43.659
Train: [0][589/750]	BT 0.159 (1.169)	DT 0.002 (1.033)	loss 9.928 (9.928)	gnorm 1800258.500 (1800258.500)	prob 2.176 (2.1763)	GS 31.344 (31.344)	mem 43.681
Train: [0][590/750]	BT 11.187 (1.186)	DT 11.010 (1.050)	loss 10.179 (10.179)	gnorm 1910603.500 (1910603.500)	prob 1.324 (1.3244)	GS 38.109 (38.109)	mem 43.671
Train: [0][591/750]	BT 0.227 (1.184)	DT 0.010 (1.048)	loss 10.267 (10.267)	gnorm 2093107.500 (2093107.500)	prob 1.557 (1.5568)	GS 33.406 (33.406)	mem 43.707
Train: [0][592/750]	BT 3.996 (1.189)	DT 3.922 (1.053)	loss 10.395 (10.395)	gnorm 2096951.750 (2096951.750)	prob 1.502 (1.5019)	GS 35.578 (35.578)	mem 43.617
Train: [0][593/750]	BT 0.128 (1.187)	DT 0.002 (1.051)	loss 10.585 (10.585)	gnorm 2025646.250 (2025646.250)	prob 1.143 (1.1430)	GS 31.688 (31.688)	mem 43.666
Train: [0][594/750]	BT 0.089 (1.186)	DT 0.002 (1.049)	loss 10.252 (10.252)	gnorm 1963841.500 (1963841.500)	prob 1.150 (1.1498)	GS 33.109 (33.109)	mem 43.593
Train: [0][595/750]	BT 0.226 (1.184)	DT 0.001 (1.047)	loss 10.330 (10.330)	gnorm 1958516.750 (1958516.750)	prob 0.654 (0.6541)	GS 31.234 (31.234)	mem 43.594
Train: [0][596/750]	BT 0.121 (1.182)	DT 0.002 (1.046)	loss 10.499 (10.499)	gnorm 1945682.875 (1945682.875)	prob 0.772 (0.7715)	GS 34.406 (34.406)	mem 43.594
Train: [0][597/750]	BT 0.128 (1.180)	DT 0.006 (1.044)	loss 10.069 (10.069)	gnorm 2252217.500 (2252217.500)	prob 0.930 (0.9296)	GS 33.297 (33.297)	mem 43.646
Train: [0][598/750]	BT 0.158 (1.179)	DT 0.004 (1.042)	loss 10.039 (10.039)	gnorm 1875770.375 (1875770.375)	prob 0.714 (0.7140)	GS 34.969 (34.969)	mem 43.666
Train: [0][599/750]	BT 0.168 (1.177)	DT 0.005 (1.040)	loss 10.162 (10.162)	gnorm 1945268.000 (1945268.000)	prob 0.716 (0.7155)	GS 31.812 (31.812)	mem 43.766
Train: [0][600/750]	BT 0.220 (1.175)	DT 0.006 (1.039)	loss 10.037 (10.037)	gnorm 2088439.750 (2088439.750)	prob 0.981 (0.9805)	GS 33.219 (33.219)	mem 43.606
Train: [0][601/750]	BT 0.096 (1.174)	DT 0.007 (1.037)	loss 10.375 (10.375)	gnorm 1829397.250 (1829397.250)	prob 0.702 (0.7025)	GS 30.125 (30.125)	mem 43.607
Train: [0][602/750]	BT 3.135 (1.177)	DT 2.998 (1.040)	loss 10.065 (10.065)	gnorm 2101088.000 (2101088.000)	prob 1.310 (1.3097)	GS 34.672 (34.672)	mem 43.645
Train: [0][603/750]	BT 0.100 (1.175)	DT 0.003 (1.038)	loss 10.050 (10.050)	gnorm 1859117.250 (1859117.250)	prob 1.437 (1.4366)	GS 32.719 (32.719)	mem 43.611
Train: [0][604/750]	BT 9.131 (1.188)	DT 9.041 (1.052)	loss 10.354 (10.354)	gnorm 2069691.875 (2069691.875)	prob 0.988 (0.9882)	GS 32.938 (32.938)	mem 43.505
Train: [0][605/750]	BT 0.076 (1.186)	DT 0.002 (1.050)	loss 10.313 (10.313)	gnorm 1969703.000 (1969703.000)	prob 0.121 (0.1213)	GS 31.641 (31.641)	mem 43.505
Train: [0][606/750]	BT 0.197 (1.185)	DT 0.001 (1.048)	loss 9.939 (9.939)	gnorm 2075185.750 (2075185.750)	prob 0.885 (0.8851)	GS 33.703 (33.703)	mem 43.511
Train: [0][607/750]	BT 0.160 (1.183)	DT 0.003 (1.047)	loss 10.142 (10.142)	gnorm 1881010.250 (1881010.250)	prob 1.168 (1.1680)	GS 32.375 (32.375)	mem 43.522
Train: [0][608/750]	BT 0.212 (1.181)	DT 0.002 (1.045)	loss 10.568 (10.568)	gnorm 1796263.500 (1796263.500)	prob 1.221 (1.2213)	GS 35.047 (35.047)	mem 43.523
Train: [0][609/750]	BT 0.105 (1.180)	DT 0.002 (1.043)	loss 10.322 (10.322)	gnorm 1929371.625 (1929371.625)	prob 1.940 (1.9396)	GS 30.828 (30.828)	mem 43.535
Train: [0][610/750]	BT 0.133 (1.178)	DT 0.008 (1.041)	loss 9.995 (9.995)	gnorm 1948001.000 (1948001.000)	prob 1.985 (1.9846)	GS 32.156 (32.156)	mem 43.537
Train: [0][611/750]	BT 0.078 (1.176)	DT 0.003 (1.040)	loss 9.972 (9.972)	gnorm 1858224.125 (1858224.125)	prob 1.229 (1.2290)	GS 32.281 (32.281)	mem 43.537
Train: [0][612/750]	BT 0.089 (1.174)	DT 0.003 (1.038)	loss 9.827 (9.827)	gnorm 1791134.625 (1791134.625)	prob 1.586 (1.5860)	GS 34.547 (34.547)	mem 43.537
Train: [0][613/750]	BT 0.101 (1.173)	DT 0.001 (1.036)	loss 10.699 (10.699)	gnorm 1951265.250 (1951265.250)	prob 0.762 (0.7615)	GS 29.391 (29.391)	mem 43.580
Train: [0][614/750]	BT 0.565 (1.172)	DT 0.383 (1.035)	loss 9.871 (9.871)	gnorm 1810140.875 (1810140.875)	prob 2.026 (2.0265)	GS 34.281 (34.281)	mem 43.673
Train: [0][615/750]	BT 0.204 (1.170)	DT 0.002 (1.034)	loss 10.071 (10.071)	gnorm 1847864.125 (1847864.125)	prob 2.143 (2.1430)	GS 31.578 (31.578)	mem 43.547
Train: [0][616/750]	BT 11.722 (1.187)	DT 11.622 (1.051)	loss 11.002 (11.002)	gnorm 2050867.875 (2050867.875)	prob 1.092 (1.0924)	GS 31.969 (31.969)	mem 43.565
Train: [0][617/750]	BT 0.143 (1.186)	DT 0.002 (1.049)	loss 9.938 (9.938)	gnorm 1909238.000 (1909238.000)	prob 2.041 (2.0408)	GS 29.031 (29.031)	mem 43.611
Train: [0][618/750]	BT 0.132 (1.184)	DT 0.002 (1.047)	loss 10.497 (10.497)	gnorm 1776248.125 (1776248.125)	prob 0.987 (0.9874)	GS 36.719 (36.719)	mem 43.758
Train: [0][619/750]	BT 0.153 (1.182)	DT 0.016 (1.046)	loss 10.244 (10.244)	gnorm 1851778.750 (1851778.750)	prob 2.080 (2.0803)	GS 31.797 (31.797)	mem 43.805
Train: [0][620/750]	BT 0.085 (1.180)	DT 0.002 (1.044)	loss 10.705 (10.705)	gnorm 2254488.250 (2254488.250)	prob 0.987 (0.9869)	GS 36.875 (36.875)	mem 43.612
Train: [0][621/750]	BT 0.090 (1.179)	DT 0.002 (1.042)	loss 10.257 (10.257)	gnorm 1981910.500 (1981910.500)	prob 1.303 (1.3026)	GS 28.078 (28.078)	mem 43.646
Train: [0][622/750]	BT 0.107 (1.177)	DT 0.002 (1.041)	loss 11.215 (11.215)	gnorm 2192136.500 (2192136.500)	prob 0.063 (0.0626)	GS 32.750 (32.750)	mem 43.613
Train: [0][623/750]	BT 0.154 (1.175)	DT 0.002 (1.039)	loss 10.547 (10.547)	gnorm 2278660.000 (2278660.000)	prob 1.832 (1.8316)	GS 30.922 (30.922)	mem 43.712
Train: [0][624/750]	BT 0.172 (1.174)	DT 0.026 (1.037)	loss 10.948 (10.948)	gnorm 2272177.250 (2272177.250)	prob 0.020 (0.0203)	GS 34.125 (34.125)	mem 43.672
Train: [0][625/750]	BT 0.093 (1.172)	DT 0.002 (1.036)	loss 10.534 (10.534)	gnorm 1983682.750 (1983682.750)	prob 0.709 (0.7090)	GS 30.641 (30.641)	mem 43.692
Train: [0][626/750]	BT 0.074 (1.170)	DT 0.002 (1.034)	loss 10.391 (10.391)	gnorm 1806962.250 (1806962.250)	prob 0.696 (0.6957)	GS 33.109 (33.109)	mem 43.883
Train: [0][627/750]	BT 0.162 (1.169)	DT 0.002 (1.032)	loss 10.636 (10.636)	gnorm 1900955.250 (1900955.250)	prob 1.713 (1.7132)	GS 29.656 (29.656)	mem 43.889
Train: [0][628/750]	BT 12.053 (1.186)	DT 11.974 (1.050)	loss 10.446 (10.446)	gnorm 2182232.500 (2182232.500)	prob 1.502 (1.5018)	GS 33.625 (33.625)	mem 43.617
Train: [0][629/750]	BT 0.130 (1.184)	DT 0.002 (1.048)	loss 10.400 (10.400)	gnorm 2057287.750 (2057287.750)	prob 1.289 (1.2893)	GS 38.281 (38.281)	mem 43.618
Train: [0][630/750]	BT 0.200 (1.183)	DT 0.001 (1.047)	loss 10.123 (10.123)	gnorm 1995336.375 (1995336.375)	prob 1.126 (1.1257)	GS 36.781 (36.781)	mem 43.741
Train: [0][631/750]	BT 0.158 (1.181)	DT 0.006 (1.045)	loss 10.653 (10.653)	gnorm 1967572.250 (1967572.250)	prob 0.328 (0.3284)	GS 27.016 (27.016)	mem 43.722
Train: [0][632/750]	BT 0.117 (1.179)	DT 0.002 (1.043)	loss 10.393 (10.393)	gnorm 2001853.875 (2001853.875)	prob 0.571 (0.5709)	GS 37.266 (37.266)	mem 43.616
Train: [0][633/750]	BT 0.129 (1.178)	DT 0.009 (1.042)	loss 10.883 (10.883)	gnorm 1932885.750 (1932885.750)	prob 0.786 (0.7860)	GS 28.453 (28.453)	mem 43.618
Train: [0][634/750]	BT 0.125 (1.176)	DT 0.008 (1.040)	loss 10.963 (10.963)	gnorm 1962782.125 (1962782.125)	prob -0.085 (-0.0846)	GS 35.656 (35.656)	mem 43.618
Train: [0][635/750]	BT 0.160 (1.174)	DT 0.003 (1.038)	loss 10.067 (10.067)	gnorm 1948010.625 (1948010.625)	prob 1.664 (1.6638)	GS 30.359 (30.359)	mem 43.682
Train: [0][636/750]	BT 0.210 (1.173)	DT 0.003 (1.037)	loss 10.066 (10.066)	gnorm 1893795.625 (1893795.625)	prob 1.308 (1.3077)	GS 35.719 (35.719)	mem 43.619
Train: [0][637/750]	BT 0.113 (1.171)	DT 0.001 (1.035)	loss 9.994 (9.994)	gnorm 1725278.375 (1725278.375)	prob 1.383 (1.3831)	GS 29.844 (29.844)	mem 43.619
Train: [0][638/750]	BT 0.097 (1.170)	DT 0.003 (1.033)	loss 10.113 (10.113)	gnorm 1830302.250 (1830302.250)	prob 1.540 (1.5397)	GS 33.656 (33.656)	mem 43.619
Train: [0][639/750]	BT 0.133 (1.168)	DT 0.010 (1.032)	loss 10.092 (10.092)	gnorm 1793609.250 (1793609.250)	prob 0.723 (0.7231)	GS 32.656 (32.656)	mem 43.664
Train: [0][640/750]	BT 14.559 (1.189)	DT 14.450 (1.053)	loss 10.077 (10.077)	gnorm 1718325.750 (1718325.750)	prob 0.947 (0.9473)	GS 36.141 (36.141)	mem 43.717
Train: [0][641/750]	BT 0.099 (1.187)	DT 0.002 (1.051)	loss 11.011 (11.011)	gnorm 1966748.500 (1966748.500)	prob 0.322 (0.3218)	GS 29.141 (29.141)	mem 43.716
Train: [0][642/750]	BT 0.185 (1.186)	DT 0.009 (1.050)	loss 10.876 (10.876)	gnorm 1829821.500 (1829821.500)	prob 0.523 (0.5235)	GS 31.719 (31.719)	mem 43.716
Train: [0][643/750]	BT 0.088 (1.184)	DT 0.002 (1.048)	loss 10.580 (10.580)	gnorm 1989838.625 (1989838.625)	prob 1.317 (1.3172)	GS 31.375 (31.375)	mem 43.745
Train: [0][644/750]	BT 0.154 (1.182)	DT 0.002 (1.046)	loss 10.584 (10.584)	gnorm 1769131.625 (1769131.625)	prob 0.767 (0.7668)	GS 31.859 (31.859)	mem 43.715
Train: [0][645/750]	BT 0.159 (1.181)	DT 0.004 (1.045)	loss 11.140 (11.140)	gnorm 2057964.625 (2057964.625)	prob 1.042 (1.0415)	GS 36.141 (36.141)	mem 43.752
Train: [0][646/750]	BT 0.132 (1.179)	DT 0.012 (1.043)	loss 9.975 (9.975)	gnorm 1854299.000 (1854299.000)	prob 1.353 (1.3532)	GS 32.938 (32.938)	mem 43.745
Train: [0][647/750]	BT 0.169 (1.178)	DT 0.002 (1.041)	loss 10.140 (10.140)	gnorm 1870763.500 (1870763.500)	prob 0.446 (0.4458)	GS 35.203 (35.203)	mem 43.782
Train: [0][648/750]	BT 0.193 (1.176)	DT 0.002 (1.040)	loss 9.664 (9.664)	gnorm 1813622.500 (1813622.500)	prob 1.235 (1.2349)	GS 31.844 (31.844)	mem 43.723
Train: [0][649/750]	BT 0.171 (1.174)	DT 0.002 (1.038)	loss 10.356 (10.356)	gnorm 1879107.625 (1879107.625)	prob 0.788 (0.7875)	GS 31.781 (31.781)	mem 43.716
Train: [0][650/750]	BT 0.102 (1.173)	DT 0.011 (1.037)	loss 10.764 (10.764)	gnorm 1825119.000 (1825119.000)	prob -0.125 (-0.1254)	GS 31.312 (31.312)	mem 43.716
Train: [0][651/750]	BT 0.108 (1.171)	DT 0.002 (1.035)	loss 10.012 (10.012)	gnorm 1796404.375 (1796404.375)	prob 1.600 (1.5996)	GS 29.953 (29.953)	mem 43.717
Train: [0][652/750]	BT 11.007 (1.186)	DT 10.861 (1.050)	loss 10.011 (10.011)	gnorm 1766510.250 (1766510.250)	prob 1.030 (1.0300)	GS 32.500 (32.500)	mem 43.646
Train: [0][653/750]	BT 0.086 (1.185)	DT 0.002 (1.049)	loss 10.526 (10.526)	gnorm 1647914.125 (1647914.125)	prob 0.129 (0.1288)	GS 30.750 (30.750)	mem 43.648
Train: [0][654/750]	BT 0.073 (1.183)	DT 0.002 (1.047)	loss 10.444 (10.444)	gnorm 2197665.500 (2197665.500)	prob 0.286 (0.2856)	GS 33.578 (33.578)	mem 43.648
Train: [0][655/750]	BT 0.112 (1.181)	DT 0.001 (1.045)	loss 10.463 (10.463)	gnorm 1883059.625 (1883059.625)	prob 0.172 (0.1717)	GS 32.016 (32.016)	mem 43.677
Train: [0][656/750]	BT 0.095 (1.180)	DT 0.001 (1.044)	loss 10.296 (10.296)	gnorm 1929937.250 (1929937.250)	prob 0.563 (0.5633)	GS 32.500 (32.500)	mem 43.898
Train: [0][657/750]	BT 0.159 (1.178)	DT 0.013 (1.042)	loss 10.954 (10.954)	gnorm 1901472.750 (1901472.750)	prob 0.268 (0.2685)	GS 30.031 (30.031)	mem 43.836
Train: [0][658/750]	BT 0.082 (1.176)	DT 0.002 (1.041)	loss 10.001 (10.001)	gnorm 1853479.750 (1853479.750)	prob 0.275 (0.2753)	GS 34.547 (34.547)	mem 43.650
Train: [0][659/750]	BT 0.222 (1.175)	DT 0.001 (1.039)	loss 10.255 (10.255)	gnorm 1535737.500 (1535737.500)	prob 0.553 (0.5534)	GS 27.469 (27.469)	mem 43.687
Train: [0][660/750]	BT 0.173 (1.173)	DT 0.031 (1.038)	loss 10.327 (10.327)	gnorm 1737292.375 (1737292.375)	prob 0.363 (0.3634)	GS 33.938 (33.938)	mem 43.651
Train: [0][661/750]	BT 0.120 (1.172)	DT 0.005 (1.036)	loss 10.398 (10.398)	gnorm 1957053.500 (1957053.500)	prob 0.481 (0.4810)	GS 44.703 (44.703)	mem 43.671
Train: [0][662/750]	BT 0.158 (1.170)	DT 0.001 (1.034)	loss 10.445 (10.445)	gnorm 1840977.750 (1840977.750)	prob 0.113 (0.1127)	GS 35.172 (35.172)	mem 43.712
Train: [0][663/750]	BT 0.116 (1.169)	DT 0.014 (1.033)	loss 9.821 (9.821)	gnorm 1550690.625 (1550690.625)	prob 1.329 (1.3289)	GS 32.078 (32.078)	mem 43.651
Train: [0][664/750]	BT 11.633 (1.184)	DT 11.543 (1.049)	loss 10.289 (10.289)	gnorm 1548329.625 (1548329.625)	prob 1.359 (1.3587)	GS 33.188 (33.188)	mem 43.683
Train: [0][665/750]	BT 0.069 (1.183)	DT 0.001 (1.047)	loss 10.453 (10.453)	gnorm 1845552.125 (1845552.125)	prob 1.215 (1.2149)	GS 30.891 (30.891)	mem 43.684
Train: [0][666/750]	BT 0.138 (1.181)	DT 0.002 (1.046)	loss 9.713 (9.713)	gnorm 1556395.500 (1556395.500)	prob 0.877 (0.8771)	GS 29.312 (29.312)	mem 43.661
Train: [0][667/750]	BT 0.127 (1.180)	DT 0.003 (1.044)	loss 10.074 (10.074)	gnorm 1587252.500 (1587252.500)	prob 1.062 (1.0624)	GS 28.344 (28.344)	mem 43.720
Train: [0][668/750]	BT 0.116 (1.178)	DT 0.002 (1.042)	loss 10.466 (10.466)	gnorm 1893026.000 (1893026.000)	prob 0.596 (0.5961)	GS 30.828 (30.828)	mem 43.642
Train: [0][669/750]	BT 0.263 (1.177)	DT 0.006 (1.041)	loss 10.219 (10.219)	gnorm 1589412.375 (1589412.375)	prob 1.069 (1.0693)	GS 34.344 (34.344)	mem 43.666
Train: [0][670/750]	BT 0.174 (1.175)	DT 0.050 (1.039)	loss 10.812 (10.812)	gnorm 1679822.250 (1679822.250)	prob 0.143 (0.1435)	GS 32.109 (32.109)	mem 43.631
Train: [0][671/750]	BT 0.103 (1.174)	DT 0.002 (1.038)	loss 10.146 (10.146)	gnorm 1730647.625 (1730647.625)	prob 1.235 (1.2350)	GS 28.125 (28.125)	mem 43.688
Train: [0][672/750]	BT 0.163 (1.172)	DT 0.006 (1.036)	loss 9.886 (9.886)	gnorm 1802344.750 (1802344.750)	prob 0.984 (0.9842)	GS 34.047 (34.047)	mem 43.651
Train: [0][673/750]	BT 0.134 (1.171)	DT 0.002 (1.035)	loss 10.013 (10.013)	gnorm 1658127.250 (1658127.250)	prob 1.585 (1.5845)	GS 31.578 (31.578)	mem 43.697
Train: [0][674/750]	BT 0.166 (1.169)	DT 0.010 (1.033)	loss 11.257 (11.257)	gnorm 1816390.750 (1816390.750)	prob -0.047 (-0.0468)	GS 34.312 (34.312)	mem 43.798
Train: [0][675/750]	BT 0.207 (1.168)	DT 0.008 (1.032)	loss 9.616 (9.616)	gnorm 1951055.500 (1951055.500)	prob 1.659 (1.6585)	GS 32.625 (32.625)	mem 43.643
Train: [0][676/750]	BT 12.527 (1.184)	DT 12.367 (1.048)	loss 10.726 (10.726)	gnorm 1697072.750 (1697072.750)	prob 0.200 (0.1995)	GS 34.094 (34.094)	mem 43.682
Train: [0][677/750]	BT 0.101 (1.183)	DT 0.009 (1.047)	loss 10.205 (10.205)	gnorm 1728889.375 (1728889.375)	prob 2.104 (2.1038)	GS 30.797 (30.797)	mem 43.814
Train: [0][678/750]	BT 0.098 (1.181)	DT 0.002 (1.045)	loss 10.001 (10.001)	gnorm 1546086.250 (1546086.250)	prob 1.660 (1.6596)	GS 35.141 (35.141)	mem 43.768
Train: [0][679/750]	BT 0.130 (1.180)	DT 0.002 (1.044)	loss 10.112 (10.112)	gnorm 1777900.875 (1777900.875)	prob 1.959 (1.9591)	GS 34.188 (34.188)	mem 43.670
Train: [0][680/750]	BT 0.115 (1.178)	DT 0.004 (1.042)	loss 10.114 (10.114)	gnorm 1547193.500 (1547193.500)	prob 1.535 (1.5352)	GS 33.500 (33.500)	mem 43.670
Train: [0][681/750]	BT 0.133 (1.177)	DT 0.007 (1.041)	loss 10.032 (10.032)	gnorm 1757325.750 (1757325.750)	prob 1.734 (1.7341)	GS 28.656 (28.656)	mem 43.669
Train: [0][682/750]	BT 2.626 (1.179)	DT 2.498 (1.043)	loss 10.416 (10.416)	gnorm 1850939.500 (1850939.500)	prob 0.584 (0.5838)	GS 36.531 (36.531)	mem 43.674
Train: [0][683/750]	BT 0.078 (1.177)	DT 0.001 (1.041)	loss 10.223 (10.223)	gnorm 1767448.250 (1767448.250)	prob 1.626 (1.6261)	GS 28.578 (28.578)	mem 43.674
Train: [0][684/750]	BT 0.140 (1.176)	DT 0.002 (1.040)	loss 10.609 (10.609)	gnorm 1902015.000 (1902015.000)	prob 0.928 (0.9278)	GS 35.281 (35.281)	mem 43.674
Train: [0][685/750]	BT 0.183 (1.174)	DT 0.004 (1.038)	loss 10.276 (10.276)	gnorm 1649779.250 (1649779.250)	prob 2.001 (2.0007)	GS 32.625 (32.625)	mem 43.674
Train: [0][686/750]	BT 0.178 (1.173)	DT 0.003 (1.037)	loss 10.387 (10.387)	gnorm 1686637.625 (1686637.625)	prob 2.217 (2.2172)	GS 35.188 (35.188)	mem 43.673
Train: [0][687/750]	BT 0.115 (1.171)	DT 0.011 (1.035)	loss 9.855 (9.855)	gnorm 1501217.625 (1501217.625)	prob 2.047 (2.0471)	GS 29.547 (29.547)	mem 43.675
Train: [0][688/750]	BT 6.087 (1.178)	DT 5.969 (1.043)	loss 10.429 (10.429)	gnorm 1646607.250 (1646607.250)	prob 1.569 (1.5689)	GS 32.062 (32.062)	mem 43.685
Train: [0][689/750]	BT 0.110 (1.177)	DT 0.002 (1.041)	loss 11.058 (11.058)	gnorm 1789058.500 (1789058.500)	prob 1.157 (1.1573)	GS 29.594 (29.594)	mem 43.685
Train: [0][690/750]	BT 2.093 (1.178)	DT 1.919 (1.042)	loss 10.426 (10.426)	gnorm 1846447.000 (1846447.000)	prob 1.447 (1.4468)	GS 36.594 (36.594)	mem 43.680
Train: [0][691/750]	BT 0.142 (1.177)	DT 0.002 (1.041)	loss 10.495 (10.495)	gnorm 1716190.000 (1716190.000)	prob 2.001 (2.0013)	GS 30.875 (30.875)	mem 43.617
Train: [0][692/750]	BT 0.172 (1.175)	DT 0.002 (1.039)	loss 10.252 (10.252)	gnorm 1787307.000 (1787307.000)	prob 1.360 (1.3600)	GS 33.391 (33.391)	mem 43.664
Train: [0][693/750]	BT 0.217 (1.174)	DT 0.009 (1.038)	loss 11.060 (11.060)	gnorm 1831990.000 (1831990.000)	prob 1.624 (1.6239)	GS 42.375 (42.375)	mem 43.715
Train: [0][694/750]	BT 3.720 (1.177)	DT 3.592 (1.042)	loss 10.458 (10.458)	gnorm 1869116.375 (1869116.375)	prob 1.289 (1.2887)	GS 35.234 (35.234)	mem 43.650
Train: [0][695/750]	BT 0.193 (1.176)	DT 0.006 (1.040)	loss 10.028 (10.028)	gnorm 1803787.750 (1803787.750)	prob 1.890 (1.8904)	GS 31.156 (31.156)	mem 43.706
Train: [0][696/750]	BT 0.102 (1.174)	DT 0.002 (1.039)	loss 10.168 (10.168)	gnorm 1641024.250 (1641024.250)	prob 1.974 (1.9740)	GS 29.797 (29.797)	mem 43.736
Train: [0][697/750]	BT 0.213 (1.173)	DT 0.010 (1.037)	loss 10.273 (10.273)	gnorm 1613660.375 (1613660.375)	prob 1.994 (1.9939)	GS 30.422 (30.422)	mem 43.713
Train: [0][698/750]	BT 0.110 (1.172)	DT 0.002 (1.036)	loss 10.575 (10.575)	gnorm 1640877.750 (1640877.750)	prob 1.871 (1.8707)	GS 32.609 (32.609)	mem 43.725
Train: [0][699/750]	BT 0.173 (1.170)	DT 0.002 (1.034)	loss 9.910 (9.910)	gnorm 1647506.500 (1647506.500)	prob 2.156 (2.1563)	GS 29.047 (29.047)	mem 43.689
Train: [0][700/750]	BT 7.792 (1.180)	DT 7.635 (1.044)	loss 10.661 (10.661)	gnorm 1629196.750 (1629196.750)	prob 1.047 (1.0475)	GS 30.609 (30.609)	mem 43.654
Train: [0][701/750]	BT 0.105 (1.178)	DT 0.009 (1.042)	loss 10.222 (10.222)	gnorm 1765017.125 (1765017.125)	prob 1.668 (1.6684)	GS 33.016 (33.016)	mem 43.654
Train: [0][702/750]	BT 3.023 (1.181)	DT 2.874 (1.045)	loss 10.551 (10.551)	gnorm 1804605.000 (1804605.000)	prob 0.870 (0.8698)	GS 37.297 (37.297)	mem 43.655
Train: [0][703/750]	BT 0.122 (1.179)	DT 0.006 (1.043)	loss 10.126 (10.126)	gnorm 1674959.500 (1674959.500)	prob 0.979 (0.9794)	GS 26.594 (26.594)	mem 43.658
Train: [0][704/750]	BT 0.215 (1.178)	DT 0.002 (1.042)	loss 11.555 (11.555)	gnorm 1952112.000 (1952112.000)	prob -0.388 (-0.3880)	GS 29.672 (29.672)	mem 43.715
Train: [0][705/750]	BT 0.249 (1.176)	DT 0.011 (1.040)	loss 10.859 (10.859)	gnorm 1753110.125 (1753110.125)	prob 0.899 (0.8993)	GS 35.562 (35.562)	mem 43.965
Train: [0][706/750]	BT 1.651 (1.177)	DT 1.528 (1.041)	loss 11.140 (11.140)	gnorm 1762222.875 (1762222.875)	prob -0.090 (-0.0902)	GS 33.375 (33.375)	mem 43.821
Train: [0][707/750]	BT 0.200 (1.176)	DT 0.002 (1.039)	loss 10.454 (10.454)	gnorm 1675823.125 (1675823.125)	prob 0.864 (0.8643)	GS 42.812 (42.812)	mem 43.782
Train: [0][708/750]	BT 0.156 (1.174)	DT 0.016 (1.038)	loss 10.891 (10.891)	gnorm 1645586.000 (1645586.000)	prob 0.683 (0.6834)	GS 34.094 (34.094)	mem 43.668
Train: [0][709/750]	BT 0.104 (1.173)	DT 0.002 (1.037)	loss 10.072 (10.072)	gnorm 1649978.000 (1649978.000)	prob 1.851 (1.8507)	GS 34.859 (34.859)	mem 43.667
Train: [0][710/750]	BT 0.098 (1.171)	DT 0.002 (1.035)	loss 10.137 (10.137)	gnorm 1481397.500 (1481397.500)	prob 1.885 (1.8852)	GS 34.281 (34.281)	mem 43.667
Train: [0][711/750]	BT 0.170 (1.170)	DT 0.011 (1.034)	loss 10.326 (10.326)	gnorm 1703206.875 (1703206.875)	prob 0.955 (0.9551)	GS 32.625 (32.625)	mem 43.726
Train: [0][712/750]	BT 8.227 (1.180)	DT 8.120 (1.044)	loss 10.701 (10.701)	gnorm 1624890.250 (1624890.250)	prob 0.874 (0.8740)	GS 41.031 (41.031)	mem 43.691
Train: [0][713/750]	BT 0.154 (1.178)	DT 0.022 (1.042)	loss 10.724 (10.724)	gnorm 1688783.875 (1688783.875)	prob 1.115 (1.1153)	GS 35.234 (35.234)	mem 43.723
Train: [0][714/750]	BT 2.607 (1.180)	DT 2.520 (1.044)	loss 10.731 (10.731)	gnorm 1766795.125 (1766795.125)	prob 0.405 (0.4049)	GS 35.109 (35.109)	mem 43.747
Train: [0][715/750]	BT 0.150 (1.179)	DT 0.002 (1.043)	loss 10.293 (10.293)	gnorm 1677564.875 (1677564.875)	prob 1.064 (1.0640)	GS 36.859 (36.859)	mem 43.724
Train: [0][716/750]	BT 0.108 (1.177)	DT 0.009 (1.041)	loss 10.631 (10.631)	gnorm 1686494.875 (1686494.875)	prob 0.286 (0.2861)	GS 32.953 (32.953)	mem 43.689
Train: [0][717/750]	BT 0.083 (1.176)	DT 0.002 (1.040)	loss 10.468 (10.468)	gnorm 1713713.125 (1713713.125)	prob 1.740 (1.7402)	GS 27.844 (27.844)	mem 43.778
Train: [0][718/750]	BT 0.130 (1.174)	DT 0.002 (1.038)	loss 10.376 (10.376)	gnorm 1539714.375 (1539714.375)	prob 1.331 (1.3308)	GS 33.984 (33.984)	mem 43.767
Train: [0][719/750]	BT 0.132 (1.173)	DT 0.002 (1.037)	loss 10.648 (10.648)	gnorm 1673615.875 (1673615.875)	prob 0.751 (0.7513)	GS 30.516 (30.516)	mem 43.692
Train: [0][720/750]	BT 0.161 (1.172)	DT 0.001 (1.036)	loss 10.154 (10.154)	gnorm 1716566.875 (1716566.875)	prob 0.865 (0.8651)	GS 35.812 (35.812)	mem 43.693
Train: [0][721/750]	BT 0.146 (1.170)	DT 0.010 (1.034)	loss 10.823 (10.823)	gnorm 1828052.250 (1828052.250)	prob -0.139 (-0.1389)	GS 27.078 (27.078)	mem 43.693
Train: [0][722/750]	BT 0.199 (1.169)	DT 0.005 (1.033)	loss 9.616 (9.616)	gnorm 1730012.375 (1730012.375)	prob 2.062 (2.0618)	GS 35.250 (35.250)	mem 43.779
Train: [0][723/750]	BT 0.205 (1.167)	DT 0.018 (1.031)	loss 10.704 (10.704)	gnorm 1703804.250 (1703804.250)	prob 0.580 (0.5800)	GS 28.672 (28.672)	mem 43.693
Train: [0][724/750]	BT 5.943 (1.174)	DT 5.822 (1.038)	loss 10.114 (10.114)	gnorm 1622740.875 (1622740.875)	prob 0.574 (0.5742)	GS 31.141 (31.141)	mem 43.699
Train: [0][725/750]	BT 0.229 (1.173)	DT 0.008 (1.037)	loss 10.110 (10.110)	gnorm 1635369.625 (1635369.625)	prob 0.634 (0.6342)	GS 24.188 (24.188)	mem 43.687
Train: [0][726/750]	BT 3.768 (1.176)	DT 3.569 (1.040)	loss 10.594 (10.594)	gnorm 1461591.500 (1461591.500)	prob 0.349 (0.3487)	GS 31.594 (31.594)	mem 43.724
Train: [0][727/750]	BT 0.164 (1.175)	DT 0.007 (1.039)	loss 10.377 (10.377)	gnorm 1501694.250 (1501694.250)	prob 0.492 (0.4915)	GS 29.359 (29.359)	mem 43.724
Train: [0][728/750]	BT 0.100 (1.173)	DT 0.003 (1.037)	loss 10.156 (10.156)	gnorm 1737532.500 (1737532.500)	prob 1.002 (1.0016)	GS 31.078 (31.078)	mem 43.556
arpack error, retry= 0
arpack error, retry= 0
arpack error, retry= 0
Train: [0][729/750]	BT 0.167 (1.172)	DT 0.011 (1.036)	loss 10.206 (10.206)	gnorm 1528690.625 (1528690.625)	prob 0.969 (0.9685)	GS 29.188 (29.188)	mem 43.482
Train: [0][730/750]	BT 0.158 (1.171)	DT 0.047 (1.034)	loss 11.238 (11.238)	gnorm 1465079.375 (1465079.375)	prob 0.089 (0.0889)	GS 33.031 (33.031)	mem 43.483
Train: [0][731/750]	BT 0.116 (1.169)	DT 0.002 (1.033)	loss 10.237 (10.237)	gnorm 1585003.375 (1585003.375)	prob 0.978 (0.9783)	GS 26.922 (26.922)	mem 43.483
Train: [0][732/750]	BT 0.104 (1.168)	DT 0.002 (1.032)	loss 10.436 (10.436)	gnorm 1603483.375 (1603483.375)	prob 0.752 (0.7520)	GS 32.594 (32.594)	mem 43.491
Train: [0][733/750]	BT 0.097 (1.166)	DT 0.003 (1.030)	loss 10.587 (10.587)	gnorm 1926571.500 (1926571.500)	prob 0.946 (0.9457)	GS 32.406 (32.406)	mem 43.491
Train: [0][734/750]	BT 0.113 (1.165)	DT 0.002 (1.029)	loss 10.135 (10.135)	gnorm 1892535.500 (1892535.500)	prob 0.606 (0.6056)	GS 38.922 (38.922)	mem 43.492
Train: [0][735/750]	BT 0.120 (1.163)	DT 0.002 (1.027)	loss 10.088 (10.088)	gnorm 1601022.500 (1601022.500)	prob 1.407 (1.4075)	GS 33.000 (33.000)	mem 43.534
Train: [0][736/750]	BT 3.346 (1.166)	DT 3.243 (1.030)	loss 10.336 (10.336)	gnorm 1673998.500 (1673998.500)	prob 0.795 (0.7952)	GS 28.594 (28.594)	mem 31.839
Train: [0][737/750]	BT 0.072 (1.165)	DT 0.001 (1.029)	loss 10.653 (10.653)	gnorm 1665920.625 (1665920.625)	prob 1.995 (1.9954)	GS 38.000 (38.000)	mem 31.839
Train: [0][738/750]	BT 5.884 (1.171)	DT 5.806 (1.035)	loss 10.819 (10.819)	gnorm 1677697.750 (1677697.750)	prob 0.858 (0.8579)	GS 32.703 (32.703)	mem 14.357
Train: [0][739/750]	BT 0.077 (1.170)	DT 0.002 (1.034)	loss 10.169 (10.169)	gnorm 1528525.250 (1528525.250)	prob 2.155 (2.1553)	GS 32.797 (32.797)	mem 14.408
Train: [0][740/750]	BT 0.087 (1.168)	DT 0.002 (1.033)	loss 10.458 (10.458)	gnorm 1550703.875 (1550703.875)	prob 1.816 (1.8160)	GS 32.656 (32.656)	mem 14.357
Train: [0][741/750]	BT 0.079 (1.167)	DT 0.001 (1.031)	loss 10.579 (10.579)	gnorm 1560343.125 (1560343.125)	prob 1.537 (1.5375)	GS 32.703 (32.703)	mem 14.357
Train: [0][742/750]	BT 0.119 (1.166)	DT 0.008 (1.030)	loss 10.463 (10.463)	gnorm 1648400.000 (1648400.000)	prob 1.423 (1.4225)	GS 32.641 (32.641)	mem 14.357
Train: [0][743/750]	BT 0.107 (1.164)	DT 0.001 (1.029)	loss 10.421 (10.421)	gnorm 1578852.625 (1578852.625)	prob 1.968 (1.9676)	GS 30.734 (30.734)	mem 14.357
Train: [0][744/750]	BT 0.097 (1.163)	DT 0.001 (1.027)	loss 10.387 (10.387)	gnorm 1577231.125 (1577231.125)	prob 1.963 (1.9635)	GS 32.312 (32.312)	mem 14.358
Train: [0][745/750]	BT 0.066 (1.161)	DT 0.002 (1.026)	loss 10.027 (10.027)	gnorm 2207285.000 (2207285.000)	prob 2.305 (2.3049)	GS 27.312 (27.312)	mem 14.359
Train: [0][746/750]	BT 0.078 (1.160)	DT 0.001 (1.024)	loss 10.106 (10.106)	gnorm 2142839.250 (2142839.250)	prob 1.987 (1.9866)	GS 40.469 (40.469)	mem 14.359
Train: [0][747/750]	BT 0.069 (1.158)	DT 0.001 (1.023)	loss 9.973 (9.973)	gnorm 2098859.250 (2098859.250)	prob 2.348 (2.3477)	GS 30.969 (30.969)	mem 14.359
Train: [0][748/750]	BT 0.137 (1.157)	DT 0.001 (1.022)	loss 11.432 (11.432)	gnorm 2600262.250 (2600262.250)	prob 1.290 (1.2895)	GS 37.562 (37.562)	mem 14.359
Train: [0][749/750]	BT 0.074 (1.155)	DT 0.003 (1.020)	loss 9.848 (9.848)	gnorm 2077862.000 (2077862.000)	prob 3.392 (3.3924)	GS 28.969 (28.969)	mem 14.359
Train: [0][750/750]	BT 3.341 (1.158)	DT 3.258 (1.023)	loss 9.988 (9.988)	gnorm 2417360.500 (2417360.500)	prob 2.859 (2.8592)	GS 36.375 (36.375)	mem 14.286
Train: [0][751/750]	BT 0.064 (1.157)	DT 0.001 (1.022)	loss 10.418 (10.418)	gnorm 2541504.250 (2541504.250)	prob 2.427 (2.4274)	GS 27.250 (27.250)	mem 14.288
Train: [0][752/750]	BT 0.064 (1.155)	DT 0.001 (1.021)	loss 10.603 (10.603)	gnorm 2529564.500 (2529564.500)	prob 2.747 (2.7468)	GS 34.188 (34.188)	mem 14.290
Train: [0][753/750]	BT 0.067 (1.154)	DT 0.001 (1.019)	loss 9.873 (9.873)	gnorm 2128374.750 (2128374.750)	prob 3.450 (3.4498)	GS 30.125 (30.125)	mem 14.291
Train: [0][754/750]	BT 0.088 (1.153)	DT 0.009 (1.018)	loss 10.165 (10.165)	gnorm 2708746.250 (2708746.250)	prob 3.010 (3.0098)	GS 40.625 (40.625)	mem 13.938
Train: [0][755/750]	BT 0.082 (1.151)	DT 0.002 (1.017)	loss 9.920 (9.920)	gnorm 2395920.500 (2395920.500)	prob 2.867 (2.8673)	GS 33.781 (33.781)	mem 12.145
Train: [0][756/750]	BT 0.083 (1.150)	DT 0.002 (1.015)	loss 10.377 (10.377)	gnorm 2377219.250 (2377219.250)	prob 2.277 (2.2770)	GS 39.688 (39.688)	mem 11.352
epoch 0, total time 869.44
==> Saving...
==> Saving...
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [1][1/750]	BT 21.021 (21.021)	DT 20.840 (20.840)	loss 10.036 (10.036)	gnorm 1696148.125 (1696148.125)	prob 2.802 (2.8018)	GS 34.719 (34.719)	mem 42.119
Train: [1][2/750]	BT 0.252 (10.636)	DT 0.002 (10.421)	loss 11.098 (11.098)	gnorm 1825768.000 (1825768.000)	prob 1.183 (1.1825)	GS 30.625 (30.625)	mem 42.113
Train: [1][3/750]	BT 0.234 (7.169)	DT 0.007 (6.950)	loss 10.507 (10.507)	gnorm 1869416.125 (1869416.125)	prob 3.071 (3.0710)	GS 30.219 (30.219)	mem 42.115
Train: [1][4/750]	BT 2.579 (6.022)	DT 2.375 (5.806)	loss 10.995 (10.995)	gnorm 1807288.500 (1807288.500)	prob 1.666 (1.6656)	GS 33.656 (33.656)	mem 42.346
Train: [1][5/750]	BT 0.093 (4.836)	DT 0.003 (4.645)	loss 10.714 (10.714)	gnorm 1582313.250 (1582313.250)	prob 2.326 (2.3256)	GS 29.781 (29.781)	mem 42.377
Train: [1][6/750]	BT 0.137 (4.053)	DT 0.001 (3.871)	loss 10.221 (10.221)	gnorm 1810352.750 (1810352.750)	prob 3.724 (3.7245)	GS 36.625 (36.625)	mem 42.272
Train: [1][7/750]	BT 0.163 (3.497)	DT 0.025 (3.322)	loss 10.196 (10.196)	gnorm 1590595.875 (1590595.875)	prob 3.464 (3.4639)	GS 32.391 (32.391)	mem 42.245
Train: [1][8/750]	BT 0.484 (3.120)	DT 0.312 (2.946)	loss 10.394 (10.394)	gnorm 1818827.750 (1818827.750)	prob 3.024 (3.0242)	GS 36.625 (36.625)	mem 42.376
Train: [1][9/750]	BT 0.100 (2.785)	DT 0.003 (2.619)	loss 10.918 (10.918)	gnorm 1619787.125 (1619787.125)	prob 2.375 (2.3753)	GS 26.734 (26.734)	mem 42.285
Train: [1][10/750]	BT 0.389 (2.545)	DT 0.146 (2.372)	loss 9.734 (9.734)	gnorm 1557385.125 (1557385.125)	prob 3.203 (3.2034)	GS 35.094 (35.094)	mem 42.244
Train: [1][11/750]	BT 0.114 (2.324)	DT 0.009 (2.157)	loss 10.727 (10.727)	gnorm 1573077.375 (1573077.375)	prob 2.452 (2.4518)	GS 34.969 (34.969)	mem 42.244
Train: [1][12/750]	BT 0.080 (2.137)	DT 0.001 (1.977)	loss 10.676 (10.676)	gnorm 1657457.000 (1657457.000)	prob 1.968 (1.9681)	GS 30.703 (30.703)	mem 42.243
Train: [1][13/750]	BT 6.802 (2.496)	DT 6.672 (2.338)	loss 10.834 (10.834)	gnorm 1561677.625 (1561677.625)	prob 2.407 (2.4066)	GS 28.484 (28.484)	mem 42.416
Train: [1][14/750]	BT 0.117 (2.326)	DT 0.001 (2.171)	loss 10.565 (10.565)	gnorm 1508711.875 (1508711.875)	prob 1.967 (1.9671)	GS 32.859 (32.859)	mem 42.416
Train: [1][15/750]	BT 0.136 (2.180)	DT 0.002 (2.027)	loss 10.200 (10.200)	gnorm 1755008.125 (1755008.125)	prob 2.663 (2.6633)	GS 32.953 (32.953)	mem 42.432
Train: [1][16/750]	BT 3.690 (2.274)	DT 3.596 (2.125)	loss 9.965 (9.965)	gnorm 1574907.125 (1574907.125)	prob 2.396 (2.3963)	GS 34.641 (34.641)	mem 42.560
Train: [1][17/750]	BT 0.097 (2.146)	DT 0.002 (2.000)	loss 10.425 (10.425)	gnorm 1551183.750 (1551183.750)	prob 2.355 (2.3554)	GS 26.625 (26.625)	mem 42.672
Train: [1][18/750]	BT 0.210 (2.039)	DT 0.002 (1.889)	loss 10.376 (10.376)	gnorm 1327024.750 (1327024.750)	prob 1.760 (1.7603)	GS 32.906 (32.906)	mem 42.634
Train: [1][19/750]	BT 0.091 (1.936)	DT 0.002 (1.790)	loss 9.958 (9.958)	gnorm 1414492.375 (1414492.375)	prob 2.520 (2.5203)	GS 29.797 (29.797)	mem 42.527
Train: [1][20/750]	BT 1.019 (1.890)	DT 0.873 (1.744)	loss 10.905 (10.905)	gnorm 1444060.625 (1444060.625)	prob 1.390 (1.3902)	GS 31.266 (31.266)	mem 42.689
Train: [1][21/750]	BT 0.096 (1.805)	DT 0.002 (1.661)	loss 10.653 (10.653)	gnorm 1404172.500 (1404172.500)	prob 1.138 (1.1384)	GS 29.516 (29.516)	mem 42.635
Train: [1][22/750]	BT 0.094 (1.727)	DT 0.002 (1.585)	loss 10.761 (10.761)	gnorm 1492223.500 (1492223.500)	prob 0.727 (0.7273)	GS 29.641 (29.641)	mem 42.671
Train: [1][23/750]	BT 0.092 (1.656)	DT 0.003 (1.517)	loss 10.114 (10.114)	gnorm 1375134.250 (1375134.250)	prob 1.378 (1.3785)	GS 34.734 (34.734)	mem 42.555
Train: [1][24/750]	BT 0.223 (1.596)	DT 0.018 (1.454)	loss 10.094 (10.094)	gnorm 1598958.125 (1598958.125)	prob 0.908 (0.9083)	GS 31.172 (31.172)	mem 42.561
Train: [1][25/750]	BT 5.426 (1.750)	DT 5.269 (1.607)	loss 10.403 (10.403)	gnorm 1520035.625 (1520035.625)	prob 0.478 (0.4779)	GS 32.234 (32.234)	mem 42.652
Train: [1][26/750]	BT 1.393 (1.736)	DT 1.245 (1.593)	loss 10.663 (10.663)	gnorm 1620144.500 (1620144.500)	prob -0.331 (-0.3313)	GS 32.891 (32.891)	mem 42.602
Train: [1][27/750]	BT 0.198 (1.679)	DT 0.007 (1.534)	loss 10.282 (10.282)	gnorm 1468241.125 (1468241.125)	prob 0.559 (0.5589)	GS 29.438 (29.438)	mem 42.569
Train: [1][28/750]	BT 2.642 (1.713)	DT 2.494 (1.568)	loss 9.911 (9.911)	gnorm 1378568.500 (1378568.500)	prob 0.645 (0.6451)	GS 35.312 (35.312)	mem 42.624
Train: [1][29/750]	BT 0.124 (1.658)	DT 0.001 (1.514)	loss 10.209 (10.209)	gnorm 1406847.625 (1406847.625)	prob 1.178 (1.1778)	GS 31.906 (31.906)	mem 42.838
Train: [1][30/750]	BT 7.314 (1.847)	DT 7.179 (1.703)	loss 10.634 (10.634)	gnorm 1528135.500 (1528135.500)	prob 0.922 (0.9219)	GS 33.297 (33.297)	mem 42.605
Train: [1][31/750]	BT 0.180 (1.793)	DT 0.001 (1.648)	loss 10.309 (10.309)	gnorm 1552575.750 (1552575.750)	prob 1.002 (1.0018)	GS 31.219 (31.219)	mem 42.720
Train: [1][32/750]	BT 0.109 (1.741)	DT 0.005 (1.597)	loss 10.710 (10.710)	gnorm 1516099.000 (1516099.000)	prob 0.922 (0.9219)	GS 35.703 (35.703)	mem 42.627
Train: [1][33/750]	BT 0.126 (1.692)	DT 0.002 (1.549)	loss 10.438 (10.438)	gnorm 1483525.375 (1483525.375)	prob 2.315 (2.3154)	GS 33.047 (33.047)	mem 42.571
Train: [1][34/750]	BT 0.220 (1.648)	DT 0.002 (1.503)	loss 10.610 (10.610)	gnorm 1590649.875 (1590649.875)	prob 1.582 (1.5820)	GS 32.516 (32.516)	mem 42.572
Train: [1][35/750]	BT 0.119 (1.605)	DT 0.004 (1.460)	loss 10.215 (10.215)	gnorm 1522984.375 (1522984.375)	prob 2.292 (2.2919)	GS 38.062 (38.062)	mem 42.604
Train: [1][36/750]	BT 0.105 (1.563)	DT 0.003 (1.420)	loss 10.314 (10.314)	gnorm 1432152.750 (1432152.750)	prob 2.028 (2.0278)	GS 32.562 (32.562)	mem 42.687
Train: [1][37/750]	BT 3.603 (1.618)	DT 3.466 (1.475)	loss 10.539 (10.539)	gnorm 1737759.125 (1737759.125)	prob 2.184 (2.1837)	GS 44.766 (44.766)	mem 42.620
Train: [1][38/750]	BT 1.308 (1.610)	DT 1.126 (1.466)	loss 10.052 (10.052)	gnorm 1386063.750 (1386063.750)	prob 1.970 (1.9702)	GS 32.891 (32.891)	mem 42.663
Train: [1][39/750]	BT 0.102 (1.571)	DT 0.002 (1.428)	loss 10.966 (10.966)	gnorm 1492263.000 (1492263.000)	prob 1.229 (1.2293)	GS 30.516 (30.516)	mem 42.640
Train: [1][40/750]	BT 0.206 (1.537)	DT 0.121 (1.396)	loss 10.457 (10.457)	gnorm 1412753.625 (1412753.625)	prob 1.510 (1.5096)	GS 31.938 (31.938)	mem 42.728
Train: [1][41/750]	BT 0.111 (1.502)	DT 0.002 (1.362)	loss 9.981 (9.981)	gnorm 1424815.875 (1424815.875)	prob 1.822 (1.8220)	GS 33.656 (33.656)	mem 42.551
Train: [1][42/750]	BT 2.107 (1.517)	DT 1.978 (1.376)	loss 10.326 (10.326)	gnorm 1597233.500 (1597233.500)	prob 1.278 (1.2782)	GS 30.328 (30.328)	mem 42.582
Train: [1][43/750]	BT 0.108 (1.484)	DT 0.003 (1.344)	loss 9.975 (9.975)	gnorm 1515751.625 (1515751.625)	prob 2.446 (2.4464)	GS 31.906 (31.906)	mem 42.582
Train: [1][44/750]	BT 2.123 (1.499)	DT 1.955 (1.358)	loss 10.178 (10.178)	gnorm 1543730.750 (1543730.750)	prob 1.716 (1.7158)	GS 33.703 (33.703)	mem 42.624
Train: [1][45/750]	BT 0.103 (1.468)	DT 0.002 (1.328)	loss 10.934 (10.934)	gnorm 1616452.375 (1616452.375)	prob 1.395 (1.3946)	GS 27.266 (27.266)	mem 42.664
Train: [1][46/750]	BT 0.238 (1.441)	DT 0.013 (1.300)	loss 10.902 (10.902)	gnorm 1562785.875 (1562785.875)	prob 0.946 (0.9463)	GS 37.203 (37.203)	mem 42.630
Train: [1][47/750]	BT 0.126 (1.413)	DT 0.003 (1.272)	loss 10.000 (10.000)	gnorm 1451951.375 (1451951.375)	prob 2.534 (2.5344)	GS 31.000 (31.000)	mem 42.626
Train: [1][48/750]	BT 0.129 (1.386)	DT 0.012 (1.246)	loss 11.058 (11.058)	gnorm 1553943.875 (1553943.875)	prob 1.336 (1.3356)	GS 32.750 (32.750)	mem 42.626
Train: [1][49/750]	BT 1.626 (1.391)	DT 1.374 (1.248)	loss 10.171 (10.171)	gnorm 1506982.250 (1506982.250)	prob 2.612 (2.6116)	GS 24.844 (24.844)	mem 42.627
Train: [1][50/750]	BT 6.447 (1.492)	DT 6.347 (1.350)	loss 11.251 (11.251)	gnorm 1614981.750 (1614981.750)	prob 1.467 (1.4665)	GS 33.469 (33.469)	mem 42.635
Train: [1][51/750]	BT 0.083 (1.464)	DT 0.001 (1.324)	loss 10.234 (10.234)	gnorm 1598376.500 (1598376.500)	prob 2.856 (2.8565)	GS 31.906 (31.906)	mem 42.635
Train: [1][52/750]	BT 0.125 (1.439)	DT 0.002 (1.298)	loss 10.829 (10.829)	gnorm 1954086.625 (1954086.625)	prob 1.817 (1.8167)	GS 34.312 (34.312)	mem 42.635
Train: [1][53/750]	BT 0.136 (1.414)	DT 0.007 (1.274)	loss 10.623 (10.623)	gnorm 1560084.125 (1560084.125)	prob 2.845 (2.8449)	GS 26.750 (26.750)	mem 42.635
Train: [1][54/750]	BT 0.091 (1.390)	DT 0.003 (1.251)	loss 10.419 (10.419)	gnorm 1420891.625 (1420891.625)	prob 2.147 (2.1470)	GS 31.516 (31.516)	mem 42.635
Train: [1][55/750]	BT 0.202 (1.368)	DT 0.003 (1.228)	loss 10.207 (10.207)	gnorm 1458839.125 (1458839.125)	prob 2.737 (2.7375)	GS 30.344 (30.344)	mem 42.764
Train: [1][56/750]	BT 4.586 (1.425)	DT 4.457 (1.286)	loss 10.625 (10.625)	gnorm 1540726.625 (1540726.625)	prob 2.057 (2.0568)	GS 36.516 (36.516)	mem 42.645
Train: [1][57/750]	BT 0.078 (1.402)	DT 0.002 (1.263)	loss 10.385 (10.385)	gnorm 1421088.250 (1421088.250)	prob 2.302 (2.3019)	GS 32.703 (32.703)	mem 42.647
Train: [1][58/750]	BT 0.133 (1.380)	DT 0.010 (1.241)	loss 10.895 (10.895)	gnorm 1577990.375 (1577990.375)	prob 1.978 (1.9781)	GS 30.750 (30.750)	mem 42.680
Train: [1][59/750]	BT 0.225 (1.360)	DT 0.005 (1.220)	loss 10.373 (10.373)	gnorm 1578653.000 (1578653.000)	prob 2.564 (2.5644)	GS 32.531 (32.531)	mem 42.696
Train: [1][60/750]	BT 0.217 (1.341)	DT 0.004 (1.200)	loss 10.808 (10.808)	gnorm 1659276.375 (1659276.375)	prob 2.019 (2.0194)	GS 29.734 (29.734)	mem 42.703
Train: [1][61/750]	BT 0.244 (1.323)	DT 0.003 (1.181)	loss 10.005 (10.005)	gnorm 1376710.000 (1376710.000)	prob 2.959 (2.9585)	GS 29.984 (29.984)	mem 42.724
Train: [1][62/750]	BT 4.737 (1.378)	DT 4.601 (1.236)	loss 10.610 (10.610)	gnorm 1593770.250 (1593770.250)	prob 2.483 (2.4829)	GS 34.250 (34.250)	mem 42.692
Train: [1][63/750]	BT 0.089 (1.358)	DT 0.002 (1.216)	loss 10.164 (10.164)	gnorm 1483419.625 (1483419.625)	prob 3.476 (3.4760)	GS 35.438 (35.438)	mem 42.776
Train: [1][64/750]	BT 0.216 (1.340)	DT 0.011 (1.197)	loss 9.716 (9.716)	gnorm 1362083.375 (1362083.375)	prob 3.150 (3.1497)	GS 33.938 (33.938)	mem 42.793
Train: [1][65/750]	BT 0.201 (1.323)	DT 0.004 (1.179)	loss 10.361 (10.361)	gnorm 1413743.000 (1413743.000)	prob 3.166 (3.1658)	GS 28.781 (28.781)	mem 42.804
Train: [1][66/750]	BT 0.724 (1.314)	DT 0.615 (1.170)	loss 10.566 (10.566)	gnorm 1457137.750 (1457137.750)	prob 2.619 (2.6190)	GS 29.438 (29.438)	mem 42.665
Train: [1][67/750]	BT 0.080 (1.295)	DT 0.002 (1.153)	loss 10.119 (10.119)	gnorm 1494444.125 (1494444.125)	prob 3.572 (3.5725)	GS 32.875 (32.875)	mem 42.695
Train: [1][68/750]	BT 6.289 (1.369)	DT 6.153 (1.227)	loss 10.681 (10.681)	gnorm 1397378.125 (1397378.125)	prob 2.084 (2.0835)	GS 34.562 (34.562)	mem 42.688
Train: [1][69/750]	BT 0.166 (1.351)	DT 0.002 (1.209)	loss 10.067 (10.067)	gnorm 1483616.250 (1483616.250)	prob 3.224 (3.2241)	GS 34.219 (34.219)	mem 42.688
Train: [1][70/750]	BT 0.124 (1.334)	DT 0.002 (1.192)	loss 10.339 (10.339)	gnorm 1457715.625 (1457715.625)	prob 2.591 (2.5912)	GS 29.297 (29.297)	mem 42.688
Train: [1][71/750]	BT 0.172 (1.317)	DT 0.002 (1.175)	loss 10.825 (10.825)	gnorm 1520200.125 (1520200.125)	prob 2.765 (2.7652)	GS 35.656 (35.656)	mem 42.723
Train: [1][72/750]	BT 0.634 (1.308)	DT 0.531 (1.166)	loss 10.691 (10.691)	gnorm 1418154.750 (1418154.750)	prob 2.451 (2.4515)	GS 35.297 (35.297)	mem 42.680
Train: [1][73/750]	BT 0.282 (1.294)	DT 0.002 (1.150)	loss 9.786 (9.786)	gnorm 1285081.875 (1285081.875)	prob 3.251 (3.2514)	GS 26.297 (26.297)	mem 42.581
Train: [1][74/750]	BT 7.907 (1.383)	DT 7.762 (1.239)	loss 10.194 (10.194)	gnorm 1372763.375 (1372763.375)	prob 2.655 (2.6554)	GS 36.125 (36.125)	mem 42.684
Train: [1][75/750]	BT 0.097 (1.366)	DT 0.002 (1.223)	loss 10.373 (10.373)	gnorm 1378309.375 (1378309.375)	prob 1.886 (1.8855)	GS 31.719 (31.719)	mem 42.687
Train: [1][76/750]	BT 0.178 (1.350)	DT 0.003 (1.207)	loss 9.231 (9.231)	gnorm 1215205.750 (1215205.750)	prob 3.578 (3.5776)	GS 32.672 (32.672)	mem 42.688
Train: [1][77/750]	BT 0.195 (1.335)	DT 0.024 (1.191)	loss 10.050 (10.050)	gnorm 1477727.250 (1477727.250)	prob 3.054 (3.0539)	GS 28.109 (28.109)	mem 42.689
Train: [1][78/750]	BT 0.128 (1.320)	DT 0.002 (1.176)	loss 11.032 (11.032)	gnorm 1348142.125 (1348142.125)	prob 1.443 (1.4433)	GS 34.703 (34.703)	mem 42.690
Train: [1][79/750]	BT 0.079 (1.304)	DT 0.002 (1.161)	loss 10.269 (10.269)	gnorm 1507834.750 (1507834.750)	prob 2.182 (2.1822)	GS 36.031 (36.031)	mem 42.690
Train: [1][80/750]	BT 2.793 (1.323)	DT 2.688 (1.180)	loss 10.086 (10.086)	gnorm 1638687.375 (1638687.375)	prob 2.485 (2.4850)	GS 30.250 (30.250)	mem 42.671
Train: [1][81/750]	BT 0.127 (1.308)	DT 0.003 (1.166)	loss 10.941 (10.941)	gnorm 1626096.750 (1626096.750)	prob 2.551 (2.5505)	GS 30.312 (30.312)	mem 42.694
Train: [1][82/750]	BT 0.104 (1.293)	DT 0.001 (1.152)	loss 9.505 (9.505)	gnorm 1342462.875 (1342462.875)	prob 3.121 (3.1211)	GS 32.984 (32.984)	mem 42.675
Train: [1][83/750]	BT 0.138 (1.279)	DT 0.004 (1.138)	loss 9.949 (9.949)	gnorm 1366058.375 (1366058.375)	prob 3.265 (3.2647)	GS 36.609 (36.609)	mem 42.686
Train: [1][84/750]	BT 0.712 (1.273)	DT 0.569 (1.131)	loss 10.527 (10.527)	gnorm 1356888.375 (1356888.375)	prob 2.821 (2.8212)	GS 31.672 (31.672)	mem 42.711
Train: [1][85/750]	BT 0.174 (1.260)	DT 0.005 (1.118)	loss 9.705 (9.705)	gnorm 1454604.375 (1454604.375)	prob 3.519 (3.5190)	GS 29.828 (29.828)	mem 42.771
Train: [1][86/750]	BT 8.007 (1.338)	DT 7.830 (1.196)	loss 11.502 (11.502)	gnorm 1484443.000 (1484443.000)	prob 1.454 (1.4537)	GS 33.859 (33.859)	mem 42.743
Train: [1][87/750]	BT 0.156 (1.325)	DT 0.010 (1.182)	loss 9.943 (9.943)	gnorm 1384073.875 (1384073.875)	prob 3.995 (3.9953)	GS 43.438 (43.438)	mem 42.744
Train: [1][88/750]	BT 0.722 (1.318)	DT 0.622 (1.176)	loss 10.556 (10.556)	gnorm 1274279.125 (1274279.125)	prob 2.921 (2.9213)	GS 35.312 (35.312)	mem 42.763
Train: [1][89/750]	BT 0.126 (1.304)	DT 0.003 (1.163)	loss 9.726 (9.726)	gnorm 1428494.125 (1428494.125)	prob 3.479 (3.4788)	GS 33.516 (33.516)	mem 42.697
Train: [1][90/750]	BT 0.368 (1.294)	DT 0.229 (1.152)	loss 10.776 (10.776)	gnorm 1509881.375 (1509881.375)	prob 2.064 (2.0644)	GS 36.438 (36.438)	mem 42.708
Train: [1][91/750]	BT 0.099 (1.281)	DT 0.002 (1.140)	loss 10.558 (10.558)	gnorm 1278829.500 (1278829.500)	prob 3.225 (3.2253)	GS 32.688 (32.688)	mem 42.737
Train: [1][92/750]	BT 0.775 (1.275)	DT 0.658 (1.134)	loss 10.334 (10.334)	gnorm 1412975.000 (1412975.000)	prob 3.381 (3.3807)	GS 33.578 (33.578)	mem 42.747
Train: [1][93/750]	BT 0.138 (1.263)	DT 0.002 (1.122)	loss 10.258 (10.258)	gnorm 1373908.500 (1373908.500)	prob 3.149 (3.1492)	GS 37.031 (37.031)	mem 42.748
Train: [1][94/750]	BT 0.106 (1.251)	DT 0.003 (1.110)	loss 10.118 (10.118)	gnorm 1352880.125 (1352880.125)	prob 3.327 (3.3273)	GS 36.766 (36.766)	mem 42.748
Train: [1][95/750]	BT 0.168 (1.239)	DT 0.013 (1.099)	loss 10.617 (10.617)	gnorm 1382433.125 (1382433.125)	prob 3.058 (3.0579)	GS 30.391 (30.391)	mem 42.748
Train: [1][96/750]	BT 2.201 (1.249)	DT 2.011 (1.108)	loss 9.856 (9.856)	gnorm 1329186.375 (1329186.375)	prob 3.846 (3.8456)	GS 33.312 (33.312)	mem 42.758
Train: [1][97/750]	BT 0.188 (1.238)	DT 0.005 (1.097)	loss 9.995 (9.995)	gnorm 1346476.875 (1346476.875)	prob 3.589 (3.5891)	GS 26.906 (26.906)	mem 42.772
Train: [1][98/750]	BT 6.227 (1.289)	DT 6.066 (1.148)	loss 10.630 (10.630)	gnorm 1414416.250 (1414416.250)	prob 2.595 (2.5947)	GS 35.859 (35.859)	mem 42.914
Train: [1][99/750]	BT 0.146 (1.278)	DT 0.002 (1.136)	loss 10.365 (10.365)	gnorm 1530388.375 (1530388.375)	prob 2.974 (2.9745)	GS 31.531 (31.531)	mem 42.914
Train: [1][100/750]	BT 4.157 (1.307)	DT 4.019 (1.165)	loss 10.564 (10.564)	gnorm 1387041.375 (1387041.375)	prob 2.676 (2.6761)	GS 37.547 (37.547)	mem 42.821
Train: [1][101/750]	BT 0.097 (1.295)	DT 0.001 (1.153)	loss 10.854 (10.854)	gnorm 1534588.875 (1534588.875)	prob 2.746 (2.7459)	GS 32.469 (32.469)	mem 42.824
Train: [1][102/750]	BT 0.196 (1.284)	DT 0.007 (1.142)	loss 10.934 (10.934)	gnorm 1390569.625 (1390569.625)	prob 2.278 (2.2778)	GS 33.938 (33.938)	mem 42.993
Train: [1][103/750]	BT 0.155 (1.273)	DT 0.019 (1.131)	loss 9.887 (9.887)	gnorm 1321760.250 (1321760.250)	prob 3.052 (3.0519)	GS 33.609 (33.609)	mem 42.940
Train: [1][104/750]	BT 0.094 (1.261)	DT 0.002 (1.120)	loss 10.868 (10.868)	gnorm 1288749.875 (1288749.875)	prob 2.025 (2.0253)	GS 29.953 (29.953)	mem 42.821
Train: [1][105/750]	BT 0.114 (1.251)	DT 0.027 (1.110)	loss 10.159 (10.159)	gnorm 1415709.625 (1415709.625)	prob 2.867 (2.8666)	GS 28.031 (28.031)	mem 42.837
Train: [1][106/750]	BT 0.182 (1.240)	DT 0.002 (1.099)	loss 10.436 (10.436)	gnorm 1319446.875 (1319446.875)	prob 2.066 (2.0665)	GS 34.391 (34.391)	mem 42.826
Train: [1][107/750]	BT 0.124 (1.230)	DT 0.002 (1.089)	loss 10.164 (10.164)	gnorm 1362085.250 (1362085.250)	prob 2.941 (2.9405)	GS 29.125 (29.125)	mem 42.933
Train: [1][108/750]	BT 5.588 (1.270)	DT 5.404 (1.129)	loss 10.377 (10.377)	gnorm 1427894.625 (1427894.625)	prob 2.490 (2.4895)	GS 32.844 (32.844)	mem 42.903
Train: [1][109/750]	BT 0.131 (1.260)	DT 0.002 (1.119)	loss 10.608 (10.608)	gnorm 1297110.500 (1297110.500)	prob 2.502 (2.5021)	GS 31.625 (31.625)	mem 43.074
Train: [1][110/750]	BT 4.457 (1.289)	DT 4.365 (1.148)	loss 9.754 (9.754)	gnorm 1307676.375 (1307676.375)	prob 2.902 (2.9020)	GS 35.062 (35.062)	mem 42.790
Train: [1][111/750]	BT 0.100 (1.278)	DT 0.010 (1.138)	loss 10.276 (10.276)	gnorm 1486893.250 (1486893.250)	prob 2.888 (2.8876)	GS 30.375 (30.375)	mem 42.792
Train: [1][112/750]	BT 2.380 (1.288)	DT 2.199 (1.148)	loss 10.786 (10.786)	gnorm 1331065.750 (1331065.750)	prob 2.220 (2.2203)	GS 34.031 (34.031)	mem 43.029
Train: [1][113/750]	BT 0.099 (1.278)	DT 0.007 (1.137)	loss 10.862 (10.862)	gnorm 1420756.500 (1420756.500)	prob 2.158 (2.1580)	GS 30.641 (30.641)	mem 42.951
Train: [1][114/750]	BT 0.173 (1.268)	DT 0.002 (1.127)	loss 10.090 (10.090)	gnorm 1425397.375 (1425397.375)	prob 2.273 (2.2733)	GS 34.750 (34.750)	mem 42.991
Train: [1][115/750]	BT 0.126 (1.258)	DT 0.010 (1.118)	loss 10.436 (10.436)	gnorm 1338051.250 (1338051.250)	prob 2.347 (2.3469)	GS 26.203 (26.203)	mem 42.803
Train: [1][116/750]	BT 0.145 (1.248)	DT 0.002 (1.108)	loss 9.741 (9.741)	gnorm 1319730.500 (1319730.500)	prob 2.923 (2.9228)	GS 35.766 (35.766)	mem 42.802
Train: [1][117/750]	BT 0.140 (1.239)	DT 0.018 (1.099)	loss 9.814 (9.814)	gnorm 1316652.250 (1316652.250)	prob 3.150 (3.1496)	GS 33.234 (33.234)	mem 42.871
Train: [1][118/750]	BT 0.188 (1.230)	DT 0.002 (1.090)	loss 10.934 (10.934)	gnorm 1256363.750 (1256363.750)	prob 2.031 (2.0306)	GS 30.562 (30.562)	mem 42.919
Train: [1][119/750]	BT 0.103 (1.221)	DT 0.012 (1.080)	loss 10.389 (10.389)	gnorm 1348346.125 (1348346.125)	prob 2.844 (2.8443)	GS 30.953 (30.953)	mem 42.802
Train: [1][120/750]	BT 5.090 (1.253)	DT 5.011 (1.113)	loss 9.941 (9.941)	gnorm 1242055.500 (1242055.500)	prob 2.840 (2.8397)	GS 32.859 (32.859)	mem 42.845
Train: [1][121/750]	BT 0.135 (1.244)	DT 0.002 (1.104)	loss 11.065 (11.065)	gnorm 1366552.375 (1366552.375)	prob 1.776 (1.7762)	GS 30.766 (30.766)	mem 42.874
Train: [1][122/750]	BT 1.773 (1.248)	DT 1.564 (1.108)	loss 9.930 (9.930)	gnorm 1413751.375 (1413751.375)	prob 3.027 (3.0268)	GS 30.266 (30.266)	mem 42.945
Train: [1][123/750]	BT 0.147 (1.239)	DT 0.010 (1.099)	loss 10.517 (10.517)	gnorm 1392374.625 (1392374.625)	prob 2.401 (2.4012)	GS 31.734 (31.734)	mem 42.918
Train: [1][124/750]	BT 4.186 (1.263)	DT 4.054 (1.123)	loss 10.579 (10.579)	gnorm 1585237.500 (1585237.500)	prob 2.311 (2.3112)	GS 33.891 (33.891)	mem 42.885
Train: [1][125/750]	BT 0.085 (1.253)	DT 0.002 (1.114)	loss 9.994 (9.994)	gnorm 1280265.250 (1280265.250)	prob 2.481 (2.4806)	GS 27.250 (27.250)	mem 42.886
Train: [1][126/750]	BT 0.086 (1.244)	DT 0.002 (1.105)	loss 10.461 (10.461)	gnorm 1288816.125 (1288816.125)	prob 2.307 (2.3071)	GS 35.672 (35.672)	mem 42.886
Train: [1][127/750]	BT 0.101 (1.235)	DT 0.002 (1.096)	loss 10.635 (10.635)	gnorm 1335612.875 (1335612.875)	prob 2.668 (2.6678)	GS 33.141 (33.141)	mem 42.887
Train: [1][128/750]	BT 1.717 (1.239)	DT 1.600 (1.100)	loss 10.733 (10.733)	gnorm 1427845.500 (1427845.500)	prob 2.266 (2.2663)	GS 37.016 (37.016)	mem 42.974
Train: [1][129/750]	BT 0.084 (1.230)	DT 0.002 (1.092)	loss 10.378 (10.378)	gnorm 1403245.875 (1403245.875)	prob 3.256 (3.2560)	GS 32.562 (32.562)	mem 42.889
Train: [1][130/750]	BT 0.173 (1.222)	DT 0.002 (1.083)	loss 10.272 (10.272)	gnorm 1445928.875 (1445928.875)	prob 2.728 (2.7281)	GS 32.188 (32.188)	mem 42.989
Train: [1][131/750]	BT 0.114 (1.213)	DT 0.006 (1.075)	loss 10.524 (10.524)	gnorm 1357484.875 (1357484.875)	prob 2.148 (2.1478)	GS 32.203 (32.203)	mem 42.891
Train: [1][132/750]	BT 0.107 (1.205)	DT 0.013 (1.067)	loss 9.716 (9.716)	gnorm 1231677.500 (1231677.500)	prob 3.016 (3.0162)	GS 33.031 (33.031)	mem 42.891
Train: [1][133/750]	BT 0.159 (1.197)	DT 0.008 (1.059)	loss 10.270 (10.270)	gnorm 1276816.875 (1276816.875)	prob 2.377 (2.3768)	GS 27.969 (27.969)	mem 42.891
Train: [1][134/750]	BT 3.221 (1.212)	DT 3.078 (1.074)	loss 10.207 (10.207)	gnorm 1349314.500 (1349314.500)	prob 1.711 (1.7110)	GS 31.484 (31.484)	mem 43.006
Train: [1][135/750]	BT 0.186 (1.205)	DT 0.017 (1.066)	loss 10.754 (10.754)	gnorm 1317119.125 (1317119.125)	prob 2.259 (2.2592)	GS 28.953 (28.953)	mem 42.894
Train: [1][136/750]	BT 8.940 (1.261)	DT 8.833 (1.123)	loss 10.265 (10.265)	gnorm 1319669.000 (1319669.000)	prob 1.599 (1.5991)	GS 34.375 (34.375)	mem 43.091
Train: [1][137/750]	BT 0.136 (1.253)	DT 0.002 (1.115)	loss 10.807 (10.807)	gnorm 1288818.875 (1288818.875)	prob 1.133 (1.1331)	GS 30.766 (30.766)	mem 43.090
Train: [1][138/750]	BT 0.161 (1.245)	DT 0.004 (1.107)	loss 9.956 (9.956)	gnorm 1346171.125 (1346171.125)	prob 1.996 (1.9958)	GS 33.766 (33.766)	mem 43.138
Train: [1][139/750]	BT 0.106 (1.237)	DT 0.012 (1.099)	loss 10.643 (10.643)	gnorm 1430098.625 (1430098.625)	prob 1.353 (1.3533)	GS 33.875 (33.875)	mem 43.131
Train: [1][140/750]	BT 0.247 (1.230)	DT 0.001 (1.091)	loss 11.340 (11.340)	gnorm 1393087.875 (1393087.875)	prob 0.228 (0.2276)	GS 29.141 (29.141)	mem 43.131
Train: [1][141/750]	BT 0.166 (1.222)	DT 0.032 (1.084)	loss 10.571 (10.571)	gnorm 1242004.000 (1242004.000)	prob 1.350 (1.3500)	GS 28.000 (28.000)	mem 43.130
Train: [1][142/750]	BT 0.092 (1.214)	DT 0.002 (1.076)	loss 10.053 (10.053)	gnorm 1153231.250 (1153231.250)	prob 2.119 (2.1194)	GS 36.062 (36.062)	mem 43.130
Train: [1][143/750]	BT 0.136 (1.207)	DT 0.001 (1.069)	loss 10.332 (10.332)	gnorm 1260054.875 (1260054.875)	prob 2.077 (2.0765)	GS 26.906 (26.906)	mem 43.206
Train: [1][144/750]	BT 0.145 (1.200)	DT 0.026 (1.062)	loss 10.699 (10.699)	gnorm 1312960.625 (1312960.625)	prob 1.564 (1.5645)	GS 33.672 (33.672)	mem 43.138
Train: [1][145/750]	BT 0.119 (1.192)	DT 0.011 (1.054)	loss 10.436 (10.436)	gnorm 1308172.000 (1308172.000)	prob 2.140 (2.1401)	GS 29.375 (29.375)	mem 43.180
Train: [1][146/750]	BT 1.741 (1.196)	DT 1.527 (1.058)	loss 10.435 (10.435)	gnorm 1433793.875 (1433793.875)	prob 1.723 (1.7226)	GS 31.875 (31.875)	mem 43.217
Train: [1][147/750]	BT 0.163 (1.189)	DT 0.003 (1.050)	loss 9.876 (9.876)	gnorm 1278272.125 (1278272.125)	prob 2.776 (2.7763)	GS 31.984 (31.984)	mem 43.334
Train: [1][148/750]	BT 10.160 (1.249)	DT 10.076 (1.111)	loss 10.551 (10.551)	gnorm 1199167.625 (1199167.625)	prob 2.150 (2.1504)	GS 37.547 (37.547)	mem 43.145
Train: [1][149/750]	BT 0.088 (1.242)	DT 0.002 (1.104)	loss 10.098 (10.098)	gnorm 1172608.625 (1172608.625)	prob 2.893 (2.8933)	GS 32.156 (32.156)	mem 43.146
Train: [1][150/750]	BT 0.222 (1.235)	DT 0.002 (1.097)	loss 11.195 (11.195)	gnorm 1412219.125 (1412219.125)	prob 1.972 (1.9724)	GS 32.203 (32.203)	mem 43.081
Train: [1][151/750]	BT 0.098 (1.227)	DT 0.002 (1.089)	loss 10.245 (10.245)	gnorm 1319669.500 (1319669.500)	prob 2.857 (2.8574)	GS 28.688 (28.688)	mem 43.081
Train: [1][152/750]	BT 0.139 (1.220)	DT 0.015 (1.082)	loss 10.069 (10.069)	gnorm 1458150.500 (1458150.500)	prob 3.035 (3.0349)	GS 35.266 (35.266)	mem 43.127
Train: [1][153/750]	BT 0.134 (1.213)	DT 0.014 (1.075)	loss 10.197 (10.197)	gnorm 1119642.625 (1119642.625)	prob 2.291 (2.2911)	GS 31.281 (31.281)	mem 43.087
Train: [1][154/750]	BT 0.076 (1.206)	DT 0.001 (1.068)	loss 10.757 (10.757)	gnorm 1251130.375 (1251130.375)	prob 1.899 (1.8987)	GS 34.344 (34.344)	mem 43.137
Train: [1][155/750]	BT 0.144 (1.199)	DT 0.011 (1.061)	loss 10.369 (10.369)	gnorm 1246624.000 (1246624.000)	prob 2.245 (2.2451)	GS 35.672 (35.672)	mem 43.289
Train: [1][156/750]	BT 0.240 (1.193)	DT 0.012 (1.055)	loss 10.349 (10.349)	gnorm 1537563.375 (1537563.375)	prob 2.509 (2.5089)	GS 34.578 (34.578)	mem 43.195
Train: [1][157/750]	BT 0.115 (1.186)	DT 0.005 (1.048)	loss 10.071 (10.071)	gnorm 1282656.125 (1282656.125)	prob 2.516 (2.5162)	GS 29.797 (29.797)	mem 43.171
Train: [1][158/750]	BT 1.739 (1.189)	DT 1.632 (1.052)	loss 10.071 (10.071)	gnorm 1321495.250 (1321495.250)	prob 2.037 (2.0370)	GS 33.172 (33.172)	mem 43.186
Train: [1][159/750]	BT 0.134 (1.183)	DT 0.002 (1.045)	loss 10.750 (10.750)	gnorm 1305565.250 (1305565.250)	prob 1.156 (1.1559)	GS 29.531 (29.531)	mem 43.163
Train: [1][160/750]	BT 10.841 (1.243)	DT 10.718 (1.106)	loss 10.492 (10.492)	gnorm 1095741.375 (1095741.375)	prob 1.283 (1.2829)	GS 30.359 (30.359)	mem 43.188
Train: [1][161/750]	BT 0.284 (1.237)	DT 0.002 (1.099)	loss 10.318 (10.318)	gnorm 1204109.625 (1204109.625)	prob 1.707 (1.7069)	GS 33.516 (33.516)	mem 43.218
Train: [1][162/750]	BT 0.218 (1.231)	DT 0.014 (1.092)	loss 10.595 (10.595)	gnorm 1292926.125 (1292926.125)	prob 2.191 (2.1909)	GS 34.266 (34.266)	mem 43.166
Train: [1][163/750]	BT 0.134 (1.224)	DT 0.012 (1.085)	loss 10.314 (10.314)	gnorm 1278942.750 (1278942.750)	prob 2.351 (2.3513)	GS 31.984 (31.984)	mem 43.219
Train: [1][164/750]	BT 0.113 (1.217)	DT 0.001 (1.079)	loss 10.789 (10.789)	gnorm 1357291.000 (1357291.000)	prob 1.484 (1.4836)	GS 32.281 (32.281)	mem 43.168
Train: [1][165/750]	BT 0.116 (1.211)	DT 0.017 (1.072)	loss 10.255 (10.255)	gnorm 1296599.375 (1296599.375)	prob 2.408 (2.4076)	GS 33.391 (33.391)	mem 43.168
Train: [1][166/750]	BT 0.189 (1.205)	DT 0.001 (1.066)	loss 10.283 (10.283)	gnorm 1369005.375 (1369005.375)	prob 1.535 (1.5351)	GS 34.234 (34.234)	mem 43.227
Train: [1][167/750]	BT 0.167 (1.198)	DT 0.003 (1.060)	loss 10.359 (10.359)	gnorm 1336597.375 (1336597.375)	prob 1.660 (1.6596)	GS 36.641 (36.641)	mem 43.270
Train: [1][168/750]	BT 0.304 (1.193)	DT 0.049 (1.054)	loss 10.460 (10.460)	gnorm 1314635.625 (1314635.625)	prob 1.107 (1.1070)	GS 29.500 (29.500)	mem 43.199
Train: [1][169/750]	BT 0.137 (1.187)	DT 0.013 (1.047)	loss 10.566 (10.566)	gnorm 1193296.000 (1193296.000)	prob 1.262 (1.2617)	GS 28.531 (28.531)	mem 43.243
Train: [1][170/750]	BT 0.228 (1.181)	DT 0.039 (1.041)	loss 9.885 (9.885)	gnorm 1253255.125 (1253255.125)	prob 1.556 (1.5563)	GS 34.484 (34.484)	mem 43.344
Train: [1][171/750]	BT 0.097 (1.175)	DT 0.003 (1.035)	loss 10.026 (10.026)	gnorm 1232623.125 (1232623.125)	prob 1.660 (1.6603)	GS 44.188 (44.188)	mem 43.345
Train: [1][172/750]	BT 14.632 (1.253)	DT 14.506 (1.114)	loss 11.282 (11.282)	gnorm 1304472.125 (1304472.125)	prob 0.600 (0.5996)	GS 34.797 (34.797)	mem 43.297
Train: [1][173/750]	BT 0.113 (1.246)	DT 0.002 (1.107)	loss 10.095 (10.095)	gnorm 1193725.375 (1193725.375)	prob 1.152 (1.1519)	GS 32.312 (32.312)	mem 43.328
Train: [1][174/750]	BT 0.119 (1.240)	DT 0.003 (1.101)	loss 9.993 (9.993)	gnorm 1116042.625 (1116042.625)	prob 1.371 (1.3711)	GS 30.188 (30.188)	mem 43.297
Train: [1][175/750]	BT 0.091 (1.233)	DT 0.002 (1.095)	loss 10.433 (10.433)	gnorm 1339843.000 (1339843.000)	prob 1.093 (1.0929)	GS 32.297 (32.297)	mem 43.297
Train: [1][176/750]	BT 0.177 (1.227)	DT 0.010 (1.088)	loss 10.449 (10.449)	gnorm 1251008.000 (1251008.000)	prob 0.848 (0.8483)	GS 34.891 (34.891)	mem 43.299
Train: [1][177/750]	BT 0.171 (1.221)	DT 0.006 (1.082)	loss 9.929 (9.929)	gnorm 1383392.875 (1383392.875)	prob 2.303 (2.3033)	GS 36.688 (36.688)	mem 43.299
Train: [1][178/750]	BT 0.196 (1.216)	DT 0.005 (1.076)	loss 10.526 (10.526)	gnorm 1367450.125 (1367450.125)	prob 0.921 (0.9211)	GS 34.594 (34.594)	mem 43.298
Train: [1][179/750]	BT 0.092 (1.209)	DT 0.002 (1.070)	loss 10.191 (10.191)	gnorm 1270883.875 (1270883.875)	prob 0.854 (0.8544)	GS 42.766 (42.766)	mem 43.234
Train: [1][180/750]	BT 0.165 (1.204)	DT 0.001 (1.064)	loss 10.158 (10.158)	gnorm 1276500.875 (1276500.875)	prob 1.530 (1.5298)	GS 36.125 (36.125)	mem 43.249
Train: [1][181/750]	BT 0.154 (1.198)	DT 0.002 (1.058)	loss 10.381 (10.381)	gnorm 1186900.625 (1186900.625)	prob 1.385 (1.3849)	GS 35.016 (35.016)	mem 43.282
Train: [1][182/750]	BT 0.103 (1.192)	DT 0.002 (1.053)	loss 10.492 (10.492)	gnorm 1234488.375 (1234488.375)	prob 1.173 (1.1729)	GS 32.578 (32.578)	mem 43.283
Train: [1][183/750]	BT 0.081 (1.186)	DT 0.002 (1.047)	loss 10.331 (10.331)	gnorm 1276911.625 (1276911.625)	prob 0.994 (0.9940)	GS 36.922 (36.922)	mem 43.284
Train: [1][184/750]	BT 15.166 (1.262)	DT 15.082 (1.123)	loss 10.084 (10.084)	gnorm 1266714.000 (1266714.000)	prob 1.318 (1.3175)	GS 36.141 (36.141)	mem 43.519
Train: [1][185/750]	BT 0.094 (1.255)	DT 0.003 (1.117)	loss 10.956 (10.956)	gnorm 1249092.625 (1249092.625)	prob 0.427 (0.4270)	GS 29.234 (29.234)	mem 43.450
Train: [1][186/750]	BT 0.094 (1.249)	DT 0.007 (1.111)	loss 10.388 (10.388)	gnorm 1247621.000 (1247621.000)	prob 0.734 (0.7342)	GS 30.797 (30.797)	mem 43.450
Train: [1][187/750]	BT 0.081 (1.243)	DT 0.002 (1.105)	loss 10.401 (10.401)	gnorm 1204896.625 (1204896.625)	prob 0.627 (0.6267)	GS 30.062 (30.062)	mem 43.450
Train: [1][188/750]	BT 0.087 (1.237)	DT 0.003 (1.099)	loss 10.503 (10.503)	gnorm 1286531.500 (1286531.500)	prob 0.467 (0.4673)	GS 27.484 (27.484)	mem 43.451
Train: [1][189/750]	BT 0.114 (1.231)	DT 0.003 (1.094)	loss 10.106 (10.106)	gnorm 1168842.750 (1168842.750)	prob 0.651 (0.6513)	GS 30.562 (30.562)	mem 43.389
Train: [1][190/750]	BT 0.167 (1.225)	DT 0.042 (1.088)	loss 10.788 (10.788)	gnorm 1167376.250 (1167376.250)	prob 0.354 (0.3538)	GS 29.188 (29.188)	mem 43.389
Train: [1][191/750]	BT 0.137 (1.219)	DT 0.011 (1.082)	loss 10.127 (10.127)	gnorm 1093002.125 (1093002.125)	prob 1.197 (1.1973)	GS 33.750 (33.750)	mem 43.389
Train: [1][192/750]	BT 0.102 (1.214)	DT 0.001 (1.077)	loss 9.808 (9.808)	gnorm 1053195.250 (1053195.250)	prob 1.716 (1.7162)	GS 33.938 (33.938)	mem 43.389
Train: [1][193/750]	BT 0.133 (1.208)	DT 0.006 (1.071)	loss 10.640 (10.640)	gnorm 1354482.375 (1354482.375)	prob 1.197 (1.1969)	GS 29.938 (29.938)	mem 43.424
Train: [1][194/750]	BT 0.137 (1.203)	DT 0.010 (1.066)	loss 10.328 (10.328)	gnorm 1419881.750 (1419881.750)	prob 0.779 (0.7788)	GS 29.938 (29.938)	mem 43.397
Train: [1][195/750]	BT 0.091 (1.197)	DT 0.002 (1.060)	loss 10.446 (10.446)	gnorm 1327727.625 (1327727.625)	prob 2.481 (2.4810)	GS 34.734 (34.734)	mem 43.397
Train: [1][196/750]	BT 9.566 (1.240)	DT 9.479 (1.103)	loss 10.330 (10.330)	gnorm 1288903.875 (1288903.875)	prob 1.910 (1.9099)	GS 31.344 (31.344)	mem 43.458
Train: [1][197/750]	BT 0.079 (1.234)	DT 0.002 (1.098)	loss 9.818 (9.818)	gnorm 1204256.875 (1204256.875)	prob 2.717 (2.7169)	GS 31.062 (31.062)	mem 43.457
Train: [1][198/750]	BT 0.094 (1.228)	DT 0.001 (1.092)	loss 10.202 (10.202)	gnorm 1204666.125 (1204666.125)	prob 1.185 (1.1853)	GS 33.969 (33.969)	mem 43.457
Train: [1][199/750]	BT 0.097 (1.222)	DT 0.002 (1.087)	loss 9.990 (9.990)	gnorm 1270541.125 (1270541.125)	prob 2.144 (2.1437)	GS 40.938 (40.938)	mem 43.457
Train: [1][200/750]	BT 0.123 (1.217)	DT 0.002 (1.081)	loss 10.056 (10.056)	gnorm 1217850.750 (1217850.750)	prob 2.100 (2.0999)	GS 30.797 (30.797)	mem 43.501
Train: [1][201/750]	BT 0.289 (1.212)	DT 0.002 (1.076)	loss 10.328 (10.328)	gnorm 1270812.000 (1270812.000)	prob 2.537 (2.5368)	GS 31.328 (31.328)	mem 43.575
Train: [1][202/750]	BT 0.144 (1.207)	DT 0.027 (1.071)	loss 10.134 (10.134)	gnorm 1295494.750 (1295494.750)	prob 2.126 (2.1256)	GS 34.781 (34.781)	mem 43.457
Train: [1][203/750]	BT 0.203 (1.202)	DT 0.027 (1.066)	loss 10.603 (10.603)	gnorm 1246271.000 (1246271.000)	prob 1.628 (1.6283)	GS 34.484 (34.484)	mem 43.515
Train: [1][204/750]	BT 0.274 (1.197)	DT 0.003 (1.060)	loss 10.147 (10.147)	gnorm 1121630.125 (1121630.125)	prob 1.796 (1.7961)	GS 35.266 (35.266)	mem 43.471
Train: [1][205/750]	BT 0.113 (1.192)	DT 0.014 (1.055)	loss 10.092 (10.092)	gnorm 1190923.750 (1190923.750)	prob 1.796 (1.7960)	GS 28.766 (28.766)	mem 43.471
Train: [1][206/750]	BT 0.101 (1.187)	DT 0.002 (1.050)	loss 10.022 (10.022)	gnorm 1167370.125 (1167370.125)	prob 2.234 (2.2340)	GS 35.422 (35.422)	mem 43.478
Train: [1][207/750]	BT 0.135 (1.182)	DT 0.007 (1.045)	loss 10.240 (10.240)	gnorm 1205043.625 (1205043.625)	prob 2.540 (2.5399)	GS 34.594 (34.594)	mem 43.516
Train: [1][208/750]	BT 9.930 (1.224)	DT 9.812 (1.087)	loss 10.009 (10.009)	gnorm 1057545.500 (1057545.500)	prob 2.123 (2.1226)	GS 29.875 (29.875)	mem 43.533
Train: [1][209/750]	BT 0.079 (1.218)	DT 0.002 (1.082)	loss 10.012 (10.012)	gnorm 1178889.500 (1178889.500)	prob 3.141 (3.1410)	GS 32.688 (32.688)	mem 43.533
Train: [1][210/750]	BT 0.078 (1.213)	DT 0.001 (1.077)	loss 10.646 (10.646)	gnorm 1289838.875 (1289838.875)	prob 2.381 (2.3806)	GS 32.969 (32.969)	mem 43.622
Train: [1][211/750]	BT 0.240 (1.208)	DT 0.002 (1.072)	loss 10.591 (10.591)	gnorm 1248456.625 (1248456.625)	prob 2.538 (2.5380)	GS 29.594 (29.594)	mem 43.609
Train: [1][212/750]	BT 0.101 (1.203)	DT 0.008 (1.067)	loss 10.094 (10.094)	gnorm 1231818.250 (1231818.250)	prob 2.779 (2.7788)	GS 34.156 (34.156)	mem 43.536
Train: [1][213/750]	BT 0.170 (1.198)	DT 0.003 (1.062)	loss 10.274 (10.274)	gnorm 1093341.250 (1093341.250)	prob 2.029 (2.0294)	GS 29.984 (29.984)	mem 43.535
Train: [1][214/750]	BT 0.077 (1.193)	DT 0.002 (1.057)	loss 10.535 (10.535)	gnorm 1262991.375 (1262991.375)	prob 2.391 (2.3908)	GS 34.500 (34.500)	mem 43.536
Train: [1][215/750]	BT 0.182 (1.188)	DT 0.001 (1.052)	loss 10.394 (10.394)	gnorm 1265944.500 (1265944.500)	prob 3.052 (3.0517)	GS 31.500 (31.500)	mem 43.613
Train: [1][216/750]	BT 1.432 (1.189)	DT 1.342 (1.053)	loss 10.070 (10.070)	gnorm 1254862.375 (1254862.375)	prob 2.609 (2.6085)	GS 33.750 (33.750)	mem 43.542
Train: [1][217/750]	BT 0.136 (1.184)	DT 0.002 (1.048)	loss 10.304 (10.304)	gnorm 1208010.125 (1208010.125)	prob 3.121 (3.1214)	GS 33.516 (33.516)	mem 43.543
Train: [1][218/750]	BT 0.234 (1.180)	DT 0.002 (1.044)	loss 10.338 (10.338)	gnorm 1194877.000 (1194877.000)	prob 2.616 (2.6161)	GS 28.531 (28.531)	mem 43.608
Train: [1][219/750]	BT 0.165 (1.175)	DT 0.014 (1.039)	loss 10.922 (10.922)	gnorm 1341718.250 (1341718.250)	prob 1.366 (1.3655)	GS 28.844 (28.844)	mem 43.609
Train: [1][220/750]	BT 11.372 (1.222)	DT 11.229 (1.085)	loss 9.839 (9.839)	gnorm 1199429.750 (1199429.750)	prob 1.809 (1.8087)	GS 35.938 (35.938)	mem 43.500
Train: [1][221/750]	BT 0.075 (1.217)	DT 0.001 (1.080)	loss 10.223 (10.223)	gnorm 1107700.125 (1107700.125)	prob 2.381 (2.3806)	GS 31.891 (31.891)	mem 43.513
Train: [1][222/750]	BT 0.105 (1.212)	DT 0.002 (1.075)	loss 10.353 (10.353)	gnorm 1254600.750 (1254600.750)	prob 2.093 (2.0933)	GS 33.969 (33.969)	mem 43.591
Train: [1][223/750]	BT 0.152 (1.207)	DT 0.003 (1.071)	loss 10.331 (10.331)	gnorm 1249153.500 (1249153.500)	prob 2.173 (2.1725)	GS 31.297 (31.297)	mem 43.668
Train: [1][224/750]	BT 0.142 (1.202)	DT 0.003 (1.066)	loss 10.202 (10.202)	gnorm 1066951.250 (1066951.250)	prob 2.096 (2.0960)	GS 32.188 (32.188)	mem 43.595
Train: [1][225/750]	BT 0.089 (1.197)	DT 0.002 (1.061)	loss 10.427 (10.427)	gnorm 1245433.500 (1245433.500)	prob 1.657 (1.6570)	GS 40.922 (40.922)	mem 43.530
Train: [1][226/750]	BT 0.118 (1.192)	DT 0.002 (1.056)	loss 9.869 (9.869)	gnorm 1196260.250 (1196260.250)	prob 2.716 (2.7164)	GS 37.641 (37.641)	mem 43.580
Train: [1][227/750]	BT 0.130 (1.188)	DT 0.017 (1.052)	loss 10.535 (10.535)	gnorm 1266638.125 (1266638.125)	prob 2.514 (2.5136)	GS 32.781 (32.781)	mem 43.539
Train: [1][228/750]	BT 3.012 (1.196)	DT 2.884 (1.060)	loss 10.769 (10.769)	gnorm 1160453.250 (1160453.250)	prob 1.650 (1.6499)	GS 29.953 (29.953)	mem 43.437
Train: [1][229/750]	BT 0.083 (1.191)	DT 0.001 (1.055)	loss 10.204 (10.204)	gnorm 1107769.000 (1107769.000)	prob 2.750 (2.7498)	GS 35.359 (35.359)	mem 43.468
Train: [1][230/750]	BT 0.082 (1.186)	DT 0.002 (1.051)	loss 10.525 (10.525)	gnorm 1238549.750 (1238549.750)	prob 2.434 (2.4337)	GS 33.000 (33.000)	mem 43.437
Train: [1][231/750]	BT 0.072 (1.181)	DT 0.001 (1.046)	loss 10.619 (10.619)	gnorm 1138035.000 (1138035.000)	prob 1.829 (1.8289)	GS 34.875 (34.875)	mem 43.437
Train: [1][232/750]	BT 11.773 (1.227)	DT 11.682 (1.092)	loss 10.267 (10.267)	gnorm 1230206.250 (1230206.250)	prob 2.073 (2.0730)	GS 36.266 (36.266)	mem 43.541
Train: [1][233/750]	BT 0.188 (1.222)	DT 0.002 (1.087)	loss 10.250 (10.250)	gnorm 1133266.250 (1133266.250)	prob 1.498 (1.4983)	GS 30.672 (30.672)	mem 43.482
Train: [1][234/750]	BT 0.144 (1.218)	DT 0.003 (1.083)	loss 10.435 (10.435)	gnorm 1184210.750 (1184210.750)	prob 1.036 (1.0357)	GS 32.766 (32.766)	mem 43.484
Train: [1][235/750]	BT 0.135 (1.213)	DT 0.034 (1.078)	loss 10.327 (10.327)	gnorm 1272089.875 (1272089.875)	prob 1.572 (1.5723)	GS 31.406 (31.406)	mem 43.488
Train: [1][236/750]	BT 0.094 (1.208)	DT 0.007 (1.074)	loss 10.026 (10.026)	gnorm 1228088.625 (1228088.625)	prob 1.232 (1.2321)	GS 30.969 (30.969)	mem 43.496
Train: [1][237/750]	BT 0.160 (1.204)	DT 0.001 (1.069)	loss 10.053 (10.053)	gnorm 1127673.500 (1127673.500)	prob 1.272 (1.2717)	GS 30.500 (30.500)	mem 43.534
Train: [1][238/750]	BT 0.220 (1.200)	DT 0.041 (1.065)	loss 9.732 (9.732)	gnorm 1082170.875 (1082170.875)	prob 1.545 (1.5450)	GS 30.062 (30.062)	mem 43.537
Train: [1][239/750]	BT 0.198 (1.196)	DT 0.001 (1.060)	loss 10.326 (10.326)	gnorm 1177877.125 (1177877.125)	prob 0.475 (0.4754)	GS 30.672 (30.672)	mem 43.578
Train: [1][240/750]	BT 0.206 (1.192)	DT 0.023 (1.056)	loss 9.917 (9.917)	gnorm 1111637.500 (1111637.500)	prob 0.897 (0.8966)	GS 36.297 (36.297)	mem 43.540
Train: [1][241/750]	BT 0.088 (1.187)	DT 0.001 (1.052)	loss 10.479 (10.479)	gnorm 1252269.000 (1252269.000)	prob 0.537 (0.5367)	GS 31.969 (31.969)	mem 43.544
Train: [1][242/750]	BT 0.268 (1.183)	DT 0.003 (1.047)	loss 10.418 (10.418)	gnorm 1275830.125 (1275830.125)	prob 0.276 (0.2757)	GS 33.094 (33.094)	mem 43.544
Train: [1][243/750]	BT 0.234 (1.179)	DT 0.013 (1.043)	loss 10.390 (10.390)	gnorm 1186101.500 (1186101.500)	prob 1.299 (1.2991)	GS 33.953 (33.953)	mem 43.589
Train: [1][244/750]	BT 13.683 (1.231)	DT 13.605 (1.095)	loss 10.290 (10.290)	gnorm 1272467.625 (1272467.625)	prob 1.307 (1.3066)	GS 32.156 (32.156)	mem 43.396
Train: [1][245/750]	BT 0.068 (1.226)	DT 0.002 (1.090)	loss 10.212 (10.212)	gnorm 1139351.000 (1139351.000)	prob 2.364 (2.3640)	GS 30.188 (30.188)	mem 43.397
Train: [1][246/750]	BT 0.115 (1.221)	DT 0.001 (1.086)	loss 10.534 (10.534)	gnorm 1185980.250 (1185980.250)	prob 1.543 (1.5434)	GS 33.203 (33.203)	mem 43.445
Train: [1][247/750]	BT 0.130 (1.217)	DT 0.002 (1.081)	loss 10.195 (10.195)	gnorm 1179264.750 (1179264.750)	prob 2.137 (2.1374)	GS 33.156 (33.156)	mem 43.534
Train: [1][248/750]	BT 0.122 (1.212)	DT 0.002 (1.077)	loss 11.067 (11.067)	gnorm 1277642.875 (1277642.875)	prob 1.273 (1.2733)	GS 34.859 (34.859)	mem 43.402
Train: [1][249/750]	BT 0.093 (1.208)	DT 0.007 (1.073)	loss 10.353 (10.353)	gnorm 1231062.625 (1231062.625)	prob 2.517 (2.5171)	GS 29.484 (29.484)	mem 43.401
Train: [1][250/750]	BT 0.173 (1.204)	DT 0.001 (1.068)	loss 10.032 (10.032)	gnorm 1163114.875 (1163114.875)	prob 2.789 (2.7894)	GS 35.375 (35.375)	mem 43.442
Train: [1][251/750]	BT 0.178 (1.200)	DT 0.013 (1.064)	loss 10.431 (10.431)	gnorm 1199140.375 (1199140.375)	prob 2.693 (2.6928)	GS 29.953 (29.953)	mem 43.447
Train: [1][252/750]	BT 0.101 (1.195)	DT 0.008 (1.060)	loss 10.319 (10.319)	gnorm 1179776.250 (1179776.250)	prob 2.299 (2.2994)	GS 36.656 (36.656)	mem 43.447
Train: [1][253/750]	BT 0.092 (1.191)	DT 0.005 (1.056)	loss 10.437 (10.437)	gnorm 1200915.875 (1200915.875)	prob 3.084 (3.0845)	GS 32.953 (32.953)	mem 43.448
Train: [1][254/750]	BT 0.143 (1.187)	DT 0.002 (1.052)	loss 10.962 (10.962)	gnorm 1256666.625 (1256666.625)	prob 2.031 (2.0312)	GS 31.578 (31.578)	mem 43.470
Train: [1][255/750]	BT 0.112 (1.183)	DT 0.013 (1.048)	loss 10.062 (10.062)	gnorm 1151880.375 (1151880.375)	prob 3.283 (3.2826)	GS 28.516 (28.516)	mem 43.546
Train: [1][256/750]	BT 10.034 (1.217)	DT 9.947 (1.082)	loss 10.661 (10.661)	gnorm 1269785.250 (1269785.250)	prob 2.076 (2.0760)	GS 32.156 (32.156)	mem 43.577
Train: [1][257/750]	BT 0.087 (1.213)	DT 0.003 (1.078)	loss 10.513 (10.513)	gnorm 1267902.875 (1267902.875)	prob 2.515 (2.5151)	GS 31.391 (31.391)	mem 43.633
Train: [1][258/750]	BT 0.160 (1.209)	DT 0.003 (1.074)	loss 10.684 (10.684)	gnorm 1171590.875 (1171590.875)	prob 1.884 (1.8841)	GS 34.422 (34.422)	mem 43.619
Train: [1][259/750]	BT 0.100 (1.204)	DT 0.003 (1.070)	loss 10.377 (10.377)	gnorm 1179241.125 (1179241.125)	prob 2.080 (2.0802)	GS 32.359 (32.359)	mem 43.549
Train: [1][260/750]	BT 0.091 (1.200)	DT 0.001 (1.066)	loss 10.325 (10.325)	gnorm 1283847.375 (1283847.375)	prob 1.911 (1.9111)	GS 31.766 (31.766)	mem 43.549
Train: [1][261/750]	BT 0.096 (1.196)	DT 0.002 (1.062)	loss 9.785 (9.785)	gnorm 1084554.250 (1084554.250)	prob 2.186 (2.1860)	GS 30.672 (30.672)	mem 43.549
Train: [1][262/750]	BT 0.122 (1.192)	DT 0.002 (1.058)	loss 10.040 (10.040)	gnorm 1091173.000 (1091173.000)	prob 2.213 (2.2133)	GS 31.750 (31.750)	mem 43.549
Train: [1][263/750]	BT 0.118 (1.188)	DT 0.002 (1.054)	loss 10.151 (10.151)	gnorm 1176965.500 (1176965.500)	prob 2.287 (2.2874)	GS 34.859 (34.859)	mem 43.551
Train: [1][264/750]	BT 2.965 (1.194)	DT 2.820 (1.060)	loss 9.651 (9.651)	gnorm 1100236.750 (1100236.750)	prob 2.383 (2.3829)	GS 36.828 (36.828)	mem 43.556
Train: [1][265/750]	BT 0.108 (1.190)	DT 0.013 (1.056)	loss 10.392 (10.392)	gnorm 1127997.875 (1127997.875)	prob 2.441 (2.4406)	GS 31.500 (31.500)	mem 43.559
Train: [1][266/750]	BT 0.252 (1.187)	DT 0.010 (1.052)	loss 9.988 (9.988)	gnorm 1067397.375 (1067397.375)	prob 2.628 (2.6281)	GS 31.359 (31.359)	mem 43.560
Train: [1][267/750]	BT 0.247 (1.183)	DT 0.003 (1.048)	loss 9.962 (9.962)	gnorm 1127989.625 (1127989.625)	prob 2.843 (2.8434)	GS 34.688 (34.688)	mem 43.540
Train: [1][268/750]	BT 6.218 (1.202)	DT 6.094 (1.067)	loss 10.908 (10.908)	gnorm 1297276.250 (1297276.250)	prob 1.835 (1.8350)	GS 29.141 (29.141)	mem 43.530
Train: [1][269/750]	BT 0.126 (1.198)	DT 0.008 (1.063)	loss 10.487 (10.487)	gnorm 1127393.250 (1127393.250)	prob 2.104 (2.1041)	GS 31.266 (31.266)	mem 43.531
Train: [1][270/750]	BT 0.214 (1.194)	DT 0.007 (1.059)	loss 11.045 (11.045)	gnorm 1221890.875 (1221890.875)	prob 1.543 (1.5431)	GS 33.875 (33.875)	mem 43.534
Train: [1][271/750]	BT 0.220 (1.191)	DT 0.019 (1.056)	loss 10.417 (10.417)	gnorm 1120960.375 (1120960.375)	prob 2.175 (2.1745)	GS 32.766 (32.766)	mem 43.598
Train: [1][272/750]	BT 0.138 (1.187)	DT 0.002 (1.052)	loss 10.319 (10.319)	gnorm 1028240.438 (1028240.438)	prob 2.137 (2.1373)	GS 32.547 (32.547)	mem 43.641
Train: [1][273/750]	BT 0.093 (1.183)	DT 0.004 (1.048)	loss 10.292 (10.292)	gnorm 1125998.375 (1125998.375)	prob 2.996 (2.9961)	GS 33.484 (33.484)	mem 43.584
Train: [1][274/750]	BT 0.124 (1.179)	DT 0.004 (1.044)	loss 10.312 (10.312)	gnorm 1156383.375 (1156383.375)	prob 2.401 (2.4008)	GS 31.391 (31.391)	mem 43.534
Train: [1][275/750]	BT 0.220 (1.176)	DT 0.003 (1.040)	loss 10.939 (10.939)	gnorm 1258234.750 (1258234.750)	prob 2.860 (2.8598)	GS 35.422 (35.422)	mem 43.534
Train: [1][276/750]	BT 4.968 (1.189)	DT 4.798 (1.054)	loss 9.896 (9.896)	gnorm 1331828.875 (1331828.875)	prob 3.330 (3.3296)	GS 40.812 (40.812)	mem 43.540
Train: [1][277/750]	BT 0.163 (1.186)	DT 0.027 (1.050)	loss 10.085 (10.085)	gnorm 1170277.750 (1170277.750)	prob 3.401 (3.4009)	GS 32.000 (32.000)	mem 43.540
Train: [1][278/750]	BT 0.130 (1.182)	DT 0.006 (1.046)	loss 10.064 (10.064)	gnorm 1077362.625 (1077362.625)	prob 3.181 (3.1806)	GS 32.234 (32.234)	mem 43.541
Train: [1][279/750]	BT 0.187 (1.178)	DT 0.024 (1.043)	loss 10.618 (10.618)	gnorm 1187105.375 (1187105.375)	prob 3.746 (3.7456)	GS 32.875 (32.875)	mem 43.545
Train: [1][280/750]	BT 5.895 (1.195)	DT 5.753 (1.060)	loss 10.272 (10.272)	gnorm 1199324.625 (1199324.625)	prob 3.057 (3.0569)	GS 33.203 (33.203)	mem 43.552
Train: [1][281/750]	BT 0.131 (1.191)	DT 0.010 (1.056)	loss 10.232 (10.232)	gnorm 1123275.500 (1123275.500)	prob 3.290 (3.2899)	GS 31.719 (31.719)	mem 43.574
Train: [1][282/750]	BT 0.141 (1.188)	DT 0.001 (1.052)	loss 10.151 (10.151)	gnorm 1092333.500 (1092333.500)	prob 3.407 (3.4073)	GS 31.531 (31.531)	mem 43.658
Train: [1][283/750]	BT 0.115 (1.184)	DT 0.006 (1.048)	loss 10.401 (10.401)	gnorm 1114799.375 (1114799.375)	prob 3.427 (3.4267)	GS 30.891 (30.891)	mem 43.553
Train: [1][284/750]	BT 0.177 (1.180)	DT 0.008 (1.045)	loss 10.310 (10.310)	gnorm 1145030.500 (1145030.500)	prob 3.331 (3.3315)	GS 32.156 (32.156)	mem 43.553
Train: [1][285/750]	BT 0.099 (1.177)	DT 0.012 (1.041)	loss 10.248 (10.248)	gnorm 1130423.500 (1130423.500)	prob 3.660 (3.6598)	GS 27.953 (27.953)	mem 43.553
Train: [1][286/750]	BT 0.197 (1.173)	DT 0.002 (1.038)	loss 10.407 (10.407)	gnorm 1068227.750 (1068227.750)	prob 3.283 (3.2834)	GS 33.781 (33.781)	mem 43.571
Train: [1][287/750]	BT 0.199 (1.170)	DT 0.003 (1.034)	loss 10.625 (10.625)	gnorm 1103447.625 (1103447.625)	prob 2.789 (2.7887)	GS 33.328 (33.328)	mem 43.573
Train: [1][288/750]	BT 6.425 (1.188)	DT 6.204 (1.052)	loss 10.119 (10.119)	gnorm 1161599.500 (1161599.500)	prob 3.298 (3.2984)	GS 33.562 (33.562)	mem 43.514
Train: [1][289/750]	BT 0.117 (1.184)	DT 0.002 (1.048)	loss 10.190 (10.190)	gnorm 1197061.250 (1197061.250)	prob 3.330 (3.3296)	GS 39.859 (39.859)	mem 43.552
Train: [1][290/750]	BT 0.101 (1.181)	DT 0.007 (1.045)	loss 10.348 (10.348)	gnorm 1179253.875 (1179253.875)	prob 3.625 (3.6253)	GS 32.344 (32.344)	mem 43.513
Train: [1][291/750]	BT 0.084 (1.177)	DT 0.001 (1.041)	loss 10.382 (10.382)	gnorm 1392305.875 (1392305.875)	prob 3.430 (3.4302)	GS 35.719 (35.719)	mem 43.514
Train: [1][292/750]	BT 5.972 (1.193)	DT 5.828 (1.057)	loss 10.239 (10.239)	gnorm 1156619.250 (1156619.250)	prob 3.524 (3.5238)	GS 30.547 (30.547)	mem 43.633
Train: [1][293/750]	BT 0.154 (1.190)	DT 0.010 (1.054)	loss 10.835 (10.835)	gnorm 1274772.750 (1274772.750)	prob 3.135 (3.1350)	GS 30.969 (30.969)	mem 43.575
Train: [1][294/750]	BT 0.150 (1.186)	DT 0.002 (1.050)	loss 10.196 (10.196)	gnorm 1121453.250 (1121453.250)	prob 3.281 (3.2815)	GS 32.922 (32.922)	mem 43.511
Train: [1][295/750]	BT 0.084 (1.182)	DT 0.005 (1.047)	loss 10.342 (10.342)	gnorm 1077906.000 (1077906.000)	prob 3.594 (3.5943)	GS 28.984 (28.984)	mem 43.511
Train: [1][296/750]	BT 0.157 (1.179)	DT 0.003 (1.043)	loss 10.035 (10.035)	gnorm 1049793.625 (1049793.625)	prob 3.586 (3.5863)	GS 30.094 (30.094)	mem 43.512
Train: [1][297/750]	BT 0.116 (1.175)	DT 0.012 (1.040)	loss 10.338 (10.338)	gnorm 1146560.500 (1146560.500)	prob 3.671 (3.6707)	GS 33.375 (33.375)	mem 43.512
Train: [1][298/750]	BT 0.085 (1.172)	DT 0.001 (1.036)	loss 10.539 (10.539)	gnorm 1218088.625 (1218088.625)	prob 3.226 (3.2261)	GS 34.219 (34.219)	mem 43.513
Train: [1][299/750]	BT 0.251 (1.169)	DT 0.002 (1.033)	loss 10.144 (10.144)	gnorm 1129763.125 (1129763.125)	prob 3.597 (3.5973)	GS 30.219 (30.219)	mem 43.512
Train: [1][300/750]	BT 4.944 (1.181)	DT 4.849 (1.046)	loss 9.829 (9.829)	gnorm 1057066.500 (1057066.500)	prob 3.782 (3.7818)	GS 30.641 (30.641)	mem 43.591
Train: [1][301/750]	BT 0.161 (1.178)	DT 0.004 (1.042)	loss 10.251 (10.251)	gnorm 1124840.750 (1124840.750)	prob 2.444 (2.4439)	GS 33.234 (33.234)	mem 43.558
Train: [1][302/750]	BT 0.206 (1.175)	DT 0.003 (1.039)	loss 10.143 (10.143)	gnorm 1067916.250 (1067916.250)	prob 2.576 (2.5760)	GS 33.250 (33.250)	mem 43.614
Train: [1][303/750]	BT 0.107 (1.171)	DT 0.002 (1.035)	loss 10.526 (10.526)	gnorm 1151271.125 (1151271.125)	prob 3.057 (3.0570)	GS 40.469 (40.469)	mem 43.760
Train: [1][304/750]	BT 4.912 (1.183)	DT 4.841 (1.048)	loss 10.310 (10.310)	gnorm 1161608.375 (1161608.375)	prob 2.216 (2.2162)	GS 31.422 (31.422)	mem 43.628
Train: [1][305/750]	BT 0.149 (1.180)	DT 0.002 (1.044)	loss 10.326 (10.326)	gnorm 1062035.750 (1062035.750)	prob 2.598 (2.5985)	GS 30.719 (30.719)	mem 43.855
Train: [1][306/750]	BT 0.178 (1.177)	DT 0.013 (1.041)	loss 9.987 (9.987)	gnorm 1021059.625 (1021059.625)	prob 3.003 (3.0027)	GS 36.359 (36.359)	mem 43.654
Train: [1][307/750]	BT 0.134 (1.173)	DT 0.002 (1.038)	loss 10.044 (10.044)	gnorm 1083832.625 (1083832.625)	prob 3.170 (3.1699)	GS 29.844 (29.844)	mem 43.570
Train: [1][308/750]	BT 0.191 (1.170)	DT 0.017 (1.034)	loss 10.347 (10.347)	gnorm 1081622.375 (1081622.375)	prob 2.833 (2.8330)	GS 35.750 (35.750)	mem 43.633
Train: [1][309/750]	BT 0.193 (1.167)	DT 0.009 (1.031)	loss 9.990 (9.990)	gnorm 1084640.625 (1084640.625)	prob 3.695 (3.6948)	GS 33.797 (33.797)	mem 43.579
Train: [1][310/750]	BT 0.148 (1.164)	DT 0.025 (1.028)	loss 10.462 (10.462)	gnorm 1117176.875 (1117176.875)	prob 2.566 (2.5660)	GS 35.844 (35.844)	mem 43.579
Train: [1][311/750]	BT 0.198 (1.161)	DT 0.033 (1.024)	loss 10.262 (10.262)	gnorm 1164252.500 (1164252.500)	prob 3.709 (3.7089)	GS 31.859 (31.859)	mem 43.578
Train: [1][312/750]	BT 11.311 (1.193)	DT 11.242 (1.057)	loss 10.462 (10.462)	gnorm 1032440.875 (1032440.875)	prob 2.976 (2.9755)	GS 33.047 (33.047)	mem 43.622
Train: [1][313/750]	BT 0.145 (1.190)	DT 0.001 (1.054)	loss 9.910 (9.910)	gnorm 1112166.125 (1112166.125)	prob 2.627 (2.6275)	GS 30.109 (30.109)	mem 43.622
Train: [1][314/750]	BT 0.092 (1.186)	DT 0.002 (1.051)	loss 10.065 (10.065)	gnorm 1001175.188 (1001175.188)	prob 3.103 (3.1032)	GS 38.234 (38.234)	mem 43.622
Train: [1][315/750]	BT 0.148 (1.183)	DT 0.001 (1.047)	loss 10.080 (10.080)	gnorm 1040803.750 (1040803.750)	prob 2.688 (2.6875)	GS 30.781 (30.781)	mem 43.670
Train: [1][316/750]	BT 0.085 (1.179)	DT 0.005 (1.044)	loss 10.212 (10.212)	gnorm 1230509.500 (1230509.500)	prob 2.586 (2.5865)	GS 36.719 (36.719)	mem 43.747
Train: [1][317/750]	BT 0.129 (1.176)	DT 0.001 (1.041)	loss 10.545 (10.545)	gnorm 1258201.250 (1258201.250)	prob 3.122 (3.1223)	GS 32.156 (32.156)	mem 43.651
Train: [1][318/750]	BT 0.143 (1.173)	DT 0.001 (1.037)	loss 10.323 (10.323)	gnorm 1094976.125 (1094976.125)	prob 2.703 (2.7030)	GS 32.594 (32.594)	mem 43.715
Train: [1][319/750]	BT 0.141 (1.170)	DT 0.020 (1.034)	loss 10.210 (10.210)	gnorm 1003067.938 (1003067.938)	prob 2.992 (2.9917)	GS 33.625 (33.625)	mem 43.650
Train: [1][320/750]	BT 0.161 (1.166)	DT 0.002 (1.031)	loss 10.491 (10.491)	gnorm 1162166.125 (1162166.125)	prob 2.576 (2.5763)	GS 32.844 (32.844)	mem 43.706
Train: [1][321/750]	BT 0.095 (1.163)	DT 0.012 (1.028)	loss 10.130 (10.130)	gnorm 1089733.625 (1089733.625)	prob 2.600 (2.5996)	GS 30.391 (30.391)	mem 43.640
Train: [1][322/750]	BT 0.060 (1.160)	DT 0.001 (1.025)	loss 10.087 (10.087)	gnorm 1104380.000 (1104380.000)	prob 2.741 (2.7411)	GS 32.062 (32.062)	mem 43.641
Train: [1][323/750]	BT 0.084 (1.156)	DT 0.001 (1.021)	loss 11.011 (11.011)	gnorm 1181144.625 (1181144.625)	prob 2.068 (2.0683)	GS 29.438 (29.438)	mem 43.641
Train: [1][324/750]	BT 10.477 (1.185)	DT 10.360 (1.050)	loss 10.136 (10.136)	gnorm 1125594.000 (1125594.000)	prob 2.458 (2.4580)	GS 35.531 (35.531)	mem 43.664
Train: [1][325/750]	BT 0.131 (1.182)	DT 0.002 (1.047)	loss 10.819 (10.819)	gnorm 1062294.125 (1062294.125)	prob 1.756 (1.7559)	GS 32.328 (32.328)	mem 43.630
Train: [1][326/750]	BT 0.088 (1.179)	DT 0.002 (1.044)	loss 10.092 (10.092)	gnorm 1067559.875 (1067559.875)	prob 3.138 (3.1384)	GS 29.938 (29.938)	mem 43.629
Train: [1][327/750]	BT 0.127 (1.175)	DT 0.002 (1.041)	loss 10.572 (10.572)	gnorm 1013517.688 (1013517.688)	prob 2.050 (2.0504)	GS 34.000 (34.000)	mem 43.629
Train: [1][328/750]	BT 0.183 (1.172)	DT 0.001 (1.037)	loss 10.322 (10.322)	gnorm 1014707.312 (1014707.312)	prob 2.491 (2.4912)	GS 31.781 (31.781)	mem 43.687
Train: [1][329/750]	BT 0.218 (1.169)	DT 0.008 (1.034)	loss 10.110 (10.110)	gnorm 1067972.250 (1067972.250)	prob 2.788 (2.7875)	GS 27.234 (27.234)	mem 43.685
Train: [1][330/750]	BT 0.242 (1.167)	DT 0.025 (1.031)	loss 10.827 (10.827)	gnorm 1107849.250 (1107849.250)	prob 1.295 (1.2950)	GS 29.125 (29.125)	mem 43.626
Train: [1][331/750]	BT 0.154 (1.164)	DT 0.002 (1.028)	loss 10.089 (10.089)	gnorm 1144444.875 (1144444.875)	prob 2.408 (2.4083)	GS 29.516 (29.516)	mem 43.678
Train: [1][332/750]	BT 0.097 (1.160)	DT 0.002 (1.025)	loss 10.200 (10.200)	gnorm 1054080.750 (1054080.750)	prob 1.797 (1.7967)	GS 32.812 (32.812)	mem 43.627
Train: [1][333/750]	BT 0.106 (1.157)	DT 0.002 (1.022)	loss 10.512 (10.512)	gnorm 1046528.938 (1046528.938)	prob 1.703 (1.7030)	GS 26.641 (26.641)	mem 43.627
Train: [1][334/750]	BT 0.333 (1.155)	DT 0.002 (1.019)	loss 10.627 (10.627)	gnorm 997376.938 (997376.938)	prob 1.880 (1.8799)	GS 32.906 (32.906)	mem 43.708
Train: [1][335/750]	BT 0.192 (1.152)	DT 0.016 (1.016)	loss 10.398 (10.398)	gnorm 1074745.250 (1074745.250)	prob 2.403 (2.4030)	GS 28.875 (28.875)	mem 43.623
Train: [1][336/750]	BT 9.773 (1.177)	DT 9.636 (1.042)	loss 10.701 (10.701)	gnorm 1103966.250 (1103966.250)	prob 1.541 (1.5414)	GS 33.234 (33.234)	mem 43.641
Train: [1][337/750]	BT 0.149 (1.174)	DT 0.003 (1.038)	loss 10.280 (10.280)	gnorm 1086843.750 (1086843.750)	prob 3.222 (3.2215)	GS 28.500 (28.500)	mem 43.640
Train: [1][338/750]	BT 0.074 (1.171)	DT 0.002 (1.035)	loss 10.232 (10.232)	gnorm 1115976.000 (1115976.000)	prob 2.719 (2.7186)	GS 36.547 (36.547)	mem 43.691
Train: [1][339/750]	BT 0.122 (1.168)	DT 0.017 (1.032)	loss 9.703 (9.703)	gnorm 1013137.188 (1013137.188)	prob 3.669 (3.6690)	GS 30.484 (30.484)	mem 43.638
Train: [1][340/750]	BT 0.175 (1.165)	DT 0.019 (1.029)	loss 10.639 (10.639)	gnorm 1143750.250 (1143750.250)	prob 2.085 (2.0852)	GS 34.234 (34.234)	mem 43.682
Train: [1][341/750]	BT 0.133 (1.162)	DT 0.002 (1.026)	loss 10.351 (10.351)	gnorm 1017528.000 (1017528.000)	prob 2.791 (2.7914)	GS 31.500 (31.500)	mem 43.637
Train: [1][342/750]	BT 0.136 (1.159)	DT 0.003 (1.023)	loss 10.323 (10.323)	gnorm 1101986.875 (1101986.875)	prob 2.856 (2.8563)	GS 37.438 (37.438)	mem 43.668
Train: [1][343/750]	BT 0.094 (1.156)	DT 0.003 (1.020)	loss 10.144 (10.144)	gnorm 1026704.250 (1026704.250)	prob 2.941 (2.9406)	GS 29.188 (29.188)	mem 43.637
Train: [1][344/750]	BT 0.151 (1.153)	DT 0.002 (1.017)	loss 10.712 (10.712)	gnorm 1136381.500 (1136381.500)	prob 2.922 (2.9216)	GS 41.109 (41.109)	mem 43.637
Train: [1][345/750]	BT 0.183 (1.150)	DT 0.015 (1.015)	loss 10.472 (10.472)	gnorm 1085675.625 (1085675.625)	prob 2.556 (2.5562)	GS 31.484 (31.484)	mem 43.568
Train: [1][346/750]	BT 0.133 (1.147)	DT 0.013 (1.012)	loss 10.533 (10.533)	gnorm 1250128.875 (1250128.875)	prob 2.267 (2.2674)	GS 31.094 (31.094)	mem 43.569
Train: [1][347/750]	BT 0.096 (1.144)	DT 0.002 (1.009)	loss 10.394 (10.394)	gnorm 1154506.250 (1154506.250)	prob 3.164 (3.1641)	GS 31.141 (31.141)	mem 43.566
Train: [1][348/750]	BT 13.496 (1.180)	DT 13.298 (1.044)	loss 10.333 (10.333)	gnorm 1073544.625 (1073544.625)	prob 2.182 (2.1822)	GS 34.688 (34.688)	mem 43.522
Train: [1][349/750]	BT 0.125 (1.177)	DT 0.003 (1.041)	loss 9.995 (9.995)	gnorm 1001900.250 (1001900.250)	prob 2.613 (2.6126)	GS 29.734 (29.734)	mem 43.521
Train: [1][350/750]	BT 0.091 (1.174)	DT 0.002 (1.038)	loss 10.449 (10.449)	gnorm 1057286.375 (1057286.375)	prob 2.219 (2.2193)	GS 30.625 (30.625)	mem 43.521
Train: [1][351/750]	BT 0.116 (1.171)	DT 0.006 (1.035)	loss 10.480 (10.480)	gnorm 1119833.125 (1119833.125)	prob 2.715 (2.7148)	GS 31.172 (31.172)	mem 43.587
Train: [1][352/750]	BT 2.058 (1.173)	DT 1.920 (1.038)	loss 10.484 (10.484)	gnorm 1106079.875 (1106079.875)	prob 1.973 (1.9726)	GS 38.266 (38.266)	mem 43.789
Train: [1][353/750]	BT 0.218 (1.170)	DT 0.038 (1.035)	loss 10.315 (10.315)	gnorm 1074552.750 (1074552.750)	prob 2.230 (2.2301)	GS 28.609 (28.609)	mem 43.549
Train: [1][354/750]	BT 0.195 (1.168)	DT 0.044 (1.032)	loss 10.292 (10.292)	gnorm 1099626.875 (1099626.875)	prob 1.626 (1.6261)	GS 34.984 (34.984)	mem 43.549
Train: [1][355/750]	BT 0.092 (1.165)	DT 0.002 (1.029)	loss 10.817 (10.817)	gnorm 1151566.625 (1151566.625)	prob 1.755 (1.7553)	GS 26.688 (26.688)	mem 43.550
Train: [1][356/750]	BT 0.128 (1.162)	DT 0.002 (1.026)	loss 10.818 (10.818)	gnorm 1008544.875 (1008544.875)	prob 1.221 (1.2205)	GS 31.750 (31.750)	mem 43.551
Train: [1][357/750]	BT 0.269 (1.159)	DT 0.024 (1.023)	loss 10.015 (10.015)	gnorm 1129804.625 (1129804.625)	prob 2.323 (2.3226)	GS 29.922 (29.922)	mem 43.707
Train: [1][358/750]	BT 0.243 (1.157)	DT 0.002 (1.021)	loss 10.353 (10.353)	gnorm 1188667.625 (1188667.625)	prob 1.422 (1.4217)	GS 33.719 (33.719)	mem 43.581
Train: [1][359/750]	BT 0.127 (1.154)	DT 0.002 (1.018)	loss 10.366 (10.366)	gnorm 1142798.500 (1142798.500)	prob 2.073 (2.0731)	GS 31.969 (31.969)	mem 43.551
Train: [1][360/750]	BT 7.609 (1.172)	DT 7.520 (1.036)	loss 10.237 (10.237)	gnorm 1271101.875 (1271101.875)	prob 1.469 (1.4685)	GS 29.656 (29.656)	mem 43.600
Train: [1][361/750]	BT 0.193 (1.169)	DT 0.040 (1.033)	loss 10.814 (10.814)	gnorm 1185979.500 (1185979.500)	prob 1.560 (1.5599)	GS 33.938 (33.938)	mem 43.605
Train: [1][362/750]	BT 0.093 (1.166)	DT 0.002 (1.030)	loss 10.039 (10.039)	gnorm 1078504.875 (1078504.875)	prob 1.413 (1.4133)	GS 30.734 (30.734)	mem 43.650
Train: [1][363/750]	BT 0.162 (1.163)	DT 0.002 (1.027)	loss 9.875 (9.875)	gnorm 1011032.438 (1011032.438)	prob 2.409 (2.4091)	GS 36.469 (36.469)	mem 43.620
Train: [1][364/750]	BT 2.133 (1.166)	DT 1.943 (1.030)	loss 10.110 (10.110)	gnorm 1072658.625 (1072658.625)	prob 2.166 (2.1659)	GS 28.016 (28.016)	mem 43.720
Train: [1][365/750]	BT 0.206 (1.163)	DT 0.008 (1.027)	loss 10.117 (10.117)	gnorm 1110435.250 (1110435.250)	prob 2.883 (2.8832)	GS 31.406 (31.406)	mem 43.699
Train: [1][366/750]	BT 0.199 (1.161)	DT 0.001 (1.024)	loss 10.038 (10.038)	gnorm 940145.938 (940145.938)	prob 2.890 (2.8895)	GS 32.594 (32.594)	mem 43.630
Train: [1][367/750]	BT 0.120 (1.158)	DT 0.020 (1.022)	loss 10.495 (10.495)	gnorm 1033134.812 (1033134.812)	prob 2.079 (2.0788)	GS 27.000 (27.000)	mem 43.561
Train: [1][368/750]	BT 0.124 (1.155)	DT 0.001 (1.019)	loss 10.432 (10.432)	gnorm 1122234.500 (1122234.500)	prob 2.195 (2.1949)	GS 32.938 (32.938)	mem 43.490
Train: [1][369/750]	BT 0.169 (1.152)	DT 0.012 (1.016)	loss 10.798 (10.798)	gnorm 1128566.875 (1128566.875)	prob 1.860 (1.8597)	GS 34.219 (34.219)	mem 43.491
Train: [1][370/750]	BT 0.241 (1.150)	DT 0.006 (1.013)	loss 9.993 (9.993)	gnorm 1101512.500 (1101512.500)	prob 1.694 (1.6935)	GS 31.953 (31.953)	mem 43.517
Train: [1][371/750]	BT 0.142 (1.147)	DT 0.018 (1.011)	loss 9.929 (9.929)	gnorm 949575.000 (949575.000)	prob 2.650 (2.6503)	GS 28.344 (28.344)	mem 43.655
Train: [1][372/750]	BT 6.970 (1.163)	DT 6.858 (1.026)	loss 10.285 (10.285)	gnorm 921163.750 (921163.750)	prob 2.189 (2.1891)	GS 34.172 (34.172)	mem 43.616
Train: [1][373/750]	BT 0.162 (1.160)	DT 0.024 (1.024)	loss 9.929 (9.929)	gnorm 1022000.250 (1022000.250)	prob 2.345 (2.3447)	GS 30.344 (30.344)	mem 43.597
Train: [1][374/750]	BT 0.206 (1.158)	DT 0.001 (1.021)	loss 9.778 (9.778)	gnorm 1043072.438 (1043072.438)	prob 2.678 (2.6781)	GS 35.547 (35.547)	mem 43.665
Train: [1][375/750]	BT 0.096 (1.155)	DT 0.022 (1.018)	loss 10.175 (10.175)	gnorm 925842.625 (925842.625)	prob 2.549 (2.5487)	GS 29.656 (29.656)	mem 43.638
Train: [1][376/750]	BT 3.077 (1.160)	DT 2.948 (1.023)	loss 10.414 (10.414)	gnorm 957787.875 (957787.875)	prob 2.395 (2.3949)	GS 30.297 (30.297)	mem 43.590
Train: [1][377/750]	BT 0.068 (1.157)	DT 0.002 (1.021)	loss 9.996 (9.996)	gnorm 997047.688 (997047.688)	prob 2.943 (2.9435)	GS 28.656 (28.656)	mem 43.724
Train: [1][378/750]	BT 0.181 (1.154)	DT 0.001 (1.018)	loss 9.973 (9.973)	gnorm 1067017.375 (1067017.375)	prob 2.550 (2.5499)	GS 32.516 (32.516)	mem 43.804
Train: [1][379/750]	BT 0.144 (1.152)	DT 0.007 (1.015)	loss 9.520 (9.520)	gnorm 1069057.875 (1069057.875)	prob 2.393 (2.3935)	GS 26.547 (26.547)	mem 43.579
Train: [1][380/750]	BT 0.098 (1.149)	DT 0.002 (1.013)	loss 10.302 (10.302)	gnorm 977523.062 (977523.062)	prob 1.534 (1.5336)	GS 34.406 (34.406)	mem 43.607
Train: [1][381/750]	BT 0.294 (1.147)	DT 0.043 (1.010)	loss 10.448 (10.448)	gnorm 1104614.250 (1104614.250)	prob 2.098 (2.0983)	GS 32.500 (32.500)	mem 43.727
Train: [1][382/750]	BT 0.109 (1.144)	DT 0.003 (1.008)	loss 10.159 (10.159)	gnorm 1029489.688 (1029489.688)	prob 1.807 (1.8070)	GS 39.250 (39.250)	mem 43.625
Train: [1][383/750]	BT 0.109 (1.141)	DT 0.001 (1.005)	loss 10.356 (10.356)	gnorm 986375.250 (986375.250)	prob 1.876 (1.8757)	GS 32.672 (32.672)	mem 43.633
Train: [1][384/750]	BT 11.794 (1.169)	DT 11.596 (1.032)	loss 10.437 (10.437)	gnorm 1053420.125 (1053420.125)	prob 1.810 (1.8104)	GS 31.516 (31.516)	mem 43.756
Train: [1][385/750]	BT 0.097 (1.166)	DT 0.005 (1.030)	loss 10.361 (10.361)	gnorm 1026380.812 (1026380.812)	prob 1.626 (1.6260)	GS 31.891 (31.891)	mem 43.556
Train: [1][386/750]	BT 0.085 (1.164)	DT 0.001 (1.027)	loss 10.013 (10.013)	gnorm 1004007.000 (1004007.000)	prob 2.157 (2.1573)	GS 37.484 (37.484)	mem 43.556
Train: [1][387/750]	BT 0.172 (1.161)	DT 0.002 (1.024)	loss 10.434 (10.434)	gnorm 984093.875 (984093.875)	prob 1.770 (1.7696)	GS 31.625 (31.625)	mem 43.556
Train: [1][388/750]	BT 0.074 (1.158)	DT 0.005 (1.022)	loss 10.292 (10.292)	gnorm 984596.562 (984596.562)	prob 2.547 (2.5471)	GS 31.594 (31.594)	mem 43.556
Train: [1][389/750]	BT 0.065 (1.155)	DT 0.001 (1.019)	loss 9.948 (9.948)	gnorm 1028977.938 (1028977.938)	prob 2.907 (2.9072)	GS 30.656 (30.656)	mem 43.563
Train: [1][390/750]	BT 0.081 (1.153)	DT 0.002 (1.017)	loss 10.752 (10.752)	gnorm 1146107.500 (1146107.500)	prob 1.579 (1.5792)	GS 31.766 (31.766)	mem 43.565
Train: [1][391/750]	BT 0.129 (1.150)	DT 0.001 (1.014)	loss 10.387 (10.387)	gnorm 1014528.438 (1014528.438)	prob 2.927 (2.9274)	GS 33.469 (33.469)	mem 43.572
Train: [1][392/750]	BT 0.126 (1.147)	DT 0.002 (1.011)	loss 10.378 (10.378)	gnorm 976196.188 (976196.188)	prob 2.027 (2.0266)	GS 30.953 (30.953)	mem 43.572
Train: [1][393/750]	BT 0.109 (1.145)	DT 0.008 (1.009)	loss 10.382 (10.382)	gnorm 1143571.625 (1143571.625)	prob 2.409 (2.4087)	GS 33.172 (33.172)	mem 43.615
Train: [1][394/750]	BT 0.172 (1.142)	DT 0.002 (1.006)	loss 10.806 (10.806)	gnorm 1016671.188 (1016671.188)	prob 0.901 (0.9012)	GS 33.250 (33.250)	mem 43.716
Train: [1][395/750]	BT 0.138 (1.140)	DT 0.005 (1.004)	loss 10.537 (10.537)	gnorm 1099015.125 (1099015.125)	prob 1.179 (1.1794)	GS 31.531 (31.531)	mem 43.575
Train: [1][396/750]	BT 11.993 (1.167)	DT 11.907 (1.031)	loss 10.568 (10.568)	gnorm 1137624.375 (1137624.375)	prob 0.672 (0.6716)	GS 30.859 (30.859)	mem 43.589
Train: [1][397/750]	BT 0.083 (1.164)	DT 0.002 (1.029)	loss 10.830 (10.830)	gnorm 1119257.750 (1119257.750)	prob 1.398 (1.3982)	GS 31.188 (31.188)	mem 43.589
Train: [1][398/750]	BT 0.079 (1.162)	DT 0.002 (1.026)	loss 10.426 (10.426)	gnorm 1020274.500 (1020274.500)	prob 1.168 (1.1678)	GS 30.531 (30.531)	mem 43.589
Train: [1][399/750]	BT 0.080 (1.159)	DT 0.002 (1.024)	loss 10.371 (10.371)	gnorm 1048198.625 (1048198.625)	prob 1.874 (1.8744)	GS 33.266 (33.266)	mem 43.590
Train: [1][400/750]	BT 0.614 (1.158)	DT 0.519 (1.022)	loss 10.029 (10.029)	gnorm 960429.938 (960429.938)	prob 1.928 (1.9276)	GS 39.109 (39.109)	mem 43.588
Train: [1][401/750]	BT 0.113 (1.155)	DT 0.001 (1.020)	loss 9.598 (9.598)	gnorm 995766.750 (995766.750)	prob 2.484 (2.4838)	GS 32.219 (32.219)	mem 43.589
Train: [1][402/750]	BT 0.080 (1.152)	DT 0.002 (1.017)	loss 9.834 (9.834)	gnorm 959495.625 (959495.625)	prob 1.765 (1.7647)	GS 33.609 (33.609)	mem 43.590
Train: [1][403/750]	BT 0.149 (1.150)	DT 0.001 (1.015)	loss 10.691 (10.691)	gnorm 1101732.250 (1101732.250)	prob 1.179 (1.1792)	GS 26.703 (26.703)	mem 43.590
Train: [1][404/750]	BT 0.082 (1.147)	DT 0.010 (1.012)	loss 10.683 (10.683)	gnorm 1083444.625 (1083444.625)	prob 0.494 (0.4944)	GS 30.031 (30.031)	mem 43.590
Train: [1][405/750]	BT 0.177 (1.145)	DT 0.002 (1.010)	loss 10.041 (10.041)	gnorm 922192.938 (922192.938)	prob 1.438 (1.4380)	GS 29.062 (29.062)	mem 43.590
Train: [1][406/750]	BT 0.133 (1.142)	DT 0.003 (1.007)	loss 10.120 (10.120)	gnorm 944943.750 (944943.750)	prob 1.625 (1.6252)	GS 31.844 (31.844)	mem 43.641
Train: [1][407/750]	BT 0.208 (1.140)	DT 0.003 (1.005)	loss 10.018 (10.018)	gnorm 970633.062 (970633.062)	prob 2.212 (2.2123)	GS 32.109 (32.109)	mem 43.659
Train: [1][408/750]	BT 10.046 (1.162)	DT 9.890 (1.027)	loss 10.227 (10.227)	gnorm 1030777.312 (1030777.312)	prob 1.921 (1.9205)	GS 36.844 (36.844)	mem 43.684
Train: [1][409/750]	BT 0.088 (1.159)	DT 0.012 (1.024)	loss 10.288 (10.288)	gnorm 908028.500 (908028.500)	prob 2.303 (2.3030)	GS 30.141 (30.141)	mem 43.610
Train: [1][410/750]	BT 0.093 (1.157)	DT 0.001 (1.022)	loss 10.142 (10.142)	gnorm 1040659.562 (1040659.562)	prob 2.220 (2.2202)	GS 29.766 (29.766)	mem 43.611
Train: [1][411/750]	BT 0.086 (1.154)	DT 0.003 (1.019)	loss 11.094 (11.094)	gnorm 1073895.875 (1073895.875)	prob 1.455 (1.4547)	GS 33.422 (33.422)	mem 43.611
Train: [1][412/750]	BT 0.419 (1.152)	DT 0.320 (1.017)	loss 10.725 (10.725)	gnorm 1089176.500 (1089176.500)	prob 1.136 (1.1359)	GS 34.469 (34.469)	mem 43.611
Train: [1][413/750]	BT 0.116 (1.150)	DT 0.002 (1.015)	loss 10.432 (10.432)	gnorm 1053797.375 (1053797.375)	prob 1.844 (1.8443)	GS 30.578 (30.578)	mem 43.610
Train: [1][414/750]	BT 0.187 (1.147)	DT 0.002 (1.013)	loss 10.882 (10.882)	gnorm 1028559.562 (1028559.562)	prob 0.463 (0.4628)	GS 33.531 (33.531)	mem 43.615
Train: [1][415/750]	BT 0.078 (1.145)	DT 0.004 (1.010)	loss 9.529 (9.529)	gnorm 979877.438 (979877.438)	prob 2.245 (2.2450)	GS 33.766 (33.766)	mem 43.643
Train: [1][416/750]	BT 0.201 (1.143)	DT 0.065 (1.008)	loss 10.678 (10.678)	gnorm 943079.375 (943079.375)	prob 0.904 (0.9044)	GS 30.812 (30.812)	mem 43.677
Train: [1][417/750]	BT 0.117 (1.140)	DT 0.013 (1.005)	loss 10.088 (10.088)	gnorm 967778.750 (967778.750)	prob 1.884 (1.8838)	GS 33.984 (33.984)	mem 43.645
Train: [1][418/750]	BT 0.097 (1.138)	DT 0.002 (1.003)	loss 10.063 (10.063)	gnorm 955023.688 (955023.688)	prob 1.655 (1.6548)	GS 31.141 (31.141)	mem 43.646
Train: [1][419/750]	BT 0.118 (1.135)	DT 0.016 (1.001)	loss 9.873 (9.873)	gnorm 953692.125 (953692.125)	prob 1.685 (1.6846)	GS 25.062 (25.062)	mem 43.702
Train: [1][420/750]	BT 10.677 (1.158)	DT 10.504 (1.023)	loss 10.148 (10.148)	gnorm 977158.125 (977158.125)	prob 2.022 (2.0222)	GS 33.453 (33.453)	mem 43.992
Train: [1][421/750]	BT 0.175 (1.156)	DT 0.004 (1.021)	loss 10.371 (10.371)	gnorm 996048.438 (996048.438)	prob 1.560 (1.5599)	GS 30.297 (30.297)	mem 44.032
Train: [1][422/750]	BT 0.095 (1.153)	DT 0.007 (1.018)	loss 10.117 (10.117)	gnorm 1044001.625 (1044001.625)	prob 0.682 (0.6824)	GS 36.719 (36.719)	mem 43.682
Train: [1][423/750]	BT 0.096 (1.151)	DT 0.002 (1.016)	loss 10.293 (10.293)	gnorm 974163.125 (974163.125)	prob 1.062 (1.0619)	GS 32.109 (32.109)	mem 43.683
Train: [1][424/750]	BT 0.081 (1.148)	DT 0.002 (1.014)	loss 10.013 (10.013)	gnorm 919709.500 (919709.500)	prob 1.328 (1.3280)	GS 36.453 (36.453)	mem 43.734
Train: [1][425/750]	BT 0.196 (1.146)	DT 0.026 (1.011)	loss 10.506 (10.506)	gnorm 893753.625 (893753.625)	prob 0.684 (0.6835)	GS 34.000 (34.000)	mem 43.685
Train: [1][426/750]	BT 0.112 (1.143)	DT 0.002 (1.009)	loss 10.199 (10.199)	gnorm 939948.875 (939948.875)	prob 1.462 (1.4616)	GS 37.688 (37.688)	mem 43.684
Train: [1][427/750]	BT 0.224 (1.141)	DT 0.026 (1.007)	loss 9.507 (9.507)	gnorm 874792.312 (874792.312)	prob 2.727 (2.7266)	GS 29.406 (29.406)	mem 43.683
Train: [1][428/750]	BT 0.128 (1.139)	DT 0.002 (1.004)	loss 10.415 (10.415)	gnorm 1039147.812 (1039147.812)	prob 1.317 (1.3170)	GS 33.578 (33.578)	mem 43.683
Train: [1][429/750]	BT 0.155 (1.136)	DT 0.006 (1.002)	loss 10.117 (10.117)	gnorm 1023392.250 (1023392.250)	prob 1.371 (1.3706)	GS 31.219 (31.219)	mem 43.763
Train: [1][430/750]	BT 0.107 (1.134)	DT 0.002 (1.000)	loss 9.923 (9.923)	gnorm 1066840.375 (1066840.375)	prob 1.194 (1.1942)	GS 32.344 (32.344)	mem 43.811
Train: [1][431/750]	BT 0.120 (1.132)	DT 0.002 (0.997)	loss 9.938 (9.938)	gnorm 1053382.125 (1053382.125)	prob 1.508 (1.5083)	GS 29.578 (29.578)	mem 43.686
Train: [1][432/750]	BT 7.537 (1.147)	DT 7.327 (1.012)	loss 10.073 (10.073)	gnorm 1073449.250 (1073449.250)	prob 1.367 (1.3670)	GS 34.359 (34.359)	mem 43.701
Train: [1][433/750]	BT 0.261 (1.145)	DT 0.017 (1.010)	loss 10.159 (10.159)	gnorm 1017205.375 (1017205.375)	prob 1.857 (1.8567)	GS 32.719 (32.719)	mem 43.669
Train: [1][434/750]	BT 0.139 (1.142)	DT 0.002 (1.007)	loss 10.865 (10.865)	gnorm 1058236.625 (1058236.625)	prob 0.653 (0.6531)	GS 36.609 (36.609)	mem 43.671
Train: [1][435/750]	BT 0.096 (1.140)	DT 0.003 (1.005)	loss 10.277 (10.277)	gnorm 1024459.812 (1024459.812)	prob 1.026 (1.0257)	GS 28.266 (28.266)	mem 43.672
Train: [1][436/750]	BT 4.346 (1.147)	DT 4.250 (1.013)	loss 10.662 (10.662)	gnorm 1051740.625 (1051740.625)	prob 0.656 (0.6555)	GS 35.297 (35.297)	mem 43.674
Train: [1][437/750]	BT 0.089 (1.145)	DT 0.002 (1.010)	loss 10.307 (10.307)	gnorm 1074726.750 (1074726.750)	prob 1.450 (1.4501)	GS 28.500 (28.500)	mem 43.675
Train: [1][438/750]	BT 0.100 (1.142)	DT 0.002 (1.008)	loss 10.426 (10.426)	gnorm 1196610.750 (1196610.750)	prob 0.626 (0.6261)	GS 39.609 (39.609)	mem 43.713
Train: [1][439/750]	BT 0.117 (1.140)	DT 0.001 (1.006)	loss 9.846 (9.846)	gnorm 965757.062 (965757.062)	prob 1.234 (1.2336)	GS 31.094 (31.094)	mem 43.761
Train: [1][440/750]	BT 0.141 (1.138)	DT 0.008 (1.003)	loss 10.065 (10.065)	gnorm 1020379.500 (1020379.500)	prob 0.909 (0.9094)	GS 37.750 (37.750)	mem 43.688
Train: [1][441/750]	BT 0.191 (1.136)	DT 0.009 (1.001)	loss 10.432 (10.432)	gnorm 1025374.938 (1025374.938)	prob 1.109 (1.1085)	GS 33.156 (33.156)	mem 43.851
Train: [1][442/750]	BT 2.503 (1.139)	DT 2.289 (1.004)	loss 10.615 (10.615)	gnorm 1052161.625 (1052161.625)	prob 0.599 (0.5985)	GS 35.984 (35.984)	mem 43.789
Train: [1][443/750]	BT 0.122 (1.136)	DT 0.008 (1.002)	loss 10.281 (10.281)	gnorm 973501.000 (973501.000)	prob 1.011 (1.0110)	GS 30.484 (30.484)	mem 43.695
Train: [1][444/750]	BT 6.547 (1.149)	DT 6.409 (1.014)	loss 10.386 (10.386)	gnorm 1054245.000 (1054245.000)	prob 0.530 (0.5295)	GS 33.156 (33.156)	mem 43.778
Train: [1][445/750]	BT 0.122 (1.146)	DT 0.002 (1.012)	loss 10.382 (10.382)	gnorm 1002767.812 (1002767.812)	prob 0.666 (0.6655)	GS 30.734 (30.734)	mem 43.692
Train: [1][446/750]	BT 0.186 (1.144)	DT 0.029 (1.009)	loss 9.168 (9.168)	gnorm 937021.875 (937021.875)	prob 1.887 (1.8869)	GS 33.578 (33.578)	mem 43.669
Train: [1][447/750]	BT 0.133 (1.142)	DT 0.002 (1.007)	loss 10.384 (10.384)	gnorm 963765.500 (963765.500)	prob 0.136 (0.1358)	GS 34.094 (34.094)	mem 43.679
Train: [1][448/750]	BT 4.541 (1.149)	DT 4.459 (1.015)	loss 9.768 (9.768)	gnorm 935796.688 (935796.688)	prob 1.037 (1.0373)	GS 35.922 (35.922)	mem 43.676
Train: [1][449/750]	BT 0.109 (1.147)	DT 0.011 (1.013)	loss 9.808 (9.808)	gnorm 939375.375 (939375.375)	prob 1.614 (1.6144)	GS 25.906 (25.906)	mem 43.677
Train: [1][450/750]	BT 0.112 (1.145)	DT 0.002 (1.010)	loss 10.941 (10.941)	gnorm 928019.500 (928019.500)	prob 0.053 (0.0532)	GS 28.969 (28.969)	mem 43.677
Train: [1][451/750]	BT 0.066 (1.142)	DT 0.001 (1.008)	loss 10.318 (10.318)	gnorm 982619.062 (982619.062)	prob 1.684 (1.6835)	GS 36.219 (36.219)	mem 43.689
Train: [1][452/750]	BT 0.130 (1.140)	DT 0.002 (1.006)	loss 9.752 (9.752)	gnorm 985529.875 (985529.875)	prob 2.042 (2.0423)	GS 32.781 (32.781)	mem 43.737
Train: [1][453/750]	BT 0.251 (1.138)	DT 0.018 (1.004)	loss 10.208 (10.208)	gnorm 960774.625 (960774.625)	prob 2.025 (2.0251)	GS 30.391 (30.391)	mem 43.678
Train: [1][454/750]	BT 2.073 (1.140)	DT 1.862 (1.006)	loss 9.900 (9.900)	gnorm 1001156.375 (1001156.375)	prob 1.670 (1.6699)	GS 32.719 (32.719)	mem 43.775
Train: [1][455/750]	BT 0.107 (1.138)	DT 0.007 (1.003)	loss 10.423 (10.423)	gnorm 967954.500 (967954.500)	prob 1.172 (1.1715)	GS 27.062 (27.062)	mem 43.692
Train: [1][456/750]	BT 5.249 (1.147)	DT 5.133 (1.013)	loss 10.186 (10.186)	gnorm 1070149.625 (1070149.625)	prob 1.556 (1.5561)	GS 31.094 (31.094)	mem 43.704
Train: [1][457/750]	BT 0.245 (1.145)	DT 0.018 (1.010)	loss 10.193 (10.193)	gnorm 1059266.625 (1059266.625)	prob 2.022 (2.0225)	GS 33.219 (33.219)	mem 43.733
Train: [1][458/750]	BT 0.086 (1.143)	DT 0.006 (1.008)	loss 10.749 (10.749)	gnorm 1035323.375 (1035323.375)	prob 0.262 (0.2621)	GS 34.719 (34.719)	mem 43.688
Train: [1][459/750]	BT 0.108 (1.141)	DT 0.003 (1.006)	loss 10.330 (10.330)	gnorm 962528.562 (962528.562)	prob 1.364 (1.3641)	GS 30.078 (30.078)	mem 43.689
Train: [1][460/750]	BT 1.985 (1.142)	DT 1.895 (1.008)	loss 10.049 (10.049)	gnorm 1076842.125 (1076842.125)	prob 1.485 (1.4846)	GS 32.141 (32.141)	mem 43.638
Train: [1][461/750]	BT 0.087 (1.140)	DT 0.002 (1.006)	loss 10.444 (10.444)	gnorm 1008969.062 (1008969.062)	prob 1.309 (1.3095)	GS 29.828 (29.828)	mem 43.639
Train: [1][462/750]	BT 0.091 (1.138)	DT 0.003 (1.004)	loss 10.567 (10.567)	gnorm 979582.812 (979582.812)	prob 0.614 (0.6143)	GS 33.844 (33.844)	mem 43.639
Train: [1][463/750]	BT 0.125 (1.136)	DT 0.016 (1.001)	loss 10.732 (10.732)	gnorm 1099492.750 (1099492.750)	prob 1.481 (1.4809)	GS 28.562 (28.562)	mem 43.636
Train: [1][464/750]	BT 0.166 (1.134)	DT 0.002 (0.999)	loss 10.562 (10.562)	gnorm 1188041.750 (1188041.750)	prob 0.667 (0.6674)	GS 27.516 (27.516)	mem 43.640
Train: [1][465/750]	BT 0.121 (1.131)	DT 0.005 (0.997)	loss 11.186 (11.186)	gnorm 1052099.875 (1052099.875)	prob 0.477 (0.4773)	GS 34.953 (34.953)	mem 43.641
Train: [1][466/750]	BT 5.909 (1.142)	DT 5.778 (1.007)	loss 11.002 (11.002)	gnorm 1030932.750 (1030932.750)	prob 0.282 (0.2822)	GS 31.859 (31.859)	mem 43.666
Train: [1][467/750]	BT 0.094 (1.139)	DT 0.007 (1.005)	loss 10.217 (10.217)	gnorm 1025130.750 (1025130.750)	prob 1.721 (1.7209)	GS 33.312 (33.312)	mem 43.665
Train: [1][468/750]	BT 6.440 (1.151)	DT 6.278 (1.017)	loss 10.334 (10.334)	gnorm 957986.812 (957986.812)	prob 1.418 (1.4180)	GS 37.219 (37.219)	mem 43.654
Train: [1][469/750]	BT 0.058 (1.148)	DT 0.001 (1.014)	loss 10.099 (10.099)	gnorm 1008311.188 (1008311.188)	prob 2.524 (2.5240)	GS 29.984 (29.984)	mem 43.655
Train: [1][470/750]	BT 0.058 (1.146)	DT 0.002 (1.012)	loss 10.272 (10.272)	gnorm 1085520.750 (1085520.750)	prob 1.599 (1.5987)	GS 33.141 (33.141)	mem 43.655
Train: [1][471/750]	BT 0.069 (1.144)	DT 0.002 (1.010)	loss 10.371 (10.371)	gnorm 1040525.938 (1040525.938)	prob 2.360 (2.3596)	GS 33.250 (33.250)	mem 43.655
Train: [1][472/750]	BT 0.084 (1.141)	DT 0.002 (1.008)	loss 10.239 (10.239)	gnorm 1075279.375 (1075279.375)	prob 1.949 (1.9489)	GS 34.016 (34.016)	mem 43.706
Train: [1][473/750]	BT 0.113 (1.139)	DT 0.002 (1.006)	loss 10.979 (10.979)	gnorm 1007692.875 (1007692.875)	prob 1.512 (1.5116)	GS 29.797 (29.797)	mem 43.656
Train: [1][474/750]	BT 0.148 (1.137)	DT 0.001 (1.004)	loss 10.737 (10.737)	gnorm 1108565.500 (1108565.500)	prob 1.261 (1.2610)	GS 34.094 (34.094)	mem 43.690
Train: [1][475/750]	BT 0.211 (1.135)	DT 0.001 (1.002)	loss 10.161 (10.161)	gnorm 966830.812 (966830.812)	prob 2.585 (2.5849)	GS 32.438 (32.438)	mem 43.892
Train: [1][476/750]	BT 0.208 (1.133)	DT 0.002 (0.999)	loss 10.652 (10.652)	gnorm 991504.500 (991504.500)	prob 1.187 (1.1871)	GS 32.906 (32.906)	mem 43.753
Train: [1][477/750]	BT 0.083 (1.131)	DT 0.001 (0.997)	loss 10.543 (10.543)	gnorm 1069090.625 (1069090.625)	prob 2.143 (2.1429)	GS 30.328 (30.328)	mem 43.656
Train: [1][478/750]	BT 4.316 (1.138)	DT 4.161 (1.004)	loss 10.507 (10.507)	gnorm 1004017.500 (1004017.500)	prob 1.669 (1.6691)	GS 33.016 (33.016)	mem 43.907
Train: [1][479/750]	BT 0.123 (1.136)	DT 0.002 (1.002)	loss 10.756 (10.756)	gnorm 1096178.250 (1096178.250)	prob 1.927 (1.9271)	GS 29.844 (29.844)	mem 43.851
Train: [1][480/750]	BT 6.585 (1.147)	DT 6.491 (1.013)	loss 9.971 (9.971)	gnorm 995417.688 (995417.688)	prob 2.334 (2.3339)	GS 33.859 (33.859)	mem 43.687
Train: [1][481/750]	BT 0.074 (1.145)	DT 0.004 (1.011)	loss 10.427 (10.427)	gnorm 986015.625 (986015.625)	prob 2.011 (2.0109)	GS 29.359 (29.359)	mem 43.689
Train: [1][482/750]	BT 0.080 (1.143)	DT 0.002 (1.009)	loss 10.062 (10.062)	gnorm 1008256.312 (1008256.312)	prob 2.359 (2.3591)	GS 36.516 (36.516)	mem 43.775
Train: [1][483/750]	BT 0.260 (1.141)	DT 0.005 (1.007)	loss 9.833 (9.833)	gnorm 1005999.000 (1005999.000)	prob 2.540 (2.5398)	GS 31.422 (31.422)	mem 43.827
Train: [1][484/750]	BT 0.186 (1.139)	DT 0.070 (1.005)	loss 9.765 (9.765)	gnorm 993099.125 (993099.125)	prob 1.762 (1.7616)	GS 35.672 (35.672)	mem 43.795
Train: [1][485/750]	BT 0.100 (1.137)	DT 0.003 (1.003)	loss 10.482 (10.482)	gnorm 861411.812 (861411.812)	prob 1.867 (1.8671)	GS 27.609 (27.609)	mem 43.691
Train: [1][486/750]	BT 0.091 (1.134)	DT 0.001 (1.001)	loss 11.036 (11.036)	gnorm 961861.375 (961861.375)	prob 0.858 (0.8584)	GS 33.906 (33.906)	mem 43.752
Train: [1][487/750]	BT 0.178 (1.133)	DT 0.002 (0.999)	loss 9.812 (9.812)	gnorm 992859.125 (992859.125)	prob 2.041 (2.0411)	GS 31.438 (31.438)	mem 43.885
Train: [1][488/750]	BT 0.171 (1.131)	DT 0.011 (0.997)	loss 10.719 (10.719)	gnorm 958321.875 (958321.875)	prob 1.048 (1.0477)	GS 34.031 (34.031)	mem 43.691
Train: [1][489/750]	BT 0.118 (1.128)	DT 0.002 (0.995)	loss 10.346 (10.346)	gnorm 1050790.875 (1050790.875)	prob 0.985 (0.9848)	GS 28.578 (28.578)	mem 43.692
Train: [1][490/750]	BT 3.531 (1.133)	DT 3.369 (1.000)	loss 10.173 (10.173)	gnorm 1062776.125 (1062776.125)	prob 1.063 (1.0629)	GS 36.547 (36.547)	mem 43.636
Train: [1][491/750]	BT 0.228 (1.132)	DT 0.009 (0.998)	loss 9.820 (9.820)	gnorm 1037053.312 (1037053.312)	prob 2.512 (2.5124)	GS 30.250 (30.250)	mem 43.636
Train: [1][492/750]	BT 8.899 (1.147)	DT 8.644 (1.013)	loss 10.982 (10.982)	gnorm 1049050.625 (1049050.625)	prob 0.208 (0.2084)	GS 33.188 (33.188)	mem 43.809
Train: [1][493/750]	BT 0.106 (1.145)	DT 0.002 (1.011)	loss 10.043 (10.043)	gnorm 896872.062 (896872.062)	prob 2.111 (2.1108)	GS 32.656 (32.656)	mem 43.734
Train: [1][494/750]	BT 0.082 (1.143)	DT 0.002 (1.009)	loss 10.143 (10.143)	gnorm 902706.250 (902706.250)	prob 1.919 (1.9195)	GS 33.453 (33.453)	mem 43.640
Train: [1][495/750]	BT 0.131 (1.141)	DT 0.002 (1.007)	loss 10.415 (10.415)	gnorm 938634.000 (938634.000)	prob 0.988 (0.9876)	GS 28.016 (28.016)	mem 43.677
Train: [1][496/750]	BT 2.286 (1.143)	DT 2.168 (1.009)	loss 10.099 (10.099)	gnorm 1109116.375 (1109116.375)	prob 1.403 (1.4026)	GS 34.016 (34.016)	mem 43.724
Train: [1][497/750]	BT 0.105 (1.141)	DT 0.002 (1.007)	loss 10.889 (10.889)	gnorm 1025772.312 (1025772.312)	prob 0.106 (0.1064)	GS 34.641 (34.641)	mem 43.864
Train: [1][498/750]	BT 0.198 (1.139)	DT 0.030 (1.005)	loss 10.049 (10.049)	gnorm 970816.750 (970816.750)	prob 0.866 (0.8656)	GS 35.062 (35.062)	mem 43.865
Train: [1][499/750]	BT 0.101 (1.137)	DT 0.003 (1.003)	loss 10.151 (10.151)	gnorm 925408.875 (925408.875)	prob 1.184 (1.1842)	GS 32.359 (32.359)	mem 43.682
Train: [1][500/750]	BT 0.091 (1.135)	DT 0.002 (1.001)	loss 10.052 (10.052)	gnorm 962888.000 (962888.000)	prob 0.838 (0.8377)	GS 31.453 (31.453)	mem 43.719
Train: [1][501/750]	BT 0.110 (1.133)	DT 0.002 (0.999)	loss 10.234 (10.234)	gnorm 990035.000 (990035.000)	prob 1.202 (1.2019)	GS 33.703 (33.703)	mem 43.711
Train: [1][502/750]	BT 5.491 (1.142)	DT 5.403 (1.008)	loss 10.757 (10.757)	gnorm 942911.000 (942911.000)	prob 0.259 (0.2586)	GS 38.906 (38.906)	mem 43.758
Train: [1][503/750]	BT 0.223 (1.140)	DT 0.029 (1.006)	loss 9.928 (9.928)	gnorm 894991.688 (894991.688)	prob 1.825 (1.8248)	GS 29.844 (29.844)	mem 43.721
Train: [1][504/750]	BT 4.699 (1.147)	DT 4.565 (1.013)	loss 10.772 (10.772)	gnorm 1055090.250 (1055090.250)	prob 0.601 (0.6006)	GS 35.609 (35.609)	mem 43.722
Train: [1][505/750]	BT 0.092 (1.145)	DT 0.002 (1.011)	loss 9.849 (9.849)	gnorm 1018639.375 (1018639.375)	prob 2.264 (2.2637)	GS 26.125 (26.125)	mem 43.627
Train: [1][506/750]	BT 0.121 (1.143)	DT 0.001 (1.009)	loss 10.045 (10.045)	gnorm 860314.125 (860314.125)	prob 1.400 (1.4004)	GS 34.766 (34.766)	mem 43.627
Train: [1][507/750]	BT 0.090 (1.141)	DT 0.004 (1.007)	loss 10.472 (10.472)	gnorm 940678.188 (940678.188)	prob 1.113 (1.1132)	GS 33.016 (33.016)	mem 43.628
Train: [1][508/750]	BT 0.106 (1.139)	DT 0.002 (1.005)	loss 10.462 (10.462)	gnorm 830293.688 (830293.688)	prob 1.383 (1.3834)	GS 32.344 (32.344)	mem 43.645
Train: [1][509/750]	BT 0.162 (1.137)	DT 0.004 (1.003)	loss 10.360 (10.360)	gnorm 911424.812 (911424.812)	prob 2.190 (2.1904)	GS 37.109 (37.109)	mem 43.876
Train: [1][510/750]	BT 0.116 (1.135)	DT 0.001 (1.001)	loss 10.900 (10.900)	gnorm 979505.812 (979505.812)	prob 1.078 (1.0779)	GS 27.109 (27.109)	mem 43.943
Train: [1][511/750]	BT 0.222 (1.133)	DT 0.009 (1.000)	loss 10.747 (10.747)	gnorm 911463.062 (911463.062)	prob 1.067 (1.0668)	GS 29.016 (29.016)	mem 43.670
Train: [1][512/750]	BT 0.110 (1.131)	DT 0.002 (0.998)	loss 10.889 (10.889)	gnorm 873820.375 (873820.375)	prob 0.673 (0.6730)	GS 32.922 (32.922)	mem 43.689
Train: [1][513/750]	BT 0.098 (1.129)	DT 0.002 (0.996)	loss 10.534 (10.534)	gnorm 937250.562 (937250.562)	prob 0.878 (0.8778)	GS 35.797 (35.797)	mem 43.830
Train: [1][514/750]	BT 9.488 (1.145)	DT 9.319 (1.012)	loss 10.285 (10.285)	gnorm 919408.312 (919408.312)	prob 1.375 (1.3754)	GS 32.594 (32.594)	mem 43.708
Train: [1][515/750]	BT 0.257 (1.144)	DT 0.012 (1.010)	loss 10.535 (10.535)	gnorm 1048570.312 (1048570.312)	prob 0.965 (0.9655)	GS 32.672 (32.672)	mem 43.708
Train: [1][516/750]	BT 2.270 (1.146)	DT 2.118 (1.012)	loss 10.895 (10.895)	gnorm 1121574.125 (1121574.125)	prob 0.345 (0.3449)	GS 31.422 (31.422)	mem 43.887
Train: [1][517/750]	BT 0.172 (1.144)	DT 0.002 (1.010)	loss 10.217 (10.217)	gnorm 1061195.750 (1061195.750)	prob 2.332 (2.3322)	GS 32.562 (32.562)	mem 43.985
Train: [1][518/750]	BT 0.144 (1.142)	DT 0.004 (1.008)	loss 10.759 (10.759)	gnorm 957600.062 (957600.062)	prob 0.916 (0.9165)	GS 30.641 (30.641)	mem 43.986
Train: [1][519/750]	BT 0.106 (1.140)	DT 0.002 (1.006)	loss 10.245 (10.245)	gnorm 981253.375 (981253.375)	prob 1.718 (1.7177)	GS 31.219 (31.219)	mem 43.715
Train: [1][520/750]	BT 0.126 (1.138)	DT 0.001 (1.004)	loss 10.734 (10.734)	gnorm 1006320.938 (1006320.938)	prob 0.884 (0.8843)	GS 31.938 (31.938)	mem 43.725
Train: [1][521/750]	BT 0.115 (1.136)	DT 0.002 (1.002)	loss 10.048 (10.048)	gnorm 935703.625 (935703.625)	prob 1.984 (1.9841)	GS 30.672 (30.672)	mem 43.839
Train: [1][522/750]	BT 0.090 (1.134)	DT 0.002 (1.000)	loss 10.059 (10.059)	gnorm 952684.750 (952684.750)	prob 1.614 (1.6141)	GS 36.297 (36.297)	mem 43.944
Train: [1][523/750]	BT 0.173 (1.132)	DT 0.003 (0.999)	loss 10.078 (10.078)	gnorm 993872.438 (993872.438)	prob 1.627 (1.6266)	GS 31.969 (31.969)	mem 43.955
Train: [1][524/750]	BT 0.106 (1.130)	DT 0.002 (0.997)	loss 10.219 (10.219)	gnorm 907593.062 (907593.062)	prob 1.438 (1.4383)	GS 36.359 (36.359)	mem 43.954
Train: [1][525/750]	BT 0.103 (1.128)	DT 0.006 (0.995)	loss 10.348 (10.348)	gnorm 987373.062 (987373.062)	prob 1.715 (1.7147)	GS 31.984 (31.984)	mem 43.954
Train: [1][526/750]	BT 11.029 (1.147)	DT 10.912 (1.014)	loss 10.637 (10.637)	gnorm 1060229.875 (1060229.875)	prob 0.712 (0.7122)	GS 34.156 (34.156)	mem 43.787
Train: [1][527/750]	BT 0.111 (1.145)	DT 0.013 (1.012)	loss 10.943 (10.943)	gnorm 935431.312 (935431.312)	prob 1.419 (1.4188)	GS 29.891 (29.891)	mem 43.712
Train: [1][528/750]	BT 2.777 (1.148)	DT 2.659 (1.015)	loss 9.751 (9.751)	gnorm 996367.875 (996367.875)	prob 2.350 (2.3501)	GS 37.719 (37.719)	mem 43.661
Train: [1][529/750]	BT 0.089 (1.146)	DT 0.001 (1.013)	loss 10.144 (10.144)	gnorm 1007731.062 (1007731.062)	prob 1.502 (1.5017)	GS 29.062 (29.062)	mem 43.661
Train: [1][530/750]	BT 0.084 (1.144)	DT 0.003 (1.011)	loss 10.555 (10.555)	gnorm 929447.438 (929447.438)	prob 1.105 (1.1052)	GS 35.047 (35.047)	mem 43.662
Train: [1][531/750]	BT 0.077 (1.142)	DT 0.001 (1.009)	loss 10.274 (10.274)	gnorm 970874.562 (970874.562)	prob 1.948 (1.9477)	GS 37.719 (37.719)	mem 43.583
Train: [1][532/750]	BT 0.218 (1.141)	DT 0.005 (1.007)	loss 9.764 (9.764)	gnorm 964175.812 (964175.812)	prob 2.690 (2.6896)	GS 37.078 (37.078)	mem 43.591
Train: [1][533/750]	BT 0.114 (1.139)	DT 0.005 (1.005)	loss 10.139 (10.139)	gnorm 872196.875 (872196.875)	prob 2.480 (2.4804)	GS 23.109 (23.109)	mem 43.527
Train: [1][534/750]	BT 0.296 (1.137)	DT 0.028 (1.003)	loss 11.224 (11.224)	gnorm 1001268.312 (1001268.312)	prob 0.868 (0.8677)	GS 30.062 (30.062)	mem 43.535
Train: [1][535/750]	BT 0.160 (1.135)	DT 0.009 (1.002)	loss 9.940 (9.940)	gnorm 886115.188 (886115.188)	prob 2.680 (2.6801)	GS 34.000 (34.000)	mem 43.535
Train: [1][536/750]	BT 0.241 (1.134)	DT 0.004 (1.000)	loss 10.366 (10.366)	gnorm 942408.562 (942408.562)	prob 1.924 (1.9240)	GS 33.141 (33.141)	mem 43.534
Train: [1][537/750]	BT 0.100 (1.132)	DT 0.002 (0.998)	loss 9.772 (9.772)	gnorm 902355.375 (902355.375)	prob 2.417 (2.4174)	GS 28.766 (28.766)	mem 43.535
Train: [1][538/750]	BT 8.633 (1.146)	DT 8.513 (1.012)	loss 10.624 (10.624)	gnorm 989894.812 (989894.812)	prob 1.415 (1.4152)	GS 33.781 (33.781)	mem 43.626
Train: [1][539/750]	BT 0.203 (1.144)	DT 0.014 (1.010)	loss 10.456 (10.456)	gnorm 924529.250 (924529.250)	prob 1.073 (1.0732)	GS 28.953 (28.953)	mem 43.624
Train: [1][540/750]	BT 4.250 (1.150)	DT 4.165 (1.016)	loss 9.942 (9.942)	gnorm 879209.312 (879209.312)	prob 2.068 (2.0679)	GS 37.312 (37.312)	mem 43.568
Train: [1][541/750]	BT 0.068 (1.148)	DT 0.001 (1.014)	loss 10.088 (10.088)	gnorm 974667.125 (974667.125)	prob 1.559 (1.5588)	GS 35.375 (35.375)	mem 43.568
Train: [1][542/750]	BT 0.111 (1.146)	DT 0.001 (1.012)	loss 10.067 (10.067)	gnorm 918654.875 (918654.875)	prob 1.468 (1.4676)	GS 32.453 (32.453)	mem 43.567
Train: [1][543/750]	BT 0.180 (1.144)	DT 0.017 (1.010)	loss 9.713 (9.713)	gnorm 922119.562 (922119.562)	prob 1.943 (1.9430)	GS 29.094 (29.094)	mem 43.567
Train: [1][544/750]	BT 0.194 (1.142)	DT 0.004 (1.008)	loss 9.838 (9.838)	gnorm 824534.812 (824534.812)	prob 1.918 (1.9181)	GS 36.812 (36.812)	mem 43.568
Train: [1][545/750]	BT 0.058 (1.140)	DT 0.002 (1.007)	loss 9.757 (9.757)	gnorm 888662.812 (888662.812)	prob 1.063 (1.0631)	GS 32.812 (32.812)	mem 43.568
Train: [1][546/750]	BT 0.085 (1.138)	DT 0.001 (1.005)	loss 9.701 (9.701)	gnorm 783983.750 (783983.750)	prob 2.238 (2.2382)	GS 35.000 (35.000)	mem 43.570
Train: [1][547/750]	BT 0.115 (1.136)	DT 0.003 (1.003)	loss 10.702 (10.702)	gnorm 955061.000 (955061.000)	prob 1.460 (1.4602)	GS 26.594 (26.594)	mem 43.571
Train: [1][548/750]	BT 0.171 (1.135)	DT 0.001 (1.001)	loss 10.164 (10.164)	gnorm 984773.938 (984773.938)	prob 1.336 (1.3361)	GS 30.734 (30.734)	mem 43.570
Train: [1][549/750]	BT 0.228 (1.133)	DT 0.004 (0.999)	loss 10.683 (10.683)	gnorm 913228.375 (913228.375)	prob 0.891 (0.8906)	GS 30.109 (30.109)	mem 43.629
Train: [1][550/750]	BT 7.907 (1.145)	DT 7.749 (1.012)	loss 10.277 (10.277)	gnorm 841429.250 (841429.250)	prob 1.457 (1.4573)	GS 34.984 (34.984)	mem 43.787
Train: [1][551/750]	BT 0.136 (1.143)	DT 0.001 (1.010)	loss 9.880 (9.880)	gnorm 852594.062 (852594.062)	prob 1.770 (1.7700)	GS 33.656 (33.656)	mem 43.871
Train: [1][552/750]	BT 3.035 (1.147)	DT 2.960 (1.013)	loss 10.146 (10.146)	gnorm 944666.938 (944666.938)	prob 1.667 (1.6673)	GS 31.812 (31.812)	mem 43.648
Train: [1][553/750]	BT 0.145 (1.145)	DT 0.001 (1.011)	loss 10.406 (10.406)	gnorm 1014824.812 (1014824.812)	prob 1.520 (1.5197)	GS 33.344 (33.344)	mem 43.657
Train: [1][554/750]	BT 0.200 (1.143)	DT 0.012 (1.010)	loss 10.424 (10.424)	gnorm 981556.812 (981556.812)	prob 0.884 (0.8839)	GS 33.562 (33.562)	mem 43.758
Train: [1][555/750]	BT 0.129 (1.141)	DT 0.002 (1.008)	loss 10.539 (10.539)	gnorm 1007151.625 (1007151.625)	prob 2.212 (2.2116)	GS 32.031 (32.031)	mem 43.685
Train: [1][556/750]	BT 0.107 (1.140)	DT 0.002 (1.006)	loss 10.627 (10.627)	gnorm 1028935.750 (1028935.750)	prob 1.193 (1.1928)	GS 33.828 (33.828)	mem 43.686
Train: [1][557/750]	BT 0.113 (1.138)	DT 0.006 (1.004)	loss 11.202 (11.202)	gnorm 891346.875 (891346.875)	prob -0.286 (-0.2859)	GS 27.859 (27.859)	mem 43.718
Train: [1][558/750]	BT 0.122 (1.136)	DT 0.001 (1.002)	loss 10.527 (10.527)	gnorm 915467.625 (915467.625)	prob 0.543 (0.5432)	GS 35.609 (35.609)	mem 43.770
Train: [1][559/750]	BT 0.116 (1.134)	DT 0.009 (1.001)	loss 10.594 (10.594)	gnorm 1011170.312 (1011170.312)	prob 0.271 (0.2713)	GS 36.219 (36.219)	mem 43.685
Train: [1][560/750]	BT 0.157 (1.132)	DT 0.002 (0.999)	loss 9.820 (9.820)	gnorm 1081771.000 (1081771.000)	prob 0.399 (0.3991)	GS 37.391 (37.391)	mem 43.685
Train: [1][561/750]	BT 0.206 (1.131)	DT 0.005 (0.997)	loss 10.715 (10.715)	gnorm 980084.938 (980084.938)	prob 0.659 (0.6592)	GS 33.859 (33.859)	mem 43.689
Train: [1][562/750]	BT 8.670 (1.144)	DT 8.568 (1.011)	loss 10.095 (10.095)	gnorm 977797.188 (977797.188)	prob 1.082 (1.0823)	GS 34.188 (34.188)	mem 43.652
Train: [1][563/750]	BT 0.105 (1.142)	DT 0.003 (1.009)	loss 10.019 (10.019)	gnorm 914804.688 (914804.688)	prob 1.071 (1.0708)	GS 32.656 (32.656)	mem 43.652
Train: [1][564/750]	BT 4.441 (1.148)	DT 4.341 (1.015)	loss 10.349 (10.349)	gnorm 919259.312 (919259.312)	prob 0.733 (0.7330)	GS 35.922 (35.922)	mem 43.551
Train: [1][565/750]	BT 0.070 (1.146)	DT 0.001 (1.013)	loss 10.939 (10.939)	gnorm 976085.250 (976085.250)	prob 0.124 (0.1243)	GS 33.188 (33.188)	mem 43.551
Train: [1][566/750]	BT 0.065 (1.144)	DT 0.001 (1.011)	loss 10.650 (10.650)	gnorm 1072843.875 (1072843.875)	prob 0.402 (0.4021)	GS 36.531 (36.531)	mem 43.564
Train: [1][567/750]	BT 0.098 (1.142)	DT 0.001 (1.009)	loss 10.920 (10.920)	gnorm 1028559.375 (1028559.375)	prob 0.626 (0.6257)	GS 30.266 (30.266)	mem 43.611
Train: [1][568/750]	BT 0.185 (1.141)	DT 0.003 (1.008)	loss 10.070 (10.070)	gnorm 1031567.812 (1031567.812)	prob 1.177 (1.1772)	GS 34.609 (34.609)	mem 43.571
Train: [1][569/750]	BT 0.193 (1.139)	DT 0.005 (1.006)	loss 10.209 (10.209)	gnorm 871533.562 (871533.562)	prob 1.288 (1.2883)	GS 31.516 (31.516)	mem 43.590
Train: [1][570/750]	BT 0.111 (1.137)	DT 0.003 (1.004)	loss 10.301 (10.301)	gnorm 892810.938 (892810.938)	prob 0.983 (0.9831)	GS 30.312 (30.312)	mem 43.650
Train: [1][571/750]	BT 0.135 (1.136)	DT 0.011 (1.002)	loss 10.614 (10.614)	gnorm 914835.938 (914835.938)	prob 1.743 (1.7433)	GS 31.734 (31.734)	mem 43.696
Train: [1][572/750]	BT 0.193 (1.134)	DT 0.003 (1.001)	loss 10.326 (10.326)	gnorm 901158.812 (901158.812)	prob 1.742 (1.7416)	GS 30.719 (30.719)	mem 43.815
Train: [1][573/750]	BT 0.123 (1.132)	DT 0.006 (0.999)	loss 10.436 (10.436)	gnorm 984396.812 (984396.812)	prob 1.395 (1.3953)	GS 29.766 (29.766)	mem 43.817
Train: [1][574/750]	BT 6.206 (1.141)	DT 6.104 (1.008)	loss 9.810 (9.810)	gnorm 813944.312 (813944.312)	prob 2.074 (2.0738)	GS 30.703 (30.703)	mem 43.719
Train: [1][575/750]	BT 0.174 (1.139)	DT 0.005 (1.006)	loss 10.528 (10.528)	gnorm 838550.812 (838550.812)	prob 1.853 (1.8532)	GS 32.766 (32.766)	mem 43.838
Train: [1][576/750]	BT 7.271 (1.150)	DT 7.116 (1.017)	loss 10.035 (10.035)	gnorm 857948.188 (857948.188)	prob 2.073 (2.0730)	GS 33.766 (33.766)	mem 43.532
Train: [1][577/750]	BT 0.095 (1.148)	DT 0.002 (1.015)	loss 9.955 (9.955)	gnorm 926804.375 (926804.375)	prob 1.628 (1.6276)	GS 28.969 (28.969)	mem 43.533
Train: [1][578/750]	BT 0.101 (1.146)	DT 0.002 (1.013)	loss 10.447 (10.447)	gnorm 932183.875 (932183.875)	prob 0.386 (0.3857)	GS 35.281 (35.281)	mem 43.571
Train: [1][579/750]	BT 0.084 (1.144)	DT 0.002 (1.011)	loss 10.454 (10.454)	gnorm 882839.375 (882839.375)	prob 0.593 (0.5929)	GS 30.297 (30.297)	mem 43.533
Train: [1][580/750]	BT 0.066 (1.143)	DT 0.001 (1.010)	loss 10.007 (10.007)	gnorm 875802.625 (875802.625)	prob 0.935 (0.9348)	GS 31.797 (31.797)	mem 43.532
Train: [1][581/750]	BT 0.082 (1.141)	DT 0.002 (1.008)	loss 10.574 (10.574)	gnorm 896979.312 (896979.312)	prob 0.520 (0.5201)	GS 29.219 (29.219)	mem 43.532
Train: [1][582/750]	BT 0.155 (1.139)	DT 0.002 (1.006)	loss 10.332 (10.332)	gnorm 851584.312 (851584.312)	prob 1.016 (1.0156)	GS 31.422 (31.422)	mem 43.532
Train: [1][583/750]	BT 0.158 (1.137)	DT 0.024 (1.004)	loss 10.421 (10.421)	gnorm 930247.500 (930247.500)	prob 1.589 (1.5886)	GS 29.422 (29.422)	mem 43.531
Train: [1][584/750]	BT 0.175 (1.136)	DT 0.023 (1.003)	loss 9.748 (9.748)	gnorm 823040.500 (823040.500)	prob 1.690 (1.6896)	GS 32.188 (32.188)	mem 43.532
Train: [1][585/750]	BT 0.118 (1.134)	DT 0.012 (1.001)	loss 10.733 (10.733)	gnorm 960165.375 (960165.375)	prob 1.491 (1.4912)	GS 33.156 (33.156)	mem 43.534
Train: [1][586/750]	BT 1.309 (1.134)	DT 1.170 (1.001)	loss 10.607 (10.607)	gnorm 889058.000 (889058.000)	prob 0.726 (0.7263)	GS 30.219 (30.219)	mem 43.557
Train: [1][587/750]	BT 0.196 (1.133)	DT 0.004 (1.000)	loss 9.829 (9.829)	gnorm 911315.688 (911315.688)	prob 1.655 (1.6547)	GS 32.875 (32.875)	mem 43.557
Train: [1][588/750]	BT 8.158 (1.145)	DT 8.040 (1.012)	loss 9.866 (9.866)	gnorm 813863.125 (813863.125)	prob 1.985 (1.9848)	GS 31.047 (31.047)	mem 43.575
Train: [1][589/750]	BT 0.102 (1.143)	DT 0.015 (1.010)	loss 9.699 (9.699)	gnorm 826870.375 (826870.375)	prob 1.625 (1.6249)	GS 31.094 (31.094)	mem 43.575
Train: [1][590/750]	BT 0.081 (1.141)	DT 0.002 (1.008)	loss 9.783 (9.783)	gnorm 872629.250 (872629.250)	prob 1.700 (1.6997)	GS 35.188 (35.188)	mem 43.576
Train: [1][591/750]	BT 0.131 (1.139)	DT 0.027 (1.007)	loss 10.601 (10.601)	gnorm 826707.688 (826707.688)	prob 0.093 (0.0933)	GS 35.609 (35.609)	mem 43.619
Train: [1][592/750]	BT 0.228 (1.138)	DT 0.004 (1.005)	loss 10.102 (10.102)	gnorm 885838.750 (885838.750)	prob 1.018 (1.0176)	GS 32.906 (32.906)	mem 43.576
Train: [1][593/750]	BT 0.120 (1.136)	DT 0.002 (1.003)	loss 10.695 (10.695)	gnorm 981481.188 (981481.188)	prob 1.139 (1.1387)	GS 30.047 (30.047)	mem 43.576
Train: [1][594/750]	BT 0.077 (1.134)	DT 0.002 (1.001)	loss 10.523 (10.523)	gnorm 831373.375 (831373.375)	prob 0.759 (0.7588)	GS 35.328 (35.328)	mem 43.668
Train: [1][595/750]	BT 0.153 (1.133)	DT 0.001 (1.000)	loss 10.335 (10.335)	gnorm 879687.250 (879687.250)	prob 1.820 (1.8202)	GS 33.656 (33.656)	mem 43.785
Train: [1][596/750]	BT 0.090 (1.131)	DT 0.002 (0.998)	loss 10.546 (10.546)	gnorm 879572.062 (879572.062)	prob 1.600 (1.6005)	GS 33.969 (33.969)	mem 43.578
Train: [1][597/750]	BT 0.090 (1.129)	DT 0.001 (0.996)	loss 10.618 (10.618)	gnorm 924633.562 (924633.562)	prob 2.394 (2.3940)	GS 36.828 (36.828)	mem 43.643
Train: [1][598/750]	BT 5.138 (1.136)	DT 4.860 (1.003)	loss 10.315 (10.315)	gnorm 855734.750 (855734.750)	prob 2.585 (2.5855)	GS 36.453 (36.453)	mem 43.644
Train: [1][599/750]	BT 0.140 (1.134)	DT 0.011 (1.001)	loss 10.498 (10.498)	gnorm 944222.625 (944222.625)	prob 2.015 (2.0146)	GS 28.297 (28.297)	mem 43.539
Train: [1][600/750]	BT 8.255 (1.146)	DT 8.161 (1.013)	loss 10.471 (10.471)	gnorm 948613.562 (948613.562)	prob 1.908 (1.9082)	GS 34.203 (34.203)	mem 43.631
Train: [1][601/750]	BT 0.111 (1.144)	DT 0.002 (1.011)	loss 9.831 (9.831)	gnorm 902783.688 (902783.688)	prob 2.605 (2.6053)	GS 33.297 (33.297)	mem 43.592
Train: [1][602/750]	BT 0.141 (1.143)	DT 0.001 (1.010)	loss 10.595 (10.595)	gnorm 885654.125 (885654.125)	prob 1.480 (1.4803)	GS 32.641 (32.641)	mem 43.594
Train: [1][603/750]	BT 0.158 (1.141)	DT 0.002 (1.008)	loss 10.644 (10.644)	gnorm 911459.000 (911459.000)	prob 1.621 (1.6211)	GS 29.344 (29.344)	mem 43.580
Train: [1][604/750]	BT 0.151 (1.139)	DT 0.014 (1.006)	loss 10.196 (10.196)	gnorm 931953.125 (931953.125)	prob 1.892 (1.8917)	GS 34.688 (34.688)	mem 43.633
Train: [1][605/750]	BT 0.102 (1.138)	DT 0.002 (1.005)	loss 10.522 (10.522)	gnorm 910901.250 (910901.250)	prob 2.438 (2.4382)	GS 34.859 (34.859)	mem 43.731
Train: [1][606/750]	BT 0.150 (1.136)	DT 0.002 (1.003)	loss 10.204 (10.204)	gnorm 865853.688 (865853.688)	prob 2.444 (2.4444)	GS 31.922 (31.922)	mem 43.730
Train: [1][607/750]	BT 0.099 (1.134)	DT 0.001 (1.002)	loss 10.312 (10.312)	gnorm 894249.250 (894249.250)	prob 1.942 (1.9416)	GS 30.656 (30.656)	mem 43.595
Train: [1][608/750]	BT 0.136 (1.133)	DT 0.011 (1.000)	loss 11.106 (11.106)	gnorm 847268.938 (847268.938)	prob 1.138 (1.1379)	GS 41.828 (41.828)	mem 43.595
Train: [1][609/750]	BT 0.090 (1.131)	DT 0.003 (0.998)	loss 10.165 (10.165)	gnorm 937610.312 (937610.312)	prob 2.962 (2.9615)	GS 30.031 (30.031)	mem 43.595
Train: [1][610/750]	BT 1.184 (1.131)	DT 0.988 (0.998)	loss 10.363 (10.363)	gnorm 872200.312 (872200.312)	prob 2.430 (2.4299)	GS 34.734 (34.734)	mem 43.594
Train: [1][611/750]	BT 0.177 (1.130)	DT 0.002 (0.997)	loss 10.044 (10.044)	gnorm 989155.062 (989155.062)	prob 2.889 (2.8892)	GS 36.906 (36.906)	mem 43.594
Train: [1][612/750]	BT 11.230 (1.146)	DT 11.089 (1.013)	loss 9.816 (9.816)	gnorm 996255.562 (996255.562)	prob 2.906 (2.9063)	GS 33.578 (33.578)	mem 43.658
Train: [1][613/750]	BT 0.150 (1.144)	DT 0.007 (1.011)	loss 10.741 (10.741)	gnorm 911248.938 (911248.938)	prob 2.748 (2.7484)	GS 32.719 (32.719)	mem 43.601
Train: [1][614/750]	BT 0.178 (1.143)	DT 0.010 (1.010)	loss 9.737 (9.737)	gnorm 857713.938 (857713.938)	prob 3.154 (3.1541)	GS 37.328 (37.328)	mem 43.607
Train: [1][615/750]	BT 0.207 (1.141)	DT 0.002 (1.008)	loss 10.540 (10.540)	gnorm 994853.312 (994853.312)	prob 3.035 (3.0353)	GS 30.344 (30.344)	mem 43.660
Train: [1][616/750]	BT 0.092 (1.140)	DT 0.008 (1.007)	loss 10.581 (10.581)	gnorm 870224.250 (870224.250)	prob 2.158 (2.1580)	GS 35.438 (35.438)	mem 43.607
Train: [1][617/750]	BT 0.132 (1.138)	DT 0.002 (1.005)	loss 9.836 (9.836)	gnorm 820230.812 (820230.812)	prob 2.889 (2.8887)	GS 29.047 (29.047)	mem 43.614
Train: [1][618/750]	BT 0.130 (1.136)	DT 0.002 (1.003)	loss 10.458 (10.458)	gnorm 1067393.000 (1067393.000)	prob 1.939 (1.9387)	GS 35.016 (35.016)	mem 43.614
Train: [1][619/750]	BT 0.170 (1.135)	DT 0.006 (1.002)	loss 10.996 (10.996)	gnorm 860349.938 (860349.938)	prob 1.856 (1.8559)	GS 33.078 (33.078)	mem 43.614
Train: [1][620/750]	BT 0.180 (1.133)	DT 0.008 (1.000)	loss 9.987 (9.987)	gnorm 819076.000 (819076.000)	prob 2.524 (2.5245)	GS 33.125 (33.125)	mem 43.614
Train: [1][621/750]	BT 0.148 (1.132)	DT 0.002 (0.998)	loss 10.032 (10.032)	gnorm 864965.188 (864965.188)	prob 3.021 (3.0211)	GS 34.828 (34.828)	mem 43.614
Train: [1][622/750]	BT 0.934 (1.131)	DT 0.766 (0.998)	loss 10.073 (10.073)	gnorm 854227.312 (854227.312)	prob 2.182 (2.1816)	GS 33.094 (33.094)	mem 43.613
Train: [1][623/750]	BT 0.117 (1.130)	DT 0.002 (0.997)	loss 10.264 (10.264)	gnorm 868926.750 (868926.750)	prob 2.390 (2.3897)	GS 29.734 (29.734)	mem 43.626
Train: [1][624/750]	BT 11.003 (1.146)	DT 10.910 (1.012)	loss 9.209 (9.209)	gnorm 790797.625 (790797.625)	prob 3.778 (3.7781)	GS 36.391 (36.391)	mem 43.589
Train: [1][625/750]	BT 0.077 (1.144)	DT 0.001 (1.011)	loss 10.104 (10.104)	gnorm 876610.188 (876610.188)	prob 2.697 (2.6968)	GS 30.453 (30.453)	mem 43.588
Train: [1][626/750]	BT 0.089 (1.142)	DT 0.002 (1.009)	loss 10.530 (10.530)	gnorm 797142.812 (797142.812)	prob 2.530 (2.5303)	GS 31.359 (31.359)	mem 43.588
Train: [1][627/750]	BT 0.083 (1.141)	DT 0.001 (1.008)	loss 10.279 (10.279)	gnorm 833555.062 (833555.062)	prob 2.684 (2.6842)	GS 28.953 (28.953)	mem 43.623
Train: [1][628/750]	BT 0.093 (1.139)	DT 0.002 (1.006)	loss 9.827 (9.827)	gnorm 829969.250 (829969.250)	prob 3.164 (3.1639)	GS 35.500 (35.500)	mem 43.657
Train: [1][629/750]	BT 0.096 (1.137)	DT 0.002 (1.004)	loss 10.187 (10.187)	gnorm 877229.812 (877229.812)	prob 2.557 (2.5566)	GS 31.750 (31.750)	mem 43.748
Train: [1][630/750]	BT 0.122 (1.136)	DT 0.001 (1.003)	loss 10.590 (10.590)	gnorm 797675.688 (797675.688)	prob 2.319 (2.3187)	GS 35.781 (35.781)	mem 43.788
Train: [1][631/750]	BT 0.139 (1.134)	DT 0.009 (1.001)	loss 10.167 (10.167)	gnorm 846088.125 (846088.125)	prob 2.738 (2.7376)	GS 32.594 (32.594)	mem 43.523
Train: [1][632/750]	BT 0.206 (1.133)	DT 0.002 (1.000)	loss 10.948 (10.948)	gnorm 899410.938 (899410.938)	prob 1.609 (1.6090)	GS 33.156 (33.156)	mem 43.523
Train: [1][633/750]	BT 0.146 (1.131)	DT 0.001 (0.998)	loss 10.417 (10.417)	gnorm 888820.000 (888820.000)	prob 2.545 (2.5448)	GS 32.016 (32.016)	mem 43.560
Train: [1][634/750]	BT 0.415 (1.130)	DT 0.220 (0.997)	loss 10.342 (10.342)	gnorm 925705.312 (925705.312)	prob 2.081 (2.0813)	GS 38.875 (38.875)	mem 43.545
Train: [1][635/750]	BT 0.090 (1.128)	DT 0.002 (0.995)	loss 10.183 (10.183)	gnorm 897548.000 (897548.000)	prob 1.928 (1.9279)	GS 33.125 (33.125)	mem 43.545
Train: [1][636/750]	BT 12.659 (1.146)	DT 12.521 (1.013)	loss 10.475 (10.475)	gnorm 891169.000 (891169.000)	prob 2.071 (2.0711)	GS 32.484 (32.484)	mem 43.633
Train: [1][637/750]	BT 0.194 (1.145)	DT 0.014 (1.012)	loss 10.562 (10.562)	gnorm 915068.500 (915068.500)	prob 1.686 (1.6855)	GS 31.156 (31.156)	mem 43.592
Train: [1][638/750]	BT 0.122 (1.143)	DT 0.007 (1.010)	loss 10.146 (10.146)	gnorm 801607.812 (801607.812)	prob 2.078 (2.0785)	GS 32.328 (32.328)	mem 43.593
Train: [1][639/750]	BT 0.099 (1.142)	DT 0.002 (1.009)	loss 10.807 (10.807)	gnorm 895969.562 (895969.562)	prob 1.629 (1.6291)	GS 30.531 (30.531)	mem 43.593
Train: [1][640/750]	BT 0.074 (1.140)	DT 0.002 (1.007)	loss 10.270 (10.270)	gnorm 891411.750 (891411.750)	prob 1.407 (1.4067)	GS 27.453 (27.453)	mem 43.595
Train: [1][641/750]	BT 0.104 (1.138)	DT 0.002 (1.006)	loss 10.447 (10.447)	gnorm 873272.375 (873272.375)	prob 1.628 (1.6283)	GS 31.578 (31.578)	mem 43.595
Train: [1][642/750]	BT 0.143 (1.137)	DT 0.008 (1.004)	loss 9.810 (9.810)	gnorm 878711.312 (878711.312)	prob 2.321 (2.3208)	GS 34.938 (34.938)	mem 43.595
Train: [1][643/750]	BT 0.099 (1.135)	DT 0.006 (1.002)	loss 10.198 (10.198)	gnorm 921226.188 (921226.188)	prob 2.641 (2.6411)	GS 33.188 (33.188)	mem 43.595
Train: [1][644/750]	BT 0.091 (1.134)	DT 0.003 (1.001)	loss 10.260 (10.260)	gnorm 908117.500 (908117.500)	prob 2.528 (2.5284)	GS 36.625 (36.625)	mem 43.595
Train: [1][645/750]	BT 0.115 (1.132)	DT 0.003 (0.999)	loss 10.495 (10.495)	gnorm 790979.562 (790979.562)	prob 2.228 (2.2280)	GS 29.844 (29.844)	mem 43.595
Train: [1][646/750]	BT 0.090 (1.130)	DT 0.002 (0.998)	loss 11.281 (11.281)	gnorm 975959.438 (975959.438)	prob 1.320 (1.3198)	GS 30.422 (30.422)	mem 43.594
Train: [1][647/750]	BT 0.148 (1.129)	DT 0.003 (0.996)	loss 10.571 (10.571)	gnorm 886153.375 (886153.375)	prob 2.362 (2.3621)	GS 27.766 (27.766)	mem 43.637
Train: [1][648/750]	BT 12.493 (1.146)	DT 12.414 (1.014)	loss 10.012 (10.012)	gnorm 839205.938 (839205.938)	prob 2.175 (2.1752)	GS 34.797 (34.797)	mem 43.687
Train: [1][649/750]	BT 0.134 (1.145)	DT 0.002 (1.012)	loss 10.123 (10.123)	gnorm 976303.750 (976303.750)	prob 2.723 (2.7231)	GS 46.094 (46.094)	mem 43.597
Train: [1][650/750]	BT 0.090 (1.143)	DT 0.002 (1.011)	loss 10.306 (10.306)	gnorm 870063.500 (870063.500)	prob 1.989 (1.9894)	GS 33.594 (33.594)	mem 43.551
Train: [1][651/750]	BT 0.090 (1.142)	DT 0.002 (1.009)	loss 10.181 (10.181)	gnorm 822400.812 (822400.812)	prob 2.488 (2.4880)	GS 28.781 (28.781)	mem 43.551
Train: [1][652/750]	BT 0.089 (1.140)	DT 0.002 (1.008)	loss 9.586 (9.586)	gnorm 888443.875 (888443.875)	prob 3.425 (3.4248)	GS 31.812 (31.812)	mem 43.551
Train: [1][653/750]	BT 0.105 (1.138)	DT 0.002 (1.006)	loss 10.423 (10.423)	gnorm 826975.625 (826975.625)	prob 3.287 (3.2867)	GS 30.594 (30.594)	mem 43.553
Train: [1][654/750]	BT 0.134 (1.137)	DT 0.002 (1.005)	loss 10.580 (10.580)	gnorm 864803.000 (864803.000)	prob 2.232 (2.2321)	GS 32.016 (32.016)	mem 43.554
Train: [1][655/750]	BT 0.186 (1.135)	DT 0.002 (1.003)	loss 10.138 (10.138)	gnorm 843310.938 (843310.938)	prob 2.360 (2.3602)	GS 32.297 (32.297)	mem 43.553
Train: [1][656/750]	BT 0.146 (1.134)	DT 0.027 (1.002)	loss 11.239 (11.239)	gnorm 914455.188 (914455.188)	prob 1.292 (1.2921)	GS 28.797 (28.797)	mem 43.553
Train: [1][657/750]	BT 0.180 (1.132)	DT 0.012 (1.000)	loss 9.862 (9.862)	gnorm 792703.688 (792703.688)	prob 2.319 (2.3192)	GS 28.375 (28.375)	mem 43.555
Train: [1][658/750]	BT 0.118 (1.131)	DT 0.003 (0.999)	loss 10.315 (10.315)	gnorm 935387.688 (935387.688)	prob 2.044 (2.0442)	GS 37.172 (37.172)	mem 43.621
Train: [1][659/750]	BT 0.125 (1.129)	DT 0.004 (0.997)	loss 10.114 (10.114)	gnorm 852748.750 (852748.750)	prob 2.720 (2.7196)	GS 31.391 (31.391)	mem 43.557
Train: [1][660/750]	BT 10.302 (1.143)	DT 10.144 (1.011)	loss 10.564 (10.564)	gnorm 840023.688 (840023.688)	prob 2.009 (2.0086)	GS 33.359 (33.359)	mem 43.662
Train: [1][661/750]	BT 0.078 (1.142)	DT 0.002 (1.009)	loss 10.164 (10.164)	gnorm 849002.000 (849002.000)	prob 2.498 (2.4976)	GS 29.578 (29.578)	mem 43.662
Train: [1][662/750]	BT 0.087 (1.140)	DT 0.002 (1.008)	loss 9.803 (9.803)	gnorm 779319.188 (779319.188)	prob 3.371 (3.3710)	GS 31.141 (31.141)	mem 43.662
Train: [1][663/750]	BT 0.110 (1.138)	DT 0.001 (1.006)	loss 9.829 (9.829)	gnorm 833569.375 (833569.375)	prob 3.246 (3.2461)	GS 30.109 (30.109)	mem 43.661
Train: [1][664/750]	BT 0.121 (1.137)	DT 0.004 (1.005)	loss 10.134 (10.134)	gnorm 885390.812 (885390.812)	prob 2.806 (2.8063)	GS 35.016 (35.016)	mem 43.661
Train: [1][665/750]	BT 0.105 (1.135)	DT 0.004 (1.003)	loss 10.135 (10.135)	gnorm 897137.188 (897137.188)	prob 2.003 (2.0031)	GS 30.656 (30.656)	mem 43.661
Train: [1][666/750]	BT 0.095 (1.134)	DT 0.007 (1.002)	loss 9.970 (9.970)	gnorm 849936.250 (849936.250)	prob 2.470 (2.4703)	GS 32.344 (32.344)	mem 43.689
Train: [1][667/750]	BT 0.123 (1.132)	DT 0.002 (1.000)	loss 10.554 (10.554)	gnorm 973948.938 (973948.938)	prob 1.412 (1.4124)	GS 36.688 (36.688)	mem 43.784
Train: [1][668/750]	BT 0.195 (1.131)	DT 0.002 (0.999)	loss 10.180 (10.180)	gnorm 899801.812 (899801.812)	prob 1.696 (1.6961)	GS 32.016 (32.016)	mem 43.777
Train: [1][669/750]	BT 0.128 (1.129)	DT 0.011 (0.997)	loss 10.346 (10.346)	gnorm 806272.625 (806272.625)	prob 1.231 (1.2314)	GS 34.203 (34.203)	mem 43.664
Train: [1][670/750]	BT 0.242 (1.128)	DT 0.002 (0.996)	loss 10.045 (10.045)	gnorm 853124.312 (853124.312)	prob 2.315 (2.3148)	GS 34.422 (34.422)	mem 43.697
Train: [1][671/750]	BT 0.160 (1.127)	DT 0.005 (0.994)	loss 10.764 (10.764)	gnorm 821971.312 (821971.312)	prob 0.977 (0.9766)	GS 27.156 (27.156)	mem 43.665
Train: [1][672/750]	BT 11.747 (1.142)	DT 11.668 (1.010)	loss 10.254 (10.254)	gnorm 858935.812 (858935.812)	prob 1.622 (1.6216)	GS 31.750 (31.750)	mem 43.823
Train: [1][673/750]	BT 0.128 (1.141)	DT 0.005 (1.009)	loss 9.902 (9.902)	gnorm 824250.125 (824250.125)	prob 2.371 (2.3709)	GS 29.031 (29.031)	mem 44.054
Train: [1][674/750]	BT 0.113 (1.139)	DT 0.002 (1.007)	loss 10.228 (10.228)	gnorm 809860.062 (809860.062)	prob 1.719 (1.7192)	GS 31.828 (31.828)	mem 43.910
Train: [1][675/750]	BT 0.077 (1.138)	DT 0.003 (1.006)	loss 10.214 (10.214)	gnorm 827778.938 (827778.938)	prob 2.523 (2.5231)	GS 29.797 (29.797)	mem 43.600
Train: [1][676/750]	BT 0.090 (1.136)	DT 0.001 (1.004)	loss 10.970 (10.970)	gnorm 796430.000 (796430.000)	prob 1.142 (1.1424)	GS 28.328 (28.328)	mem 43.599
Train: [1][677/750]	BT 0.132 (1.135)	DT 0.014 (1.003)	loss 10.421 (10.421)	gnorm 897699.938 (897699.938)	prob 2.463 (2.4630)	GS 29.719 (29.719)	mem 43.599
Train: [1][678/750]	BT 0.081 (1.133)	DT 0.003 (1.001)	loss 10.260 (10.260)	gnorm 854007.688 (854007.688)	prob 1.528 (1.5278)	GS 33.484 (33.484)	mem 43.606
Train: [1][679/750]	BT 0.090 (1.132)	DT 0.001 (1.000)	loss 10.535 (10.535)	gnorm 891272.062 (891272.062)	prob 1.936 (1.9358)	GS 32.812 (32.812)	mem 43.786
Train: [1][680/750]	BT 0.363 (1.131)	DT 0.003 (0.998)	loss 9.996 (9.996)	gnorm 748832.062 (748832.062)	prob 2.087 (2.0871)	GS 39.703 (39.703)	mem 43.808
Train: [1][681/750]	BT 0.177 (1.129)	DT 0.002 (0.997)	loss 10.996 (10.996)	gnorm 863085.812 (863085.812)	prob 1.221 (1.2207)	GS 34.266 (34.266)	mem 43.603
Train: [1][682/750]	BT 0.077 (1.128)	DT 0.002 (0.995)	loss 9.814 (9.814)	gnorm 833240.500 (833240.500)	prob 2.671 (2.6708)	GS 32.781 (32.781)	mem 43.604
Train: [1][683/750]	BT 0.156 (1.126)	DT 0.001 (0.994)	loss 10.166 (10.166)	gnorm 881125.625 (881125.625)	prob 2.362 (2.3616)	GS 36.422 (36.422)	mem 43.640
Train: [1][684/750]	BT 10.503 (1.140)	DT 10.379 (1.008)	loss 10.222 (10.222)	gnorm 855489.188 (855489.188)	prob 2.199 (2.1986)	GS 33.484 (33.484)	mem 43.683
Train: [1][685/750]	BT 0.132 (1.138)	DT 0.001 (1.006)	loss 10.383 (10.383)	gnorm 881180.688 (881180.688)	prob 1.628 (1.6280)	GS 29.703 (29.703)	mem 43.660
Train: [1][686/750]	BT 0.081 (1.137)	DT 0.002 (1.005)	loss 10.366 (10.366)	gnorm 942871.875 (942871.875)	prob 1.346 (1.3457)	GS 31.031 (31.031)	mem 43.660
Train: [1][687/750]	BT 0.085 (1.135)	DT 0.002 (1.003)	loss 10.132 (10.132)	gnorm 822832.688 (822832.688)	prob 2.186 (2.1858)	GS 32.484 (32.484)	mem 43.755
Train: [1][688/750]	BT 0.214 (1.134)	DT 0.003 (1.002)	loss 10.436 (10.436)	gnorm 812266.062 (812266.062)	prob 1.566 (1.5659)	GS 33.172 (33.172)	mem 43.769
Train: [1][689/750]	BT 0.105 (1.133)	DT 0.015 (1.000)	loss 9.873 (9.873)	gnorm 811421.125 (811421.125)	prob 2.297 (2.2969)	GS 30.031 (30.031)	mem 43.665
Train: [1][690/750]	BT 0.098 (1.131)	DT 0.002 (0.999)	loss 10.730 (10.730)	gnorm 898685.750 (898685.750)	prob 1.424 (1.4238)	GS 35.781 (35.781)	mem 43.677
Train: [1][691/750]	BT 0.153 (1.130)	DT 0.010 (0.998)	loss 10.693 (10.693)	gnorm 882228.000 (882228.000)	prob 1.331 (1.3306)	GS 30.297 (30.297)	mem 43.739
Train: [1][692/750]	BT 0.162 (1.128)	DT 0.002 (0.996)	loss 10.901 (10.901)	gnorm 941098.500 (941098.500)	prob 0.964 (0.9636)	GS 31.812 (31.812)	mem 43.792
Train: [1][693/750]	BT 0.130 (1.127)	DT 0.006 (0.995)	loss 10.498 (10.498)	gnorm 820593.188 (820593.188)	prob 1.654 (1.6536)	GS 33.750 (33.750)	mem 43.939
Train: [1][694/750]	BT 0.141 (1.125)	DT 0.026 (0.993)	loss 10.173 (10.173)	gnorm 811960.188 (811960.188)	prob 1.899 (1.8993)	GS 33.469 (33.469)	mem 43.903
Train: [1][695/750]	BT 0.161 (1.124)	DT 0.018 (0.992)	loss 10.386 (10.386)	gnorm 872162.500 (872162.500)	prob 1.462 (1.4624)	GS 30.500 (30.500)	mem 43.857
Train: [1][696/750]	BT 8.763 (1.135)	DT 8.654 (1.003)	loss 10.316 (10.316)	gnorm 903186.875 (903186.875)	prob 1.821 (1.8209)	GS 34.906 (34.906)	mem 44.022
Train: [1][697/750]	BT 0.173 (1.134)	DT 0.002 (1.001)	loss 10.473 (10.473)	gnorm 832772.312 (832772.312)	prob 1.957 (1.9572)	GS 28.328 (28.328)	mem 44.065
Train: [1][698/750]	BT 0.139 (1.132)	DT 0.004 (1.000)	loss 10.656 (10.656)	gnorm 862443.500 (862443.500)	prob 1.176 (1.1764)	GS 33.984 (33.984)	mem 43.598
Train: [1][699/750]	BT 0.207 (1.131)	DT 0.005 (0.999)	loss 10.129 (10.129)	gnorm 821592.438 (821592.438)	prob 1.678 (1.6782)	GS 27.281 (27.281)	mem 43.598
Train: [1][700/750]	BT 0.298 (1.130)	DT 0.025 (0.997)	loss 10.195 (10.195)	gnorm 882595.250 (882595.250)	prob 1.313 (1.3126)	GS 35.281 (35.281)	mem 43.652
Train: [1][701/750]	BT 0.136 (1.128)	DT 0.003 (0.996)	loss 10.142 (10.142)	gnorm 799697.750 (799697.750)	prob 2.235 (2.2353)	GS 30.688 (30.688)	mem 43.598
Train: [1][702/750]	BT 0.189 (1.127)	DT 0.004 (0.994)	loss 10.639 (10.639)	gnorm 861837.250 (861837.250)	prob 1.172 (1.1725)	GS 32.922 (32.922)	mem 43.597
Train: [1][703/750]	BT 0.151 (1.126)	DT 0.001 (0.993)	loss 10.184 (10.184)	gnorm 761787.188 (761787.188)	prob 1.968 (1.9681)	GS 28.078 (28.078)	mem 43.597
Train: [1][704/750]	BT 0.135 (1.124)	DT 0.019 (0.992)	loss 9.592 (9.592)	gnorm 774515.625 (774515.625)	prob 2.561 (2.5610)	GS 32.766 (32.766)	mem 43.597
Train: [1][705/750]	BT 0.097 (1.123)	DT 0.002 (0.990)	loss 10.312 (10.312)	gnorm 821374.938 (821374.938)	prob 1.787 (1.7867)	GS 40.953 (40.953)	mem 43.598
Train: [1][706/750]	BT 0.184 (1.121)	DT 0.002 (0.989)	loss 11.088 (11.088)	gnorm 917763.625 (917763.625)	prob 0.333 (0.3328)	GS 33.156 (33.156)	mem 43.661
Train: [1][707/750]	BT 0.288 (1.120)	DT 0.029 (0.987)	loss 10.205 (10.205)	gnorm 738274.938 (738274.938)	prob 1.344 (1.3438)	GS 29.500 (29.500)	mem 43.746
Train: [1][708/750]	BT 12.222 (1.136)	DT 12.130 (1.003)	loss 10.345 (10.345)	gnorm 825318.188 (825318.188)	prob 1.099 (1.0994)	GS 37.625 (37.625)	mem 43.771
Train: [1][709/750]	BT 0.123 (1.134)	DT 0.002 (1.002)	loss 10.411 (10.411)	gnorm 840024.500 (840024.500)	prob 1.370 (1.3701)	GS 26.500 (26.500)	mem 43.691
Train: [1][710/750]	BT 0.081 (1.133)	DT 0.002 (1.000)	loss 10.787 (10.787)	gnorm 879608.000 (879608.000)	prob 0.487 (0.4871)	GS 32.812 (32.812)	mem 43.692
Train: [1][711/750]	BT 0.153 (1.132)	DT 0.005 (0.999)	loss 10.082 (10.082)	gnorm 810502.000 (810502.000)	prob 2.313 (2.3126)	GS 29.703 (29.703)	mem 43.694
Train: [1][712/750]	BT 0.259 (1.130)	DT 0.043 (0.998)	loss 9.882 (9.882)	gnorm 876868.125 (876868.125)	prob 1.908 (1.9078)	GS 33.703 (33.703)	mem 43.695
Train: [1][713/750]	BT 0.088 (1.129)	DT 0.011 (0.996)	loss 10.144 (10.144)	gnorm 815274.938 (815274.938)	prob 1.044 (1.0437)	GS 32.703 (32.703)	mem 43.763
Train: [1][714/750]	BT 0.358 (1.128)	DT 0.129 (0.995)	loss 10.121 (10.121)	gnorm 820212.812 (820212.812)	prob 1.062 (1.0618)	GS 35.703 (35.703)	mem 43.606
Train: [1][715/750]	BT 0.210 (1.126)	DT 0.012 (0.994)	loss 10.434 (10.434)	gnorm 807667.812 (807667.812)	prob 0.129 (0.1289)	GS 35.844 (35.844)	mem 43.607
Train: [1][716/750]	BT 0.094 (1.125)	DT 0.008 (0.992)	loss 10.318 (10.318)	gnorm 787260.938 (787260.938)	prob 0.563 (0.5626)	GS 34.031 (34.031)	mem 43.607
Train: [1][717/750]	BT 0.259 (1.124)	DT 0.003 (0.991)	loss 10.501 (10.501)	gnorm 815313.312 (815313.312)	prob 0.512 (0.5125)	GS 30.016 (30.016)	mem 43.606
Train: [1][718/750]	BT 0.258 (1.123)	DT 0.016 (0.990)	loss 10.062 (10.062)	gnorm 850568.062 (850568.062)	prob 1.403 (1.4029)	GS 32.812 (32.812)	mem 43.608
Train: [1][719/750]	BT 0.114 (1.121)	DT 0.003 (0.988)	loss 10.451 (10.451)	gnorm 824722.750 (824722.750)	prob 0.442 (0.4422)	GS 32.219 (32.219)	mem 43.608
Train: [1][720/750]	BT 10.494 (1.134)	DT 10.399 (1.001)	loss 10.427 (10.427)	gnorm 763742.188 (763742.188)	prob 0.272 (0.2723)	GS 32.531 (32.531)	mem 43.679
Train: [1][721/750]	BT 0.122 (1.133)	DT 0.009 (1.000)	loss 10.070 (10.070)	gnorm 869907.000 (869907.000)	prob 1.940 (1.9396)	GS 28.891 (28.891)	mem 43.680
Train: [1][722/750]	BT 0.133 (1.131)	DT 0.012 (0.999)	loss 10.354 (10.354)	gnorm 751185.875 (751185.875)	prob 0.941 (0.9407)	GS 32.531 (32.531)	mem 43.680
Train: [1][723/750]	BT 0.161 (1.130)	DT 0.010 (0.997)	loss 10.751 (10.751)	gnorm 868570.562 (868570.562)	prob 1.430 (1.4303)	GS 30.766 (30.766)	mem 43.680
Train: [1][724/750]	BT 0.761 (1.130)	DT 0.495 (0.996)	loss 9.841 (9.841)	gnorm 745508.938 (745508.938)	prob 1.873 (1.8733)	GS 34.641 (34.641)	mem 43.682
Train: [1][725/750]	BT 0.100 (1.128)	DT 0.002 (0.995)	loss 10.323 (10.323)	gnorm 823096.625 (823096.625)	prob 2.245 (2.2450)	GS 32.531 (32.531)	mem 43.731
Train: [1][726/750]	BT 0.092 (1.127)	DT 0.003 (0.994)	loss 10.262 (10.262)	gnorm 797033.438 (797033.438)	prob 1.813 (1.8133)	GS 33.641 (33.641)	mem 43.857
Train: [1][727/750]	BT 0.211 (1.125)	DT 0.001 (0.992)	loss 9.949 (9.949)	gnorm 852028.875 (852028.875)	prob 2.998 (2.9982)	GS 29.297 (29.297)	mem 43.907
Train: [1][728/750]	BT 0.186 (1.124)	DT 0.094 (0.991)	loss 10.824 (10.824)	gnorm 872791.688 (872791.688)	prob 1.343 (1.3426)	GS 36.500 (36.500)	mem 43.686
Train: [1][729/750]	BT 0.149 (1.123)	DT 0.005 (0.990)	loss 10.022 (10.022)	gnorm 819768.438 (819768.438)	prob 1.351 (1.3508)	GS 36.062 (36.062)	mem 43.687
Train: [1][730/750]	BT 0.155 (1.122)	DT 0.008 (0.988)	loss 10.470 (10.470)	gnorm 840847.750 (840847.750)	prob 0.812 (0.8115)	GS 33.438 (33.438)	mem 43.700
Train: [1][731/750]	BT 0.137 (1.120)	DT 0.004 (0.987)	loss 10.147 (10.147)	gnorm 858474.688 (858474.688)	prob 0.923 (0.9229)	GS 28.109 (28.109)	mem 43.728
Train: [1][732/750]	BT 11.591 (1.135)	DT 11.513 (1.001)	loss 10.080 (10.080)	gnorm 1039570.625 (1039570.625)	prob 0.483 (0.4829)	GS 35.125 (35.125)	mem 43.238
Train: [1][733/750]	BT 0.064 (1.133)	DT 0.001 (1.000)	loss 10.609 (10.609)	gnorm 813983.688 (813983.688)	prob 1.008 (1.0078)	GS 29.641 (29.641)	mem 43.127
Train: [1][734/750]	BT 0.153 (1.132)	DT 0.001 (0.999)	loss 10.131 (10.131)	gnorm 826557.562 (826557.562)	prob 0.958 (0.9578)	GS 32.453 (32.453)	mem 43.127
Train: [1][735/750]	BT 0.109 (1.130)	DT 0.002 (0.997)	loss 9.910 (9.910)	gnorm 858731.125 (858731.125)	prob 0.953 (0.9527)	GS 30.000 (30.000)	mem 43.127
Train: [1][736/750]	BT 0.134 (1.129)	DT 0.004 (0.996)	loss 10.076 (10.076)	gnorm 753742.375 (753742.375)	prob 1.314 (1.3142)	GS 30.531 (30.531)	mem 43.059
Train: [1][737/750]	BT 0.130 (1.128)	DT 0.006 (0.995)	loss 10.305 (10.305)	gnorm 793962.062 (793962.062)	prob 1.527 (1.5273)	GS 28.641 (28.641)	mem 43.126
Train: [1][738/750]	BT 0.157 (1.126)	DT 0.046 (0.993)	loss 10.125 (10.125)	gnorm 828188.188 (828188.188)	prob 1.162 (1.1624)	GS 33.859 (33.859)	mem 43.067
Train: [1][739/750]	BT 0.070 (1.125)	DT 0.002 (0.992)	loss 10.472 (10.472)	gnorm 825751.438 (825751.438)	prob -0.022 (-0.0224)	GS 29.922 (29.922)	mem 43.084
Train: [1][740/750]	BT 0.105 (1.123)	DT 0.002 (0.991)	loss 10.466 (10.466)	gnorm 822725.750 (822725.750)	prob 0.606 (0.6058)	GS 29.234 (29.234)	mem 43.113
Train: [1][741/750]	BT 0.094 (1.122)	DT 0.002 (0.989)	loss 10.745 (10.745)	gnorm 883688.125 (883688.125)	prob 0.565 (0.5648)	GS 33.875 (33.875)	mem 43.205
Train: [1][742/750]	BT 0.124 (1.121)	DT 0.004 (0.988)	loss 10.052 (10.052)	gnorm 821909.562 (821909.562)	prob 0.832 (0.8317)	GS 32.219 (32.219)	mem 43.094
Train: [1][743/750]	BT 0.083 (1.119)	DT 0.002 (0.987)	loss 10.771 (10.771)	gnorm 840844.062 (840844.062)	prob 0.103 (0.1028)	GS 31.656 (31.656)	mem 43.095
Train: [1][744/750]	BT 5.171 (1.125)	DT 5.084 (0.992)	loss 9.781 (9.781)	gnorm 784038.750 (784038.750)	prob 0.823 (0.8229)	GS 32.000 (32.000)	mem 11.325
Train: [1][745/750]	BT 0.066 (1.123)	DT 0.002 (0.991)	loss 10.335 (10.335)	gnorm 1085939.500 (1085939.500)	prob 0.063 (0.0627)	GS 29.750 (29.750)	mem 11.325
Train: [1][746/750]	BT 0.093 (1.122)	DT 0.001 (0.990)	loss 10.258 (10.258)	gnorm 1257420.000 (1257420.000)	prob -0.004 (-0.0039)	GS 36.906 (36.906)	mem 11.325
Train: [1][747/750]	BT 0.073 (1.121)	DT 0.002 (0.988)	loss 10.498 (10.498)	gnorm 1134999.500 (1134999.500)	prob 0.222 (0.2222)	GS 32.375 (32.375)	mem 11.325
Train: [1][748/750]	BT 0.057 (1.119)	DT 0.001 (0.987)	loss 10.110 (10.110)	gnorm 1038608.250 (1038608.250)	prob 0.472 (0.4717)	GS 34.750 (34.750)	mem 11.325
Train: [1][749/750]	BT 0.060 (1.118)	DT 0.001 (0.986)	loss 10.140 (10.140)	gnorm 1023228.000 (1023228.000)	prob 0.950 (0.9497)	GS 30.719 (30.719)	mem 11.325
Train: [1][750/750]	BT 0.072 (1.116)	DT 0.001 (0.984)	loss 10.508 (10.508)	gnorm 1082779.500 (1082779.500)	prob 1.188 (1.1876)	GS 37.062 (37.062)	mem 11.325
Train: [1][751/750]	BT 0.065 (1.115)	DT 0.001 (0.983)	loss 9.684 (9.684)	gnorm 1098927.875 (1098927.875)	prob 1.922 (1.9222)	GS 29.156 (29.156)	mem 11.325
Train: [1][752/750]	BT 0.067 (1.114)	DT 0.001 (0.982)	loss 10.165 (10.165)	gnorm 1179999.500 (1179999.500)	prob 1.681 (1.6806)	GS 32.406 (32.406)	mem 11.467
Train: [1][753/750]	BT 0.086 (1.112)	DT 0.001 (0.980)	loss 9.650 (9.650)	gnorm 1315049.500 (1315049.500)	prob 2.134 (2.1341)	GS 58.281 (58.281)	mem 11.326
Train: [1][754/750]	BT 0.090 (1.111)	DT 0.011 (0.979)	loss 9.832 (9.832)	gnorm 1361372.375 (1361372.375)	prob 1.494 (1.4944)	GS 36.000 (36.000)	mem 11.326
Train: [1][755/750]	BT 0.074 (1.109)	DT 0.001 (0.978)	loss 9.439 (9.439)	gnorm 1146050.750 (1146050.750)	prob 1.581 (1.5814)	GS 32.906 (32.906)	mem 11.423
Train: [1][756/750]	BT 1.576 (1.110)	DT 1.503 (0.978)	loss 10.170 (10.170)	gnorm 1252893.250 (1252893.250)	prob 1.305 (1.3052)	GS 32.188 (32.188)	mem 11.291
epoch 1, total time 839.44
==> Saving...
==> Saving...
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [2][1/750]	BT 22.997 (22.997)	DT 22.747 (22.747)	loss 9.647 (9.647)	gnorm 852078.312 (852078.312)	prob 1.744 (1.7439)	GS 37.625 (37.625)	mem 42.282
Train: [2][2/750]	BT 0.121 (11.559)	DT 0.012 (11.380)	loss 10.929 (10.929)	gnorm 895816.438 (895816.438)	prob 0.414 (0.4139)	GS 31.797 (31.797)	mem 42.235
Train: [2][3/750]	BT 0.151 (7.756)	DT 0.001 (7.587)	loss 10.427 (10.427)	gnorm 822231.250 (822231.250)	prob 2.351 (2.3506)	GS 32.812 (32.812)	mem 42.316
Train: [2][4/750]	BT 0.865 (6.033)	DT 0.730 (5.873)	loss 10.100 (10.100)	gnorm 809575.375 (809575.375)	prob 1.943 (1.9430)	GS 34.328 (34.328)	mem 42.256
Train: [2][5/750]	BT 0.088 (4.844)	DT 0.002 (4.698)	loss 10.382 (10.382)	gnorm 845825.500 (845825.500)	prob 2.676 (2.6760)	GS 30.828 (30.828)	mem 42.257
Train: [2][6/750]	BT 0.091 (4.052)	DT 0.002 (3.916)	loss 9.761 (9.761)	gnorm 852391.688 (852391.688)	prob 2.389 (2.3893)	GS 33.391 (33.391)	mem 42.258
Train: [2][7/750]	BT 0.163 (3.496)	DT 0.002 (3.357)	loss 10.892 (10.892)	gnorm 723839.188 (723839.188)	prob 1.489 (1.4885)	GS 30.609 (30.609)	mem 42.278
Train: [2][8/750]	BT 0.171 (3.081)	DT 0.010 (2.938)	loss 10.577 (10.577)	gnorm 878926.312 (878926.312)	prob 1.673 (1.6726)	GS 39.719 (39.719)	mem 42.411
Train: [2][9/750]	BT 0.356 (2.778)	DT 0.007 (2.613)	loss 9.848 (9.848)	gnorm 762310.250 (762310.250)	prob 2.795 (2.7948)	GS 28.781 (28.781)	mem 42.287
Train: [2][10/750]	BT 0.160 (2.516)	DT 0.009 (2.352)	loss 9.964 (9.964)	gnorm 793804.438 (793804.438)	prob 2.436 (2.4356)	GS 35.828 (35.828)	mem 42.289
Train: [2][11/750]	BT 0.941 (2.373)	DT 0.854 (2.216)	loss 10.387 (10.387)	gnorm 780584.125 (780584.125)	prob 1.850 (1.8503)	GS 34.750 (34.750)	mem 42.354
Train: [2][12/750]	BT 0.145 (2.187)	DT 0.001 (2.031)	loss 10.017 (10.017)	gnorm 848416.938 (848416.938)	prob 2.639 (2.6389)	GS 34.375 (34.375)	mem 42.338
Train: [2][13/750]	BT 12.633 (2.991)	DT 12.459 (2.834)	loss 10.039 (10.039)	gnorm 835110.750 (835110.750)	prob 2.244 (2.2443)	GS 39.312 (39.312)	mem 42.446
Train: [2][14/750]	BT 1.362 (2.874)	DT 1.162 (2.714)	loss 10.683 (10.683)	gnorm 841264.562 (841264.562)	prob 1.930 (1.9300)	GS 37.891 (37.891)	mem 42.439
Train: [2][15/750]	BT 0.334 (2.705)	DT 0.003 (2.533)	loss 10.004 (10.004)	gnorm 782839.188 (782839.188)	prob 2.236 (2.2357)	GS 30.938 (30.938)	mem 42.425
Train: [2][16/750]	BT 0.168 (2.547)	DT 0.010 (2.376)	loss 11.269 (11.269)	gnorm 953261.500 (953261.500)	prob 0.871 (0.8710)	GS 33.453 (33.453)	mem 42.392
Train: [2][17/750]	BT 0.107 (2.403)	DT 0.006 (2.236)	loss 10.157 (10.157)	gnorm 766974.188 (766974.188)	prob 2.276 (2.2763)	GS 32.000 (32.000)	mem 42.441
Train: [2][18/750]	BT 0.258 (2.284)	DT 0.002 (2.112)	loss 9.739 (9.739)	gnorm 843086.625 (843086.625)	prob 1.915 (1.9153)	GS 34.891 (34.891)	mem 42.414
Train: [2][19/750]	BT 0.143 (2.171)	DT 0.006 (2.001)	loss 10.084 (10.084)	gnorm 850990.062 (850990.062)	prob 1.556 (1.5564)	GS 32.719 (32.719)	mem 42.543
Train: [2][20/750]	BT 0.265 (2.076)	DT 0.002 (1.901)	loss 10.319 (10.319)	gnorm 903883.750 (903883.750)	prob 0.730 (0.7303)	GS 33.406 (33.406)	mem 42.471
Train: [2][21/750]	BT 0.162 (1.985)	DT 0.006 (1.811)	loss 10.462 (10.462)	gnorm 821122.312 (821122.312)	prob 1.772 (1.7722)	GS 29.359 (29.359)	mem 42.414
Train: [2][22/750]	BT 0.148 (1.901)	DT 0.003 (1.729)	loss 10.202 (10.202)	gnorm 907216.375 (907216.375)	prob 0.973 (0.9725)	GS 35.766 (35.766)	mem 42.429
Train: [2][23/750]	BT 0.176 (1.826)	DT 0.003 (1.654)	loss 10.036 (10.036)	gnorm 914166.125 (914166.125)	prob 2.158 (2.1576)	GS 33.203 (33.203)	mem 42.501
Train: [2][24/750]	BT 0.130 (1.756)	DT 0.003 (1.585)	loss 11.039 (11.039)	gnorm 825387.562 (825387.562)	prob 0.445 (0.4455)	GS 31.172 (31.172)	mem 42.582
Train: [2][25/750]	BT 8.493 (2.025)	DT 8.401 (1.858)	loss 10.784 (10.784)	gnorm 768965.562 (768965.562)	prob 1.120 (1.1204)	GS 32.578 (32.578)	mem 42.545
Train: [2][26/750]	BT 3.861 (2.096)	DT 3.755 (1.931)	loss 10.592 (10.592)	gnorm 820967.625 (820967.625)	prob 0.963 (0.9631)	GS 33.922 (33.922)	mem 42.459
Train: [2][27/750]	BT 0.266 (2.028)	DT 0.002 (1.859)	loss 10.175 (10.175)	gnorm 783322.938 (783322.938)	prob 0.743 (0.7430)	GS 27.375 (27.375)	mem 42.473
Train: [2][28/750]	BT 0.215 (1.963)	DT 0.068 (1.795)	loss 10.133 (10.133)	gnorm 809224.250 (809224.250)	prob 1.026 (1.0256)	GS 34.125 (34.125)	mem 42.520
Train: [2][29/750]	BT 0.093 (1.899)	DT 0.002 (1.733)	loss 9.812 (9.812)	gnorm 739490.000 (739490.000)	prob 1.113 (1.1127)	GS 29.844 (29.844)	mem 42.683
Train: [2][30/750]	BT 0.144 (1.840)	DT 0.003 (1.676)	loss 9.615 (9.615)	gnorm 827809.188 (827809.188)	prob 1.068 (1.0675)	GS 33.672 (33.672)	mem 42.658
Train: [2][31/750]	BT 0.086 (1.784)	DT 0.003 (1.622)	loss 9.999 (9.999)	gnorm 838711.562 (838711.562)	prob 2.017 (2.0167)	GS 36.297 (36.297)	mem 42.541
Train: [2][32/750]	BT 0.135 (1.732)	DT 0.006 (1.571)	loss 10.521 (10.521)	gnorm 778966.438 (778966.438)	prob 0.522 (0.5218)	GS 29.719 (29.719)	mem 42.585
Train: [2][33/750]	BT 0.135 (1.684)	DT 0.003 (1.524)	loss 10.335 (10.335)	gnorm 806913.188 (806913.188)	prob 0.834 (0.8342)	GS 34.500 (34.500)	mem 42.601
Train: [2][34/750]	BT 1.222 (1.670)	DT 1.124 (1.512)	loss 10.633 (10.633)	gnorm 721671.750 (721671.750)	prob 0.392 (0.3920)	GS 35.891 (35.891)	mem 42.536
Train: [2][35/750]	BT 0.083 (1.625)	DT 0.002 (1.469)	loss 10.409 (10.409)	gnorm 806205.438 (806205.438)	prob 0.114 (0.1142)	GS 31.266 (31.266)	mem 42.503
Train: [2][36/750]	BT 0.084 (1.582)	DT 0.001 (1.428)	loss 9.981 (9.981)	gnorm 783528.125 (783528.125)	prob 1.266 (1.2656)	GS 31.328 (31.328)	mem 42.502
Train: [2][37/750]	BT 4.406 (1.658)	DT 4.236 (1.504)	loss 10.907 (10.907)	gnorm 834332.062 (834332.062)	prob 1.331 (1.3314)	GS 35.406 (35.406)	mem 42.613
Train: [2][38/750]	BT 7.813 (1.820)	DT 7.683 (1.667)	loss 10.513 (10.513)	gnorm 829041.750 (829041.750)	prob 0.835 (0.8351)	GS 32.344 (32.344)	mem 42.688
Train: [2][39/750]	BT 0.190 (1.778)	DT 0.016 (1.624)	loss 10.739 (10.739)	gnorm 852374.875 (852374.875)	prob 0.894 (0.8943)	GS 31.469 (31.469)	mem 42.639
Train: [2][40/750]	BT 0.081 (1.736)	DT 0.001 (1.584)	loss 9.690 (9.690)	gnorm 737735.625 (737735.625)	prob 1.763 (1.7633)	GS 35.469 (35.469)	mem 42.640
Train: [2][41/750]	BT 0.187 (1.698)	DT 0.002 (1.545)	loss 9.715 (9.715)	gnorm 850723.375 (850723.375)	prob 2.269 (2.2687)	GS 27.016 (27.016)	mem 42.640
Train: [2][42/750]	BT 0.122 (1.661)	DT 0.002 (1.508)	loss 10.642 (10.642)	gnorm 760883.250 (760883.250)	prob 0.197 (0.1972)	GS 35.391 (35.391)	mem 42.640
Train: [2][43/750]	BT 0.132 (1.625)	DT 0.010 (1.474)	loss 10.621 (10.621)	gnorm 861590.625 (861590.625)	prob 1.223 (1.2234)	GS 32.109 (32.109)	mem 42.671
Train: [2][44/750]	BT 0.107 (1.591)	DT 0.001 (1.440)	loss 10.395 (10.395)	gnorm 811033.500 (811033.500)	prob 1.035 (1.0350)	GS 39.516 (39.516)	mem 42.643
Train: [2][45/750]	BT 0.102 (1.558)	DT 0.004 (1.408)	loss 9.993 (9.993)	gnorm 810205.938 (810205.938)	prob 1.422 (1.4224)	GS 33.469 (33.469)	mem 42.642
Train: [2][46/750]	BT 0.869 (1.543)	DT 0.755 (1.394)	loss 10.507 (10.507)	gnorm 757926.562 (757926.562)	prob 0.730 (0.7302)	GS 34.766 (34.766)	mem 42.681
Train: [2][47/750]	BT 0.125 (1.512)	DT 0.003 (1.364)	loss 10.018 (10.018)	gnorm 768665.250 (768665.250)	prob 2.237 (2.2373)	GS 31.156 (31.156)	mem 42.643
Train: [2][48/750]	BT 0.127 (1.484)	DT 0.018 (1.336)	loss 9.881 (9.881)	gnorm 754195.750 (754195.750)	prob 1.521 (1.5207)	GS 32.172 (32.172)	mem 42.644
Train: [2][49/750]	BT 0.202 (1.457)	DT 0.003 (1.309)	loss 9.967 (9.967)	gnorm 762937.188 (762937.188)	prob 1.669 (1.6695)	GS 33.500 (33.500)	mem 42.670
Train: [2][50/750]	BT 10.746 (1.643)	DT 10.629 (1.496)	loss 10.217 (10.217)	gnorm 696867.250 (696867.250)	prob 1.420 (1.4199)	GS 33.812 (33.812)	mem 42.929
Train: [2][51/750]	BT 0.290 (1.617)	DT 0.005 (1.466)	loss 9.794 (9.794)	gnorm 787581.938 (787581.938)	prob 2.353 (2.3526)	GS 32.016 (32.016)	mem 42.764
Train: [2][52/750]	BT 0.231 (1.590)	DT 0.039 (1.439)	loss 9.931 (9.931)	gnorm 779269.125 (779269.125)	prob 1.557 (1.5570)	GS 34.078 (34.078)	mem 42.699
Train: [2][53/750]	BT 0.150 (1.563)	DT 0.002 (1.412)	loss 10.658 (10.658)	gnorm 757232.312 (757232.312)	prob 0.457 (0.4571)	GS 27.281 (27.281)	mem 42.699
Train: [2][54/750]	BT 0.198 (1.538)	DT 0.009 (1.386)	loss 10.293 (10.293)	gnorm 771392.125 (771392.125)	prob 0.901 (0.9015)	GS 36.438 (36.438)	mem 42.720
Train: [2][55/750]	BT 0.091 (1.511)	DT 0.002 (1.361)	loss 10.346 (10.346)	gnorm 702534.312 (702534.312)	prob 1.092 (1.0919)	GS 30.266 (30.266)	mem 42.752
Train: [2][56/750]	BT 0.125 (1.486)	DT 0.002 (1.336)	loss 10.417 (10.417)	gnorm 780439.938 (780439.938)	prob 1.102 (1.1019)	GS 32.562 (32.562)	mem 42.701
Train: [2][57/750]	BT 0.077 (1.462)	DT 0.001 (1.313)	loss 10.970 (10.970)	gnorm 827209.000 (827209.000)	prob 0.934 (0.9336)	GS 27.422 (27.422)	mem 42.704
Train: [2][58/750]	BT 2.647 (1.482)	DT 2.554 (1.334)	loss 10.426 (10.426)	gnorm 863092.062 (863092.062)	prob 1.036 (1.0360)	GS 34.609 (34.609)	mem 42.750
Train: [2][59/750]	BT 0.092 (1.459)	DT 0.002 (1.312)	loss 10.523 (10.523)	gnorm 876061.312 (876061.312)	prob 1.590 (1.5898)	GS 29.703 (29.703)	mem 42.769
Train: [2][60/750]	BT 0.157 (1.437)	DT 0.002 (1.290)	loss 10.408 (10.408)	gnorm 810670.312 (810670.312)	prob 1.398 (1.3982)	GS 31.797 (31.797)	mem 42.799
Train: [2][61/750]	BT 0.327 (1.419)	DT 0.002 (1.269)	loss 10.538 (10.538)	gnorm 811810.438 (811810.438)	prob 1.391 (1.3912)	GS 33.531 (33.531)	mem 42.855
Train: [2][62/750]	BT 11.704 (1.585)	DT 11.543 (1.434)	loss 10.569 (10.569)	gnorm 760785.188 (760785.188)	prob 0.970 (0.9702)	GS 33.844 (33.844)	mem 43.052
Train: [2][63/750]	BT 0.213 (1.563)	DT 0.014 (1.412)	loss 10.270 (10.270)	gnorm 804393.375 (804393.375)	prob 2.295 (2.2948)	GS 30.375 (30.375)	mem 43.051
Train: [2][64/750]	BT 0.089 (1.540)	DT 0.002 (1.390)	loss 9.997 (9.997)	gnorm 823609.188 (823609.188)	prob 2.071 (2.0713)	GS 32.125 (32.125)	mem 43.051
Train: [2][65/750]	BT 0.090 (1.518)	DT 0.003 (1.369)	loss 10.423 (10.423)	gnorm 820563.125 (820563.125)	prob 2.085 (2.0852)	GS 34.641 (34.641)	mem 43.052
Train: [2][66/750]	BT 0.130 (1.497)	DT 0.002 (1.348)	loss 10.196 (10.196)	gnorm 738853.625 (738853.625)	prob 1.739 (1.7389)	GS 31.766 (31.766)	mem 43.052
Train: [2][67/750]	BT 0.136 (1.476)	DT 0.001 (1.328)	loss 9.756 (9.756)	gnorm 771738.750 (771738.750)	prob 1.983 (1.9829)	GS 32.312 (32.312)	mem 43.078
Train: [2][68/750]	BT 0.134 (1.456)	DT 0.006 (1.308)	loss 10.037 (10.037)	gnorm 731363.688 (731363.688)	prob 1.018 (1.0179)	GS 29.469 (29.469)	mem 43.220
Train: [2][69/750]	BT 0.309 (1.440)	DT 0.011 (1.290)	loss 9.739 (9.739)	gnorm 786324.500 (786324.500)	prob 2.127 (2.1269)	GS 37.141 (37.141)	mem 43.154
Train: [2][70/750]	BT 1.562 (1.442)	DT 1.477 (1.292)	loss 10.090 (10.090)	gnorm 754624.625 (754624.625)	prob 1.271 (1.2715)	GS 35.672 (35.672)	mem 43.117
Train: [2][71/750]	BT 0.179 (1.424)	DT 0.002 (1.274)	loss 10.061 (10.061)	gnorm 821611.625 (821611.625)	prob 0.744 (0.7440)	GS 35.188 (35.188)	mem 43.070
Train: [2][72/750]	BT 0.077 (1.405)	DT 0.003 (1.256)	loss 10.395 (10.395)	gnorm 733830.062 (733830.062)	prob 0.812 (0.8115)	GS 33.359 (33.359)	mem 43.165
Train: [2][73/750]	BT 0.135 (1.388)	DT 0.003 (1.239)	loss 10.790 (10.790)	gnorm 787489.750 (787489.750)	prob 0.056 (0.0561)	GS 28.719 (28.719)	mem 43.155
Train: [2][74/750]	BT 9.378 (1.496)	DT 9.288 (1.348)	loss 10.161 (10.161)	gnorm 854999.125 (854999.125)	prob 1.136 (1.1358)	GS 34.625 (34.625)	mem 43.405
Train: [2][75/750]	BT 0.090 (1.477)	DT 0.002 (1.330)	loss 9.942 (9.942)	gnorm 726120.000 (726120.000)	prob 1.959 (1.9587)	GS 31.938 (31.938)	mem 43.308
Train: [2][76/750]	BT 0.094 (1.459)	DT 0.002 (1.313)	loss 10.153 (10.153)	gnorm 744585.438 (744585.438)	prob 1.482 (1.4820)	GS 32.719 (32.719)	mem 43.271
Train: [2][77/750]	BT 0.081 (1.441)	DT 0.002 (1.296)	loss 10.202 (10.202)	gnorm 707026.000 (707026.000)	prob 0.571 (0.5705)	GS 29.891 (29.891)	mem 43.272
Train: [2][78/750]	BT 0.138 (1.424)	DT 0.001 (1.279)	loss 10.336 (10.336)	gnorm 735286.375 (735286.375)	prob 1.756 (1.7557)	GS 32.906 (32.906)	mem 43.273
Train: [2][79/750]	BT 0.139 (1.408)	DT 0.003 (1.263)	loss 10.030 (10.030)	gnorm 777245.312 (777245.312)	prob 1.400 (1.3998)	GS 30.438 (30.438)	mem 43.274
Train: [2][80/750]	BT 0.106 (1.392)	DT 0.002 (1.247)	loss 9.994 (9.994)	gnorm 793648.875 (793648.875)	prob 1.720 (1.7201)	GS 30.344 (30.344)	mem 43.273
Train: [2][81/750]	BT 0.184 (1.377)	DT 0.013 (1.232)	loss 10.335 (10.335)	gnorm 774163.125 (774163.125)	prob 1.764 (1.7644)	GS 31.453 (31.453)	mem 43.281
Train: [2][82/750]	BT 3.645 (1.404)	DT 3.547 (1.260)	loss 9.809 (9.809)	gnorm 730705.125 (730705.125)	prob 2.134 (2.1342)	GS 35.219 (35.219)	mem 43.373
Train: [2][83/750]	BT 0.148 (1.389)	DT 0.002 (1.245)	loss 10.192 (10.192)	gnorm 827341.375 (827341.375)	prob 1.569 (1.5690)	GS 35.219 (35.219)	mem 43.390
Train: [2][84/750]	BT 0.084 (1.374)	DT 0.002 (1.230)	loss 10.773 (10.773)	gnorm 841454.375 (841454.375)	prob 0.581 (0.5814)	GS 41.234 (41.234)	mem 43.340
Train: [2][85/750]	BT 0.144 (1.359)	DT 0.001 (1.216)	loss 10.715 (10.715)	gnorm 785086.750 (785086.750)	prob 1.210 (1.2098)	GS 34.812 (34.812)	mem 43.356
Train: [2][86/750]	BT 8.958 (1.448)	DT 8.816 (1.304)	loss 10.590 (10.590)	gnorm 820217.375 (820217.375)	prob 0.502 (0.5025)	GS 29.531 (29.531)	mem 43.321
Train: [2][87/750]	BT 0.092 (1.432)	DT 0.003 (1.289)	loss 10.960 (10.960)	gnorm 908188.125 (908188.125)	prob 0.643 (0.6434)	GS 34.500 (34.500)	mem 43.321
Train: [2][88/750]	BT 1.340 (1.431)	DT 1.208 (1.288)	loss 10.237 (10.237)	gnorm 853590.250 (853590.250)	prob 1.076 (1.0755)	GS 35.391 (35.391)	mem 43.323
Train: [2][89/750]	BT 0.102 (1.416)	DT 0.002 (1.274)	loss 10.098 (10.098)	gnorm 863008.562 (863008.562)	prob 1.838 (1.8375)	GS 30.750 (30.750)	mem 43.324
Train: [2][90/750]	BT 0.187 (1.402)	DT 0.005 (1.260)	loss 10.274 (10.274)	gnorm 913355.750 (913355.750)	prob 0.772 (0.7722)	GS 32.484 (32.484)	mem 43.324
Train: [2][91/750]	BT 0.089 (1.388)	DT 0.002 (1.246)	loss 10.139 (10.139)	gnorm 883917.750 (883917.750)	prob 1.700 (1.7003)	GS 31.406 (31.406)	mem 43.430
Train: [2][92/750]	BT 0.103 (1.374)	DT 0.002 (1.232)	loss 9.912 (9.912)	gnorm 791349.312 (791349.312)	prob 1.806 (1.8055)	GS 37.250 (37.250)	mem 43.400
Train: [2][93/750]	BT 0.084 (1.360)	DT 0.006 (1.219)	loss 10.194 (10.194)	gnorm 750977.562 (750977.562)	prob 1.506 (1.5057)	GS 32.953 (32.953)	mem 43.326
Train: [2][94/750]	BT 2.008 (1.367)	DT 1.858 (1.226)	loss 10.693 (10.693)	gnorm 799867.188 (799867.188)	prob 1.234 (1.2339)	GS 31.422 (31.422)	mem 43.325
Train: [2][95/750]	BT 0.141 (1.354)	DT 0.001 (1.213)	loss 10.351 (10.351)	gnorm 699002.188 (699002.188)	prob 0.764 (0.7637)	GS 27.922 (27.922)	mem 43.326
Train: [2][96/750]	BT 0.213 (1.342)	DT 0.011 (1.200)	loss 10.287 (10.287)	gnorm 752350.438 (752350.438)	prob 1.267 (1.2673)	GS 29.844 (29.844)	mem 43.325
Train: [2][97/750]	BT 0.094 (1.329)	DT 0.002 (1.188)	loss 9.781 (9.781)	gnorm 761510.500 (761510.500)	prob 1.560 (1.5600)	GS 29.828 (29.828)	mem 43.326
Train: [2][98/750]	BT 7.587 (1.393)	DT 7.520 (1.253)	loss 10.226 (10.226)	gnorm 719702.250 (719702.250)	prob 1.619 (1.6192)	GS 42.531 (42.531)	mem 43.383
Train: [2][99/750]	BT 0.109 (1.380)	DT 0.001 (1.240)	loss 10.161 (10.161)	gnorm 833370.812 (833370.812)	prob 1.358 (1.3577)	GS 28.609 (28.609)	mem 43.466
Train: [2][100/750]	BT 1.679 (1.383)	DT 1.583 (1.243)	loss 10.850 (10.850)	gnorm 803225.500 (803225.500)	prob 0.940 (0.9399)	GS 32.172 (32.172)	mem 43.331
Train: [2][101/750]	BT 0.129 (1.371)	DT 0.002 (1.231)	loss 10.204 (10.204)	gnorm 741612.875 (741612.875)	prob 1.518 (1.5184)	GS 32.781 (32.781)	mem 43.360
Train: [2][102/750]	BT 0.099 (1.358)	DT 0.002 (1.219)	loss 10.073 (10.073)	gnorm 770634.438 (770634.438)	prob 1.900 (1.8996)	GS 34.062 (34.062)	mem 43.440
Train: [2][103/750]	BT 0.146 (1.347)	DT 0.002 (1.207)	loss 10.285 (10.285)	gnorm 782832.875 (782832.875)	prob 1.327 (1.3266)	GS 33.234 (33.234)	mem 43.579
Train: [2][104/750]	BT 0.096 (1.335)	DT 0.005 (1.196)	loss 9.722 (9.722)	gnorm 777074.812 (777074.812)	prob 1.890 (1.8903)	GS 29.266 (29.266)	mem 43.596
Train: [2][105/750]	BT 0.291 (1.325)	DT 0.006 (1.184)	loss 10.669 (10.669)	gnorm 776891.688 (776891.688)	prob 1.447 (1.4470)	GS 31.812 (31.812)	mem 43.339
Train: [2][106/750]	BT 1.431 (1.326)	DT 1.341 (1.186)	loss 10.307 (10.307)	gnorm 881908.688 (881908.688)	prob 1.434 (1.4345)	GS 34.422 (34.422)	mem 43.342
Train: [2][107/750]	BT 0.087 (1.314)	DT 0.002 (1.175)	loss 10.478 (10.478)	gnorm 724879.375 (724879.375)	prob 2.233 (2.2325)	GS 32.766 (32.766)	mem 43.342
Train: [2][108/750]	BT 0.104 (1.303)	DT 0.001 (1.164)	loss 9.924 (9.924)	gnorm 733087.062 (733087.062)	prob 2.183 (2.1832)	GS 33.234 (33.234)	mem 43.355
Train: [2][109/750]	BT 0.149 (1.292)	DT 0.005 (1.153)	loss 10.500 (10.500)	gnorm 772399.000 (772399.000)	prob 1.652 (1.6521)	GS 32.859 (32.859)	mem 43.384
Train: [2][110/750]	BT 6.661 (1.341)	DT 6.568 (1.203)	loss 9.779 (9.779)	gnorm 849960.375 (849960.375)	prob 2.251 (2.2508)	GS 33.375 (33.375)	mem 43.324
Train: [2][111/750]	BT 0.204 (1.331)	DT 0.005 (1.192)	loss 10.544 (10.544)	gnorm 920143.500 (920143.500)	prob 2.628 (2.6281)	GS 34.609 (34.609)	mem 43.461
Train: [2][112/750]	BT 3.889 (1.354)	DT 3.755 (1.215)	loss 10.259 (10.259)	gnorm 755241.062 (755241.062)	prob 1.825 (1.8250)	GS 37.703 (37.703)	mem 43.369
Train: [2][113/750]	BT 0.184 (1.343)	DT 0.002 (1.204)	loss 10.411 (10.411)	gnorm 976308.938 (976308.938)	prob 2.873 (2.8726)	GS 32.703 (32.703)	mem 43.411
Train: [2][114/750]	BT 0.180 (1.333)	DT 0.013 (1.193)	loss 10.135 (10.135)	gnorm 793205.188 (793205.188)	prob 2.027 (2.0272)	GS 34.891 (34.891)	mem 43.369
Train: [2][115/750]	BT 0.121 (1.323)	DT 0.011 (1.183)	loss 9.899 (9.899)	gnorm 689428.062 (689428.062)	prob 3.197 (3.1973)	GS 34.828 (34.828)	mem 43.337
Train: [2][116/750]	BT 0.178 (1.313)	DT 0.002 (1.173)	loss 10.012 (10.012)	gnorm 788764.750 (788764.750)	prob 2.557 (2.5565)	GS 33.984 (33.984)	mem 43.403
Train: [2][117/750]	BT 0.267 (1.304)	DT 0.002 (1.163)	loss 10.479 (10.479)	gnorm 793637.500 (793637.500)	prob 2.623 (2.6225)	GS 26.500 (26.500)	mem 43.466
Train: [2][118/750]	BT 0.157 (1.294)	DT 0.041 (1.153)	loss 9.626 (9.626)	gnorm 764604.250 (764604.250)	prob 3.202 (3.2025)	GS 36.266 (36.266)	mem 43.363
Train: [2][119/750]	BT 0.118 (1.284)	DT 0.002 (1.144)	loss 10.546 (10.546)	gnorm 744325.438 (744325.438)	prob 2.393 (2.3925)	GS 31.328 (31.328)	mem 43.388
Train: [2][120/750]	BT 0.124 (1.274)	DT 0.001 (1.134)	loss 11.092 (11.092)	gnorm 797273.312 (797273.312)	prob 1.258 (1.2583)	GS 36.672 (36.672)	mem 43.414
Train: [2][121/750]	BT 0.269 (1.266)	DT 0.018 (1.125)	loss 9.826 (9.826)	gnorm 767344.438 (767344.438)	prob 2.606 (2.6059)	GS 31.141 (31.141)	mem 43.432
Train: [2][122/750]	BT 6.550 (1.309)	DT 6.378 (1.168)	loss 9.627 (9.627)	gnorm 744463.250 (744463.250)	prob 2.899 (2.8990)	GS 34.750 (34.750)	mem 43.514
Train: [2][123/750]	BT 0.214 (1.301)	DT 0.002 (1.159)	loss 9.853 (9.853)	gnorm 687919.188 (687919.188)	prob 2.477 (2.4766)	GS 34.125 (34.125)	mem 43.379
Train: [2][124/750]	BT 7.113 (1.347)	DT 7.031 (1.206)	loss 10.542 (10.542)	gnorm 690139.062 (690139.062)	prob 1.628 (1.6279)	GS 34.578 (34.578)	mem 43.343
Train: [2][125/750]	BT 0.112 (1.338)	DT 0.001 (1.196)	loss 10.080 (10.080)	gnorm 880656.188 (880656.188)	prob 2.620 (2.6197)	GS 41.219 (41.219)	mem 43.343
Train: [2][126/750]	BT 0.102 (1.328)	DT 0.002 (1.187)	loss 9.743 (9.743)	gnorm 718341.000 (718341.000)	prob 2.529 (2.5292)	GS 32.547 (32.547)	mem 43.342
Train: [2][127/750]	BT 0.153 (1.318)	DT 0.002 (1.178)	loss 10.158 (10.158)	gnorm 786893.688 (786893.688)	prob 2.233 (2.2329)	GS 31.000 (31.000)	mem 43.342
Train: [2][128/750]	BT 0.168 (1.310)	DT 0.003 (1.168)	loss 9.842 (9.842)	gnorm 788495.438 (788495.438)	prob 2.416 (2.4161)	GS 31.922 (31.922)	mem 43.345
Train: [2][129/750]	BT 0.122 (1.300)	DT 0.009 (1.159)	loss 10.768 (10.768)	gnorm 738427.312 (738427.312)	prob 2.174 (2.1738)	GS 33.938 (33.938)	mem 43.346
Train: [2][130/750]	BT 0.329 (1.293)	DT 0.211 (1.152)	loss 10.604 (10.604)	gnorm 749255.812 (749255.812)	prob 2.157 (2.1568)	GS 31.375 (31.375)	mem 43.383
Train: [2][131/750]	BT 0.135 (1.284)	DT 0.007 (1.143)	loss 10.068 (10.068)	gnorm 826865.062 (826865.062)	prob 2.952 (2.9519)	GS 30.719 (30.719)	mem 43.354
Train: [2][132/750]	BT 0.101 (1.275)	DT 0.001 (1.135)	loss 10.158 (10.158)	gnorm 700447.375 (700447.375)	prob 1.772 (1.7720)	GS 35.484 (35.484)	mem 43.404
Train: [2][133/750]	BT 0.104 (1.266)	DT 0.002 (1.126)	loss 10.072 (10.072)	gnorm 758456.562 (758456.562)	prob 2.013 (2.0129)	GS 27.234 (27.234)	mem 43.389
Train: [2][134/750]	BT 6.993 (1.309)	DT 6.868 (1.169)	loss 10.311 (10.311)	gnorm 762518.250 (762518.250)	prob 1.761 (1.7606)	GS 34.891 (34.891)	mem 43.410
Train: [2][135/750]	BT 0.086 (1.300)	DT 0.002 (1.160)	loss 10.084 (10.084)	gnorm 729749.812 (729749.812)	prob 2.648 (2.6477)	GS 27.828 (27.828)	mem 43.444
Train: [2][136/750]	BT 7.191 (1.343)	DT 7.113 (1.204)	loss 10.880 (10.880)	gnorm 728923.000 (728923.000)	prob 1.197 (1.1968)	GS 30.516 (30.516)	mem 43.395
Train: [2][137/750]	BT 0.107 (1.334)	DT 0.001 (1.195)	loss 10.162 (10.162)	gnorm 697136.438 (697136.438)	prob 1.621 (1.6211)	GS 32.031 (32.031)	mem 43.389
Train: [2][138/750]	BT 0.102 (1.325)	DT 0.013 (1.187)	loss 10.461 (10.461)	gnorm 748225.375 (748225.375)	prob 1.992 (1.9921)	GS 32.625 (32.625)	mem 43.390
Train: [2][139/750]	BT 0.079 (1.316)	DT 0.002 (1.178)	loss 10.161 (10.161)	gnorm 713905.438 (713905.438)	prob 2.021 (2.0206)	GS 30.594 (30.594)	mem 43.396
Train: [2][140/750]	BT 0.148 (1.308)	DT 0.001 (1.170)	loss 10.644 (10.644)	gnorm 765672.438 (765672.438)	prob 1.190 (1.1898)	GS 31.406 (31.406)	mem 43.497
Train: [2][141/750]	BT 0.143 (1.300)	DT 0.027 (1.162)	loss 10.026 (10.026)	gnorm 748056.375 (748056.375)	prob 2.259 (2.2594)	GS 29.719 (29.719)	mem 43.423
Train: [2][142/750]	BT 0.140 (1.292)	DT 0.001 (1.154)	loss 9.856 (9.856)	gnorm 737534.250 (737534.250)	prob 1.562 (1.5615)	GS 34.438 (34.438)	mem 43.502
Train: [2][143/750]	BT 0.250 (1.284)	DT 0.033 (1.146)	loss 11.255 (11.255)	gnorm 734515.500 (734515.500)	prob 0.162 (0.1623)	GS 27.047 (27.047)	mem 43.436
Train: [2][144/750]	BT 0.124 (1.276)	DT 0.020 (1.138)	loss 9.570 (9.570)	gnorm 751087.250 (751087.250)	prob 2.274 (2.2744)	GS 34.828 (34.828)	mem 43.395
Train: [2][145/750]	BT 0.099 (1.268)	DT 0.009 (1.130)	loss 10.267 (10.267)	gnorm 723967.125 (723967.125)	prob 1.240 (1.2395)	GS 33.172 (33.172)	mem 43.448
Train: [2][146/750]	BT 3.897 (1.286)	DT 3.689 (1.148)	loss 10.054 (10.054)	gnorm 679027.812 (679027.812)	prob 2.197 (2.1972)	GS 33.891 (33.891)	mem 43.468
Train: [2][147/750]	BT 0.085 (1.278)	DT 0.005 (1.140)	loss 9.989 (9.989)	gnorm 703642.750 (703642.750)	prob 2.726 (2.7259)	GS 30.250 (30.250)	mem 43.469
Train: [2][148/750]	BT 7.480 (1.320)	DT 7.397 (1.182)	loss 10.114 (10.114)	gnorm 771589.625 (771589.625)	prob 1.934 (1.9339)	GS 33.984 (33.984)	mem 43.433
Train: [2][149/750]	BT 0.074 (1.311)	DT 0.001 (1.174)	loss 10.504 (10.504)	gnorm 801249.875 (801249.875)	prob 2.443 (2.4433)	GS 32.812 (32.812)	mem 43.434
Train: [2][150/750]	BT 0.083 (1.303)	DT 0.002 (1.166)	loss 10.561 (10.561)	gnorm 782984.688 (782984.688)	prob 1.932 (1.9320)	GS 32.234 (32.234)	mem 43.434
Train: [2][151/750]	BT 0.189 (1.296)	DT 0.025 (1.159)	loss 10.478 (10.478)	gnorm 872266.750 (872266.750)	prob 2.503 (2.5033)	GS 29.250 (29.250)	mem 43.434
Train: [2][152/750]	BT 0.105 (1.288)	DT 0.001 (1.151)	loss 10.962 (10.962)	gnorm 837413.000 (837413.000)	prob 1.240 (1.2395)	GS 33.266 (33.266)	mem 43.435
Train: [2][153/750]	BT 0.219 (1.281)	DT 0.005 (1.144)	loss 10.001 (10.001)	gnorm 782931.625 (782931.625)	prob 2.718 (2.7179)	GS 30.703 (30.703)	mem 43.595
Train: [2][154/750]	BT 0.215 (1.274)	DT 0.029 (1.137)	loss 10.495 (10.495)	gnorm 712451.688 (712451.688)	prob 1.249 (1.2485)	GS 33.656 (33.656)	mem 43.435
Train: [2][155/750]	BT 0.093 (1.267)	DT 0.006 (1.129)	loss 10.401 (10.401)	gnorm 790485.250 (790485.250)	prob 1.755 (1.7552)	GS 33.047 (33.047)	mem 43.435
Train: [2][156/750]	BT 0.115 (1.259)	DT 0.002 (1.122)	loss 10.404 (10.404)	gnorm 763261.375 (763261.375)	prob 1.713 (1.7132)	GS 35.734 (35.734)	mem 43.466
Train: [2][157/750]	BT 0.177 (1.252)	DT 0.012 (1.115)	loss 10.076 (10.076)	gnorm 767362.625 (767362.625)	prob 2.484 (2.4842)	GS 28.172 (28.172)	mem 43.469
Train: [2][158/750]	BT 3.958 (1.269)	DT 3.884 (1.132)	loss 9.789 (9.789)	gnorm 801009.000 (801009.000)	prob 2.380 (2.3802)	GS 37.906 (37.906)	mem 43.407
Train: [2][159/750]	BT 0.171 (1.262)	DT 0.001 (1.125)	loss 10.046 (10.046)	gnorm 748025.438 (748025.438)	prob 1.638 (1.6380)	GS 31.672 (31.672)	mem 43.409
Train: [2][160/750]	BT 10.476 (1.320)	DT 10.386 (1.183)	loss 10.214 (10.214)	gnorm 783044.438 (783044.438)	prob 1.308 (1.3083)	GS 35.672 (35.672)	mem 43.382
Train: [2][161/750]	BT 0.093 (1.312)	DT 0.003 (1.176)	loss 10.238 (10.238)	gnorm 741900.938 (741900.938)	prob 1.065 (1.0650)	GS 29.734 (29.734)	mem 43.382
Train: [2][162/750]	BT 0.116 (1.305)	DT 0.001 (1.169)	loss 9.946 (9.946)	gnorm 757012.375 (757012.375)	prob 1.326 (1.3259)	GS 30.609 (30.609)	mem 43.382
Train: [2][163/750]	BT 0.097 (1.298)	DT 0.004 (1.161)	loss 10.107 (10.107)	gnorm 798050.250 (798050.250)	prob 2.349 (2.3487)	GS 32.453 (32.453)	mem 43.381
Train: [2][164/750]	BT 0.127 (1.290)	DT 0.002 (1.154)	loss 9.790 (9.790)	gnorm 772892.688 (772892.688)	prob 1.777 (1.7768)	GS 31.969 (31.969)	mem 43.381
Train: [2][165/750]	BT 0.089 (1.283)	DT 0.005 (1.147)	loss 10.315 (10.315)	gnorm 685868.688 (685868.688)	prob 1.613 (1.6128)	GS 31.391 (31.391)	mem 43.381
Train: [2][166/750]	BT 0.079 (1.276)	DT 0.002 (1.141)	loss 9.946 (9.946)	gnorm 677488.125 (677488.125)	prob 1.846 (1.8461)	GS 33.109 (33.109)	mem 43.460
Train: [2][167/750]	BT 0.181 (1.269)	DT 0.026 (1.134)	loss 9.963 (9.963)	gnorm 672103.438 (672103.438)	prob 1.862 (1.8621)	GS 28.516 (28.516)	mem 43.382
Train: [2][168/750]	BT 0.097 (1.262)	DT 0.001 (1.127)	loss 10.234 (10.234)	gnorm 701587.812 (701587.812)	prob 1.651 (1.6510)	GS 33.141 (33.141)	mem 43.383
Train: [2][169/750]	BT 0.246 (1.256)	DT 0.007 (1.121)	loss 9.937 (9.937)	gnorm 736103.750 (736103.750)	prob 2.220 (2.2203)	GS 32.797 (32.797)	mem 43.383
Train: [2][170/750]	BT 2.436 (1.263)	DT 2.167 (1.127)	loss 10.648 (10.648)	gnorm 767439.062 (767439.062)	prob 1.073 (1.0727)	GS 34.078 (34.078)	mem 43.608
Train: [2][171/750]	BT 0.146 (1.257)	DT 0.010 (1.120)	loss 10.050 (10.050)	gnorm 774568.000 (774568.000)	prob 1.791 (1.7910)	GS 37.656 (37.656)	mem 43.447
Train: [2][172/750]	BT 7.123 (1.291)	DT 7.013 (1.154)	loss 9.712 (9.712)	gnorm 749175.000 (749175.000)	prob 2.099 (2.0992)	GS 36.641 (36.641)	mem 43.496
Train: [2][173/750]	BT 0.145 (1.284)	DT 0.002 (1.148)	loss 10.303 (10.303)	gnorm 738694.688 (738694.688)	prob 1.794 (1.7936)	GS 31.719 (31.719)	mem 43.495
Train: [2][174/750]	BT 0.247 (1.278)	DT 0.002 (1.141)	loss 10.393 (10.393)	gnorm 736809.500 (736809.500)	prob 1.192 (1.1920)	GS 35.344 (35.344)	mem 43.496
Train: [2][175/750]	BT 0.089 (1.272)	DT 0.005 (1.135)	loss 10.027 (10.027)	gnorm 782162.250 (782162.250)	prob 1.301 (1.3014)	GS 32.328 (32.328)	mem 43.496
Train: [2][176/750]	BT 0.164 (1.265)	DT 0.002 (1.128)	loss 10.150 (10.150)	gnorm 730752.625 (730752.625)	prob 0.739 (0.7394)	GS 31.344 (31.344)	mem 43.526
Train: [2][177/750]	BT 0.143 (1.259)	DT 0.002 (1.122)	loss 10.128 (10.128)	gnorm 765099.562 (765099.562)	prob 1.495 (1.4946)	GS 31.375 (31.375)	mem 43.508
Train: [2][178/750]	BT 0.092 (1.252)	DT 0.005 (1.116)	loss 10.398 (10.398)	gnorm 722812.750 (722812.750)	prob 0.946 (0.9460)	GS 36.062 (36.062)	mem 43.535
Train: [2][179/750]	BT 0.206 (1.246)	DT 0.002 (1.109)	loss 10.190 (10.190)	gnorm 669199.062 (669199.062)	prob 1.151 (1.1513)	GS 29.391 (29.391)	mem 43.496
Train: [2][180/750]	BT 0.143 (1.240)	DT 0.009 (1.103)	loss 10.353 (10.353)	gnorm 814878.125 (814878.125)	prob 1.128 (1.1279)	GS 31.172 (31.172)	mem 43.497
Train: [2][181/750]	BT 0.100 (1.234)	DT 0.002 (1.097)	loss 9.947 (9.947)	gnorm 721180.812 (721180.812)	prob 1.230 (1.2300)	GS 37.125 (37.125)	mem 43.497
Train: [2][182/750]	BT 4.256 (1.251)	DT 4.089 (1.114)	loss 10.659 (10.659)	gnorm 701888.312 (701888.312)	prob 0.879 (0.8792)	GS 32.266 (32.266)	mem 43.571
Train: [2][183/750]	BT 0.176 (1.245)	DT 0.007 (1.108)	loss 10.500 (10.500)	gnorm 696899.750 (696899.750)	prob 2.278 (2.2776)	GS 28.641 (28.641)	mem 43.599
Train: [2][184/750]	BT 10.219 (1.294)	DT 10.135 (1.157)	loss 10.474 (10.474)	gnorm 759277.438 (759277.438)	prob 1.980 (1.9796)	GS 34.688 (34.688)	mem 43.594
Train: [2][185/750]	BT 0.084 (1.287)	DT 0.003 (1.150)	loss 10.360 (10.360)	gnorm 753968.312 (753968.312)	prob 1.611 (1.6107)	GS 34.297 (34.297)	mem 43.674
Train: [2][186/750]	BT 0.149 (1.281)	DT 0.010 (1.144)	loss 10.087 (10.087)	gnorm 750355.438 (750355.438)	prob 2.666 (2.6663)	GS 31.234 (31.234)	mem 43.818
Train: [2][187/750]	BT 0.119 (1.275)	DT 0.002 (1.138)	loss 10.085 (10.085)	gnorm 742901.562 (742901.562)	prob 2.476 (2.4764)	GS 34.141 (34.141)	mem 43.807
Train: [2][188/750]	BT 0.113 (1.269)	DT 0.014 (1.132)	loss 10.505 (10.505)	gnorm 783474.625 (783474.625)	prob 2.435 (2.4347)	GS 35.328 (35.328)	mem 43.761
Train: [2][189/750]	BT 0.224 (1.263)	DT 0.001 (1.126)	loss 10.622 (10.622)	gnorm 779137.188 (779137.188)	prob 2.907 (2.9073)	GS 34.438 (34.438)	mem 43.635
Train: [2][190/750]	BT 0.159 (1.257)	DT 0.002 (1.120)	loss 10.150 (10.150)	gnorm 743047.375 (743047.375)	prob 2.277 (2.2772)	GS 34.203 (34.203)	mem 43.630
Train: [2][191/750]	BT 0.216 (1.252)	DT 0.003 (1.114)	loss 10.121 (10.121)	gnorm 883293.188 (883293.188)	prob 3.288 (3.2880)	GS 35.344 (35.344)	mem 43.565
Train: [2][192/750]	BT 0.138 (1.246)	DT 0.014 (1.109)	loss 10.098 (10.098)	gnorm 735660.812 (735660.812)	prob 3.049 (3.0488)	GS 37.453 (37.453)	mem 43.565
Train: [2][193/750]	BT 0.107 (1.240)	DT 0.003 (1.103)	loss 10.327 (10.327)	gnorm 760596.562 (760596.562)	prob 3.139 (3.1386)	GS 33.156 (33.156)	mem 43.565
Train: [2][194/750]	BT 0.156 (1.234)	DT 0.050 (1.098)	loss 10.808 (10.808)	gnorm 753877.312 (753877.312)	prob 2.397 (2.3965)	GS 32.062 (32.062)	mem 43.570
Train: [2][195/750]	BT 0.071 (1.228)	DT 0.001 (1.092)	loss 10.020 (10.020)	gnorm 709168.688 (709168.688)	prob 2.882 (2.8823)	GS 30.766 (30.766)	mem 43.609
Train: [2][196/750]	BT 13.929 (1.293)	DT 13.764 (1.157)	loss 10.294 (10.294)	gnorm 685482.188 (685482.188)	prob 2.764 (2.7642)	GS 33.547 (33.547)	mem 43.543
Train: [2][197/750]	BT 0.104 (1.287)	DT 0.003 (1.151)	loss 9.825 (9.825)	gnorm 712812.812 (712812.812)	prob 3.359 (3.3594)	GS 41.078 (41.078)	mem 43.511
Train: [2][198/750]	BT 0.109 (1.281)	DT 0.010 (1.145)	loss 10.501 (10.501)	gnorm 724688.250 (724688.250)	prob 2.695 (2.6948)	GS 33.812 (33.812)	mem 43.517
Train: [2][199/750]	BT 0.114 (1.275)	DT 0.026 (1.139)	loss 10.033 (10.033)	gnorm 662338.250 (662338.250)	prob 1.566 (1.5662)	GS 28.547 (28.547)	mem 43.522
Train: [2][200/750]	BT 0.114 (1.270)	DT 0.002 (1.134)	loss 10.516 (10.516)	gnorm 699909.375 (699909.375)	prob 2.059 (2.0590)	GS 32.328 (32.328)	mem 43.678
Train: [2][201/750]	BT 0.152 (1.264)	DT 0.003 (1.128)	loss 10.622 (10.622)	gnorm 822205.062 (822205.062)	prob 1.096 (1.0959)	GS 31.125 (31.125)	mem 43.640
Train: [2][202/750]	BT 0.082 (1.258)	DT 0.002 (1.122)	loss 10.480 (10.480)	gnorm 700204.875 (700204.875)	prob 0.558 (0.5584)	GS 35.703 (35.703)	mem 43.525
Train: [2][203/750]	BT 0.219 (1.253)	DT 0.018 (1.117)	loss 10.161 (10.161)	gnorm 721910.875 (721910.875)	prob 1.934 (1.9339)	GS 35.531 (35.531)	mem 43.544
Train: [2][204/750]	BT 0.078 (1.247)	DT 0.007 (1.112)	loss 10.129 (10.129)	gnorm 787220.125 (787220.125)	prob 1.585 (1.5853)	GS 34.234 (34.234)	mem 43.545
Train: [2][205/750]	BT 0.100 (1.242)	DT 0.003 (1.106)	loss 9.881 (9.881)	gnorm 732402.000 (732402.000)	prob 2.009 (2.0092)	GS 34.516 (34.516)	mem 43.469
Train: [2][206/750]	BT 1.939 (1.245)	DT 1.818 (1.110)	loss 10.063 (10.063)	gnorm 722710.500 (722710.500)	prob 1.822 (1.8223)	GS 35.344 (35.344)	mem 43.543
Train: [2][207/750]	BT 0.206 (1.240)	DT 0.021 (1.104)	loss 9.998 (9.998)	gnorm 734663.688 (734663.688)	prob 1.112 (1.1120)	GS 31.516 (31.516)	mem 43.580
Train: [2][208/750]	BT 12.463 (1.294)	DT 12.393 (1.159)	loss 9.887 (9.887)	gnorm 632847.938 (632847.938)	prob 1.820 (1.8199)	GS 35.969 (35.969)	mem 43.473
Train: [2][209/750]	BT 0.086 (1.288)	DT 0.001 (1.153)	loss 10.446 (10.446)	gnorm 744816.312 (744816.312)	prob 1.128 (1.1277)	GS 32.641 (32.641)	mem 43.569
Train: [2][210/750]	BT 0.138 (1.283)	DT 0.002 (1.148)	loss 10.149 (10.149)	gnorm 689094.250 (689094.250)	prob 1.387 (1.3872)	GS 33.578 (33.578)	mem 43.473
Train: [2][211/750]	BT 0.096 (1.277)	DT 0.002 (1.142)	loss 11.082 (11.082)	gnorm 751642.875 (751642.875)	prob 1.271 (1.2712)	GS 32.031 (32.031)	mem 43.474
Train: [2][212/750]	BT 0.105 (1.272)	DT 0.010 (1.137)	loss 9.821 (9.821)	gnorm 689903.625 (689903.625)	prob 1.758 (1.7580)	GS 34.375 (34.375)	mem 43.474
Train: [2][213/750]	BT 0.089 (1.266)	DT 0.003 (1.131)	loss 10.179 (10.179)	gnorm 715260.812 (715260.812)	prob 1.653 (1.6530)	GS 30.844 (30.844)	mem 43.504
Train: [2][214/750]	BT 0.129 (1.261)	DT 0.003 (1.126)	loss 10.363 (10.363)	gnorm 694438.750 (694438.750)	prob 0.930 (0.9299)	GS 32.312 (32.312)	mem 43.507
Train: [2][215/750]	BT 0.126 (1.255)	DT 0.004 (1.121)	loss 10.126 (10.126)	gnorm 833099.625 (833099.625)	prob 1.318 (1.3175)	GS 27.922 (27.922)	mem 43.476
Train: [2][216/750]	BT 0.101 (1.250)	DT 0.002 (1.116)	loss 10.482 (10.482)	gnorm 746469.812 (746469.812)	prob 1.083 (1.0834)	GS 34.562 (34.562)	mem 43.476
Train: [2][217/750]	BT 0.078 (1.245)	DT 0.001 (1.111)	loss 10.236 (10.236)	gnorm 739406.500 (739406.500)	prob 1.797 (1.7972)	GS 29.766 (29.766)	mem 43.476
Train: [2][218/750]	BT 0.128 (1.240)	DT 0.020 (1.106)	loss 9.844 (9.844)	gnorm 689485.000 (689485.000)	prob 1.743 (1.7432)	GS 34.000 (34.000)	mem 43.509
Train: [2][219/750]	BT 0.210 (1.235)	DT 0.008 (1.101)	loss 10.437 (10.437)	gnorm 768467.000 (768467.000)	prob 1.987 (1.9871)	GS 31.016 (31.016)	mem 43.476
Train: [2][220/750]	BT 13.486 (1.291)	DT 13.273 (1.156)	loss 10.396 (10.396)	gnorm 753710.125 (753710.125)	prob 1.476 (1.4759)	GS 33.594 (33.594)	mem 43.510
Train: [2][221/750]	BT 0.108 (1.285)	DT 0.021 (1.151)	loss 9.489 (9.489)	gnorm 678609.812 (678609.812)	prob 2.620 (2.6195)	GS 39.219 (39.219)	mem 43.511
Train: [2][222/750]	BT 0.107 (1.280)	DT 0.004 (1.146)	loss 10.358 (10.358)	gnorm 640978.562 (640978.562)	prob 1.663 (1.6627)	GS 35.484 (35.484)	mem 43.512
Train: [2][223/750]	BT 0.124 (1.275)	DT 0.007 (1.141)	loss 9.598 (9.598)	gnorm 665110.375 (665110.375)	prob 2.510 (2.5096)	GS 34.688 (34.688)	mem 43.517
Train: [2][224/750]	BT 0.251 (1.270)	DT 0.051 (1.136)	loss 10.138 (10.138)	gnorm 681670.250 (681670.250)	prob 1.693 (1.6932)	GS 33.734 (33.734)	mem 43.536
Train: [2][225/750]	BT 0.107 (1.265)	DT 0.003 (1.131)	loss 9.935 (9.935)	gnorm 658560.312 (658560.312)	prob 1.937 (1.9370)	GS 33.953 (33.953)	mem 43.469
Train: [2][226/750]	BT 0.081 (1.260)	DT 0.003 (1.126)	loss 10.145 (10.145)	gnorm 662527.375 (662527.375)	prob 1.109 (1.1090)	GS 33.438 (33.438)	mem 43.468
Train: [2][227/750]	BT 0.111 (1.255)	DT 0.001 (1.121)	loss 10.336 (10.336)	gnorm 710319.250 (710319.250)	prob 0.973 (0.9734)	GS 30.719 (30.719)	mem 43.469
Train: [2][228/750]	BT 0.145 (1.250)	DT 0.009 (1.116)	loss 10.126 (10.126)	gnorm 640543.500 (640543.500)	prob 1.351 (1.3513)	GS 35.172 (35.172)	mem 43.480
Train: [2][229/750]	BT 0.092 (1.245)	DT 0.003 (1.111)	loss 10.161 (10.161)	gnorm 741583.812 (741583.812)	prob 1.633 (1.6333)	GS 34.328 (34.328)	mem 43.481
Train: [2][230/750]	BT 0.160 (1.240)	DT 0.003 (1.106)	loss 10.578 (10.578)	gnorm 716892.125 (716892.125)	prob 1.068 (1.0683)	GS 30.531 (30.531)	mem 43.517
Train: [2][231/750]	BT 0.197 (1.236)	DT 0.026 (1.102)	loss 9.862 (9.862)	gnorm 706451.000 (706451.000)	prob 3.038 (3.0384)	GS 31.656 (31.656)	mem 43.556
Train: [2][232/750]	BT 11.110 (1.278)	DT 10.945 (1.144)	loss 9.982 (9.982)	gnorm 646248.312 (646248.312)	prob 1.651 (1.6510)	GS 32.312 (32.312)	mem 43.678
Train: [2][233/750]	BT 0.103 (1.273)	DT 0.004 (1.139)	loss 9.455 (9.455)	gnorm 677379.625 (677379.625)	prob 3.397 (3.3969)	GS 28.828 (28.828)	mem 43.615
Train: [2][234/750]	BT 0.208 (1.269)	DT 0.001 (1.134)	loss 10.225 (10.225)	gnorm 718499.750 (718499.750)	prob 1.717 (1.7171)	GS 36.469 (36.469)	mem 43.708
Train: [2][235/750]	BT 0.109 (1.264)	DT 0.023 (1.129)	loss 10.284 (10.284)	gnorm 766577.500 (766577.500)	prob 0.889 (0.8887)	GS 29.438 (29.438)	mem 43.749
Train: [2][236/750]	BT 0.128 (1.259)	DT 0.001 (1.125)	loss 9.663 (9.663)	gnorm 622410.312 (622410.312)	prob 1.514 (1.5140)	GS 34.359 (34.359)	mem 44.027
Train: [2][237/750]	BT 0.136 (1.254)	DT 0.002 (1.120)	loss 10.578 (10.578)	gnorm 713641.375 (713641.375)	prob 1.118 (1.1175)	GS 27.766 (27.766)	mem 44.009
Train: [2][238/750]	BT 0.119 (1.249)	DT 0.003 (1.115)	loss 10.110 (10.110)	gnorm 676796.688 (676796.688)	prob 1.115 (1.1154)	GS 35.297 (35.297)	mem 44.047
Train: [2][239/750]	BT 0.176 (1.245)	DT 0.002 (1.111)	loss 10.217 (10.217)	gnorm 671807.562 (671807.562)	prob 1.501 (1.5006)	GS 29.469 (29.469)	mem 43.659
Train: [2][240/750]	BT 0.080 (1.240)	DT 0.001 (1.106)	loss 10.093 (10.093)	gnorm 750950.812 (750950.812)	prob 1.428 (1.4279)	GS 33.500 (33.500)	mem 43.718
Train: [2][241/750]	BT 0.109 (1.235)	DT 0.001 (1.101)	loss 10.632 (10.632)	gnorm 693160.125 (693160.125)	prob 0.183 (0.1833)	GS 41.438 (41.438)	mem 43.659
Train: [2][242/750]	BT 0.104 (1.231)	DT 0.002 (1.097)	loss 10.730 (10.730)	gnorm 726899.312 (726899.312)	prob 0.012 (0.0120)	GS 34.953 (34.953)	mem 43.659
Train: [2][243/750]	BT 0.167 (1.226)	DT 0.002 (1.092)	loss 10.121 (10.121)	gnorm 687336.625 (687336.625)	prob 0.718 (0.7182)	GS 33.984 (33.984)	mem 43.661
Train: [2][244/750]	BT 11.808 (1.270)	DT 11.668 (1.136)	loss 9.972 (9.972)	gnorm 711573.062 (711573.062)	prob 0.924 (0.9243)	GS 36.562 (36.562)	mem 43.763
Train: [2][245/750]	BT 0.118 (1.265)	DT 0.011 (1.131)	loss 10.675 (10.675)	gnorm 750077.188 (750077.188)	prob 0.415 (0.4149)	GS 40.547 (40.547)	mem 43.654
Train: [2][246/750]	BT 0.069 (1.260)	DT 0.002 (1.126)	loss 10.411 (10.411)	gnorm 825494.875 (825494.875)	prob 0.078 (0.0778)	GS 37.328 (37.328)	mem 43.655
Train: [2][247/750]	BT 0.177 (1.256)	DT 0.002 (1.122)	loss 10.546 (10.546)	gnorm 820183.125 (820183.125)	prob 0.738 (0.7379)	GS 32.281 (32.281)	mem 43.762
Train: [2][248/750]	BT 0.120 (1.251)	DT 0.007 (1.117)	loss 10.165 (10.165)	gnorm 851143.938 (851143.938)	prob 0.400 (0.4000)	GS 35.016 (35.016)	mem 43.763
Train: [2][249/750]	BT 0.192 (1.247)	DT 0.002 (1.113)	loss 10.834 (10.834)	gnorm 799825.562 (799825.562)	prob 0.922 (0.9225)	GS 31.281 (31.281)	mem 43.691
Train: [2][250/750]	BT 0.089 (1.242)	DT 0.002 (1.109)	loss 10.189 (10.189)	gnorm 753447.750 (753447.750)	prob 1.212 (1.2125)	GS 34.719 (34.719)	mem 43.658
Train: [2][251/750]	BT 0.172 (1.238)	DT 0.002 (1.104)	loss 10.664 (10.664)	gnorm 757205.312 (757205.312)	prob 1.413 (1.4130)	GS 32.297 (32.297)	mem 43.659
Train: [2][252/750]	BT 0.222 (1.234)	DT 0.001 (1.100)	loss 9.696 (9.696)	gnorm 685023.375 (685023.375)	prob 2.033 (2.0327)	GS 35.516 (35.516)	mem 43.724
Train: [2][253/750]	BT 0.136 (1.230)	DT 0.016 (1.095)	loss 11.150 (11.150)	gnorm 762179.125 (762179.125)	prob 1.222 (1.2218)	GS 26.266 (26.266)	mem 43.661
Train: [2][254/750]	BT 0.094 (1.225)	DT 0.003 (1.091)	loss 10.614 (10.614)	gnorm 718701.875 (718701.875)	prob 0.673 (0.6727)	GS 32.219 (32.219)	mem 43.661
Train: [2][255/750]	BT 0.138 (1.221)	DT 0.002 (1.087)	loss 10.204 (10.204)	gnorm 679459.062 (679459.062)	prob 1.476 (1.4765)	GS 30.781 (30.781)	mem 43.653
Train: [2][256/750]	BT 9.551 (1.253)	DT 9.448 (1.120)	loss 10.282 (10.282)	gnorm 671956.562 (671956.562)	prob 1.432 (1.4322)	GS 33.812 (33.812)	mem 43.658
Train: [2][257/750]	BT 0.150 (1.249)	DT 0.052 (1.115)	loss 10.442 (10.442)	gnorm 748768.688 (748768.688)	prob 1.727 (1.7269)	GS 24.797 (24.797)	mem 43.680
Train: [2][258/750]	BT 0.303 (1.245)	DT 0.002 (1.111)	loss 10.217 (10.217)	gnorm 741946.500 (741946.500)	prob 1.511 (1.5113)	GS 32.562 (32.562)	mem 43.675
Train: [2][259/750]	BT 0.135 (1.241)	DT 0.007 (1.107)	loss 9.791 (9.791)	gnorm 679221.250 (679221.250)	prob 1.559 (1.5588)	GS 32.844 (32.844)	mem 43.635
Train: [2][260/750]	BT 0.099 (1.237)	DT 0.003 (1.103)	loss 10.148 (10.148)	gnorm 654250.938 (654250.938)	prob 1.796 (1.7961)	GS 32.375 (32.375)	mem 43.634
Train: [2][261/750]	BT 0.101 (1.232)	DT 0.003 (1.098)	loss 10.093 (10.093)	gnorm 661798.750 (661798.750)	prob 1.569 (1.5685)	GS 30.922 (30.922)	mem 43.634
Train: [2][262/750]	BT 0.148 (1.228)	DT 0.003 (1.094)	loss 10.058 (10.058)	gnorm 718337.938 (718337.938)	prob 1.369 (1.3688)	GS 32.422 (32.422)	mem 43.633
Train: [2][263/750]	BT 0.078 (1.224)	DT 0.002 (1.090)	loss 10.559 (10.559)	gnorm 696499.875 (696499.875)	prob 0.960 (0.9596)	GS 29.891 (29.891)	mem 43.635
Train: [2][264/750]	BT 0.085 (1.220)	DT 0.001 (1.086)	loss 9.543 (9.543)	gnorm 659584.438 (659584.438)	prob 1.782 (1.7824)	GS 30.141 (30.141)	mem 43.636
Train: [2][265/750]	BT 0.076 (1.215)	DT 0.002 (1.082)	loss 10.689 (10.689)	gnorm 734356.000 (734356.000)	prob 0.906 (0.9057)	GS 30.797 (30.797)	mem 43.636
Train: [2][266/750]	BT 0.089 (1.211)	DT 0.002 (1.078)	loss 9.641 (9.641)	gnorm 728081.938 (728081.938)	prob 2.033 (2.0332)	GS 32.844 (32.844)	mem 43.636
Train: [2][267/750]	BT 0.127 (1.207)	DT 0.002 (1.074)	loss 10.537 (10.537)	gnorm 656407.750 (656407.750)	prob 0.992 (0.9923)	GS 32.547 (32.547)	mem 43.636
Train: [2][268/750]	BT 11.295 (1.245)	DT 11.128 (1.111)	loss 10.111 (10.111)	gnorm 646320.438 (646320.438)	prob 1.761 (1.7606)	GS 33.141 (33.141)	mem 43.645
Train: [2][269/750]	BT 0.141 (1.240)	DT 0.002 (1.107)	loss 10.175 (10.175)	gnorm 654341.125 (654341.125)	prob 2.612 (2.6120)	GS 31.062 (31.062)	mem 43.583
Train: [2][270/750]	BT 0.142 (1.236)	DT 0.032 (1.103)	loss 10.627 (10.627)	gnorm 654825.500 (654825.500)	prob 1.464 (1.4640)	GS 32.641 (32.641)	mem 43.583
Train: [2][271/750]	BT 0.161 (1.232)	DT 0.005 (1.099)	loss 10.170 (10.170)	gnorm 694181.000 (694181.000)	prob 2.013 (2.0130)	GS 28.078 (28.078)	mem 43.583
Train: [2][272/750]	BT 0.219 (1.229)	DT 0.029 (1.095)	loss 10.003 (10.003)	gnorm 638959.438 (638959.438)	prob 1.805 (1.8050)	GS 36.984 (36.984)	mem 43.583
Train: [2][273/750]	BT 0.149 (1.225)	DT 0.017 (1.091)	loss 10.463 (10.463)	gnorm 698706.000 (698706.000)	prob 1.022 (1.0215)	GS 30.922 (30.922)	mem 43.582
Train: [2][274/750]	BT 0.147 (1.221)	DT 0.006 (1.087)	loss 10.332 (10.332)	gnorm 664308.625 (664308.625)	prob 1.596 (1.5955)	GS 34.984 (34.984)	mem 43.581
Train: [2][275/750]	BT 0.144 (1.217)	DT 0.002 (1.083)	loss 9.950 (9.950)	gnorm 685599.812 (685599.812)	prob 2.601 (2.6009)	GS 32.094 (32.094)	mem 43.581
Train: [2][276/750]	BT 0.093 (1.213)	DT 0.001 (1.079)	loss 9.477 (9.477)	gnorm 651542.812 (651542.812)	prob 2.882 (2.8816)	GS 34.109 (34.109)	mem 43.581
Train: [2][277/750]	BT 0.158 (1.209)	DT 0.005 (1.075)	loss 9.793 (9.793)	gnorm 690536.625 (690536.625)	prob 2.540 (2.5397)	GS 31.844 (31.844)	mem 43.622
Train: [2][278/750]	BT 0.275 (1.206)	DT 0.072 (1.072)	loss 9.771 (9.771)	gnorm 673406.625 (673406.625)	prob 2.420 (2.4196)	GS 35.797 (35.797)	mem 43.631
Train: [2][279/750]	BT 0.082 (1.202)	DT 0.002 (1.068)	loss 10.349 (10.349)	gnorm 712201.500 (712201.500)	prob 1.201 (1.2007)	GS 33.641 (33.641)	mem 43.643
Train: [2][280/750]	BT 13.674 (1.246)	DT 13.516 (1.112)	loss 10.348 (10.348)	gnorm 715056.875 (715056.875)	prob 1.212 (1.2122)	GS 36.328 (36.328)	mem 43.764
Train: [2][281/750]	BT 0.096 (1.242)	DT 0.011 (1.109)	loss 10.240 (10.240)	gnorm 722889.500 (722889.500)	prob 1.757 (1.7566)	GS 34.609 (34.609)	mem 43.584
Train: [2][282/750]	BT 0.086 (1.238)	DT 0.002 (1.105)	loss 11.093 (11.093)	gnorm 730393.688 (730393.688)	prob 0.330 (0.3305)	GS 34.906 (34.906)	mem 43.585
Train: [2][283/750]	BT 0.122 (1.234)	DT 0.009 (1.101)	loss 10.053 (10.053)	gnorm 757899.250 (757899.250)	prob 1.883 (1.8834)	GS 34.344 (34.344)	mem 43.585
Train: [2][284/750]	BT 0.102 (1.230)	DT 0.001 (1.097)	loss 10.313 (10.313)	gnorm 745745.562 (745745.562)	prob 0.551 (0.5513)	GS 32.625 (32.625)	mem 43.591
Train: [2][285/750]	BT 0.090 (1.226)	DT 0.009 (1.093)	loss 9.648 (9.648)	gnorm 666115.688 (666115.688)	prob 2.767 (2.7674)	GS 31.047 (31.047)	mem 43.678
Train: [2][286/750]	BT 0.113 (1.222)	DT 0.002 (1.089)	loss 10.302 (10.302)	gnorm 692684.438 (692684.438)	prob 1.325 (1.3248)	GS 35.344 (35.344)	mem 43.717
Train: [2][287/750]	BT 0.101 (1.218)	DT 0.001 (1.085)	loss 10.212 (10.212)	gnorm 670079.500 (670079.500)	prob 1.458 (1.4580)	GS 33.359 (33.359)	mem 43.589
Train: [2][288/750]	BT 0.077 (1.214)	DT 0.001 (1.082)	loss 9.958 (9.958)	gnorm 744742.250 (744742.250)	prob 1.374 (1.3738)	GS 34.688 (34.688)	mem 43.681
Train: [2][289/750]	BT 0.196 (1.211)	DT 0.009 (1.078)	loss 9.787 (9.787)	gnorm 710673.438 (710673.438)	prob 1.516 (1.5160)	GS 32.266 (32.266)	mem 43.586
Train: [2][290/750]	BT 0.087 (1.207)	DT 0.002 (1.074)	loss 10.689 (10.689)	gnorm 718788.250 (718788.250)	prob 0.439 (0.4392)	GS 33.891 (33.891)	mem 43.586
Train: [2][291/750]	BT 0.147 (1.203)	DT 0.001 (1.071)	loss 9.991 (9.991)	gnorm 694318.188 (694318.188)	prob 1.817 (1.8173)	GS 35.797 (35.797)	mem 43.623
Train: [2][292/750]	BT 11.770 (1.239)	DT 11.639 (1.107)	loss 10.085 (10.085)	gnorm 656799.125 (656799.125)	prob 1.291 (1.2915)	GS 37.094 (37.094)	mem 43.602
Train: [2][293/750]	BT 0.089 (1.235)	DT 0.002 (1.103)	loss 10.022 (10.022)	gnorm 637042.000 (637042.000)	prob 0.713 (0.7132)	GS 32.906 (32.906)	mem 43.603
Train: [2][294/750]	BT 0.107 (1.232)	DT 0.002 (1.099)	loss 10.450 (10.450)	gnorm 709190.125 (709190.125)	prob 0.656 (0.6565)	GS 33.688 (33.688)	mem 43.543
Train: [2][295/750]	BT 0.244 (1.228)	DT 0.008 (1.096)	loss 10.697 (10.697)	gnorm 713282.312 (713282.312)	prob 0.181 (0.1808)	GS 30.031 (30.031)	mem 43.544
Train: [2][296/750]	BT 0.114 (1.225)	DT 0.005 (1.092)	loss 10.587 (10.587)	gnorm 733303.188 (733303.188)	prob 0.248 (0.2482)	GS 37.984 (37.984)	mem 43.582
Train: [2][297/750]	BT 0.121 (1.221)	DT 0.002 (1.088)	loss 10.556 (10.556)	gnorm 690386.438 (690386.438)	prob 1.060 (1.0598)	GS 30.922 (30.922)	mem 43.545
Train: [2][298/750]	BT 0.191 (1.217)	DT 0.008 (1.085)	loss 9.996 (9.996)	gnorm 616936.688 (616936.688)	prob 1.529 (1.5291)	GS 35.266 (35.266)	mem 43.545
Train: [2][299/750]	BT 0.079 (1.214)	DT 0.003 (1.081)	loss 10.029 (10.029)	gnorm 725335.750 (725335.750)	prob 1.704 (1.7039)	GS 32.469 (32.469)	mem 43.544
Train: [2][300/750]	BT 0.084 (1.210)	DT 0.001 (1.077)	loss 9.800 (9.800)	gnorm 706023.125 (706023.125)	prob 0.912 (0.9119)	GS 36.781 (36.781)	mem 43.544
Train: [2][301/750]	BT 0.180 (1.206)	DT 0.002 (1.074)	loss 10.385 (10.385)	gnorm 730180.750 (730180.750)	prob 0.979 (0.9793)	GS 26.125 (26.125)	mem 43.546
Train: [2][302/750]	BT 0.101 (1.203)	DT 0.002 (1.070)	loss 10.408 (10.408)	gnorm 729710.500 (729710.500)	prob 0.765 (0.7651)	GS 30.906 (30.906)	mem 43.546
Train: [2][303/750]	BT 0.105 (1.199)	DT 0.003 (1.067)	loss 10.236 (10.236)	gnorm 665511.312 (665511.312)	prob 0.538 (0.5376)	GS 30.453 (30.453)	mem 43.546
Train: [2][304/750]	BT 11.492 (1.233)	DT 11.373 (1.101)	loss 10.180 (10.180)	gnorm 684813.812 (684813.812)	prob 0.928 (0.9276)	GS 33.906 (33.906)	mem 43.589
Train: [2][305/750]	BT 0.063 (1.229)	DT 0.001 (1.097)	loss 10.102 (10.102)	gnorm 715063.062 (715063.062)	prob 1.596 (1.5963)	GS 29.125 (29.125)	mem 43.589
Train: [2][306/750]	BT 0.072 (1.225)	DT 0.001 (1.093)	loss 10.561 (10.561)	gnorm 688362.938 (688362.938)	prob 0.592 (0.5922)	GS 33.125 (33.125)	mem 43.622
Train: [2][307/750]	BT 0.106 (1.222)	DT 0.002 (1.090)	loss 10.577 (10.577)	gnorm 742804.375 (742804.375)	prob 1.134 (1.1344)	GS 31.969 (31.969)	mem 43.642
Train: [2][308/750]	BT 0.125 (1.218)	DT 0.006 (1.086)	loss 10.416 (10.416)	gnorm 750342.000 (750342.000)	prob 1.237 (1.2373)	GS 35.547 (35.547)	mem 43.646
Train: [2][309/750]	BT 0.117 (1.215)	DT 0.013 (1.083)	loss 10.516 (10.516)	gnorm 773790.312 (773790.312)	prob 1.663 (1.6625)	GS 29.438 (29.438)	mem 43.591
Train: [2][310/750]	BT 0.181 (1.211)	DT 0.013 (1.079)	loss 10.327 (10.327)	gnorm 796024.438 (796024.438)	prob 1.217 (1.2168)	GS 33.328 (33.328)	mem 43.655
Train: [2][311/750]	BT 0.113 (1.208)	DT 0.008 (1.076)	loss 10.638 (10.638)	gnorm 768000.688 (768000.688)	prob 1.586 (1.5863)	GS 32.125 (32.125)	mem 43.581
Train: [2][312/750]	BT 0.148 (1.204)	DT 0.009 (1.073)	loss 10.363 (10.363)	gnorm 759052.188 (759052.188)	prob 1.148 (1.1484)	GS 32.234 (32.234)	mem 43.581
Train: [2][313/750]	BT 0.122 (1.201)	DT 0.007 (1.069)	loss 10.830 (10.830)	gnorm 731709.062 (731709.062)	prob 1.483 (1.4832)	GS 33.359 (33.359)	mem 43.580
Train: [2][314/750]	BT 0.088 (1.197)	DT 0.003 (1.066)	loss 9.449 (9.449)	gnorm 724880.062 (724880.062)	prob 2.056 (2.0558)	GS 31.000 (31.000)	mem 43.593
Train: [2][315/750]	BT 0.119 (1.194)	DT 0.001 (1.062)	loss 10.494 (10.494)	gnorm 759052.438 (759052.438)	prob 1.686 (1.6856)	GS 34.500 (34.500)	mem 43.814
Train: [2][316/750]	BT 10.056 (1.222)	DT 9.888 (1.090)	loss 10.490 (10.490)	gnorm 705702.125 (705702.125)	prob 0.565 (0.5649)	GS 33.688 (33.688)	mem 43.659
Train: [2][317/750]	BT 0.098 (1.218)	DT 0.007 (1.087)	loss 10.497 (10.497)	gnorm 663948.750 (663948.750)	prob 0.854 (0.8543)	GS 28.203 (28.203)	mem 43.664
Train: [2][318/750]	BT 0.096 (1.215)	DT 0.002 (1.083)	loss 10.603 (10.603)	gnorm 670879.562 (670879.562)	prob 0.598 (0.5985)	GS 30.078 (30.078)	mem 43.664
Train: [2][319/750]	BT 0.118 (1.211)	DT 0.009 (1.080)	loss 9.935 (9.935)	gnorm 692668.438 (692668.438)	prob 2.041 (2.0408)	GS 26.922 (26.922)	mem 43.664
Train: [2][320/750]	BT 0.105 (1.208)	DT 0.002 (1.077)	loss 10.533 (10.533)	gnorm 675545.438 (675545.438)	prob 1.395 (1.3951)	GS 33.281 (33.281)	mem 43.696
Train: [2][321/750]	BT 0.086 (1.204)	DT 0.002 (1.073)	loss 9.877 (9.877)	gnorm 638196.500 (638196.500)	prob 2.115 (2.1146)	GS 37.312 (37.312)	mem 43.664
Train: [2][322/750]	BT 0.144 (1.201)	DT 0.003 (1.070)	loss 10.464 (10.464)	gnorm 697624.250 (697624.250)	prob 1.295 (1.2947)	GS 32.031 (32.031)	mem 43.665
Train: [2][323/750]	BT 0.145 (1.198)	DT 0.016 (1.067)	loss 10.331 (10.331)	gnorm 661049.375 (661049.375)	prob 1.955 (1.9547)	GS 33.500 (33.500)	mem 43.664
Train: [2][324/750]	BT 0.167 (1.195)	DT 0.005 (1.064)	loss 10.167 (10.167)	gnorm 720548.000 (720548.000)	prob 1.470 (1.4701)	GS 35.828 (35.828)	mem 43.663
Train: [2][325/750]	BT 0.148 (1.191)	DT 0.003 (1.060)	loss 10.232 (10.232)	gnorm 675562.562 (675562.562)	prob 1.515 (1.5153)	GS 28.391 (28.391)	mem 43.665
Train: [2][326/750]	BT 0.399 (1.189)	DT 0.006 (1.057)	loss 9.600 (9.600)	gnorm 707542.812 (707542.812)	prob 2.228 (2.2280)	GS 32.844 (32.844)	mem 43.665
Train: [2][327/750]	BT 0.418 (1.187)	DT 0.015 (1.054)	loss 10.198 (10.198)	gnorm 670646.250 (670646.250)	prob 1.947 (1.9467)	GS 36.844 (36.844)	mem 43.666
Train: [2][328/750]	BT 9.016 (1.211)	DT 8.927 (1.078)	loss 10.025 (10.025)	gnorm 742881.375 (742881.375)	prob 2.182 (2.1824)	GS 31.359 (31.359)	mem 43.727
Train: [2][329/750]	BT 0.121 (1.207)	DT 0.002 (1.075)	loss 10.202 (10.202)	gnorm 677850.250 (677850.250)	prob 1.164 (1.1642)	GS 25.844 (25.844)	mem 43.812
Train: [2][330/750]	BT 2.362 (1.211)	DT 2.228 (1.078)	loss 10.417 (10.417)	gnorm 675879.000 (675879.000)	prob 0.749 (0.7494)	GS 32.234 (32.234)	mem 43.725
Train: [2][331/750]	BT 0.096 (1.207)	DT 0.005 (1.075)	loss 10.839 (10.839)	gnorm 712572.500 (712572.500)	prob 0.727 (0.7274)	GS 32.812 (32.812)	mem 43.738
Train: [2][332/750]	BT 0.121 (1.204)	DT 0.002 (1.072)	loss 10.356 (10.356)	gnorm 732803.188 (732803.188)	prob 0.533 (0.5334)	GS 31.688 (31.688)	mem 43.666
Train: [2][333/750]	BT 0.172 (1.201)	DT 0.005 (1.068)	loss 9.974 (9.974)	gnorm 661227.375 (661227.375)	prob 2.548 (2.5482)	GS 32.125 (32.125)	mem 43.666
Train: [2][334/750]	BT 0.090 (1.198)	DT 0.002 (1.065)	loss 10.157 (10.157)	gnorm 691954.812 (691954.812)	prob 1.811 (1.8115)	GS 32.125 (32.125)	mem 43.666
Train: [2][335/750]	BT 0.097 (1.194)	DT 0.003 (1.062)	loss 10.784 (10.784)	gnorm 682311.750 (682311.750)	prob 0.534 (0.5337)	GS 32.797 (32.797)	mem 43.666
Train: [2][336/750]	BT 0.131 (1.191)	DT 0.012 (1.059)	loss 10.300 (10.300)	gnorm 749226.000 (749226.000)	prob 0.812 (0.8123)	GS 35.266 (35.266)	mem 43.667
Train: [2][337/750]	BT 0.088 (1.188)	DT 0.001 (1.056)	loss 10.564 (10.564)	gnorm 705276.062 (705276.062)	prob 0.554 (0.5537)	GS 26.531 (26.531)	mem 43.731
Train: [2][338/750]	BT 0.171 (1.185)	DT 0.054 (1.053)	loss 9.514 (9.514)	gnorm 675563.812 (675563.812)	prob 1.745 (1.7446)	GS 34.734 (34.734)	mem 43.814
Train: [2][339/750]	BT 0.126 (1.182)	DT 0.003 (1.050)	loss 9.896 (9.896)	gnorm 651747.500 (651747.500)	prob 0.978 (0.9784)	GS 29.031 (29.031)	mem 43.690
Train: [2][340/750]	BT 9.398 (1.206)	DT 9.185 (1.074)	loss 10.311 (10.311)	gnorm 677542.000 (677542.000)	prob 0.636 (0.6360)	GS 31.969 (31.969)	mem 43.954
Train: [2][341/750]	BT 0.096 (1.203)	DT 0.002 (1.071)	loss 10.032 (10.032)	gnorm 637869.688 (637869.688)	prob 1.048 (1.0479)	GS 33.703 (33.703)	mem 43.683
Train: [2][342/750]	BT 0.675 (1.201)	DT 0.542 (1.069)	loss 10.372 (10.372)	gnorm 659266.562 (659266.562)	prob 0.911 (0.9114)	GS 35.234 (35.234)	mem 43.745
Train: [2][343/750]	BT 0.119 (1.198)	DT 0.009 (1.066)	loss 9.645 (9.645)	gnorm 642595.125 (642595.125)	prob 1.330 (1.3304)	GS 23.688 (23.688)	mem 43.681
Train: [2][344/750]	BT 0.116 (1.195)	DT 0.016 (1.063)	loss 10.128 (10.128)	gnorm 654206.625 (654206.625)	prob 1.325 (1.3252)	GS 33.969 (33.969)	mem 43.757
Train: [2][345/750]	BT 0.146 (1.192)	DT 0.045 (1.060)	loss 10.242 (10.242)	gnorm 712149.500 (712149.500)	prob 0.868 (0.8683)	GS 35.750 (35.750)	mem 43.720
Train: [2][346/750]	BT 0.202 (1.189)	DT 0.001 (1.057)	loss 10.291 (10.291)	gnorm 682889.750 (682889.750)	prob 0.817 (0.8165)	GS 31.219 (31.219)	mem 43.794
Train: [2][347/750]	BT 0.220 (1.186)	DT 0.018 (1.054)	loss 10.693 (10.693)	gnorm 696916.938 (696916.938)	prob 0.442 (0.4418)	GS 32.094 (32.094)	mem 43.684
Train: [2][348/750]	BT 0.148 (1.183)	DT 0.010 (1.051)	loss 10.545 (10.545)	gnorm 715487.875 (715487.875)	prob 0.778 (0.7782)	GS 33.828 (33.828)	mem 43.722
Train: [2][349/750]	BT 0.127 (1.180)	DT 0.003 (1.048)	loss 9.762 (9.762)	gnorm 716144.625 (716144.625)	prob 2.269 (2.2690)	GS 30.719 (30.719)	mem 43.724
Train: [2][350/750]	BT 6.980 (1.197)	DT 6.891 (1.065)	loss 10.581 (10.581)	gnorm 644992.875 (644992.875)	prob 1.295 (1.2948)	GS 36.141 (36.141)	mem 43.663
Train: [2][351/750]	BT 0.120 (1.194)	DT 0.002 (1.061)	loss 10.547 (10.547)	gnorm 650040.625 (650040.625)	prob 1.557 (1.5574)	GS 33.297 (33.297)	mem 43.719
Train: [2][352/750]	BT 6.670 (1.209)	DT 6.482 (1.077)	loss 10.439 (10.439)	gnorm 691211.000 (691211.000)	prob 0.846 (0.8459)	GS 32.453 (32.453)	mem 43.541
Train: [2][353/750]	BT 0.087 (1.206)	DT 0.002 (1.074)	loss 10.003 (10.003)	gnorm 656064.000 (656064.000)	prob 2.001 (2.0009)	GS 29.109 (29.109)	mem 43.542
Train: [2][354/750]	BT 0.069 (1.203)	DT 0.001 (1.071)	loss 10.243 (10.243)	gnorm 657055.938 (657055.938)	prob 1.387 (1.3866)	GS 34.062 (34.062)	mem 43.541
Train: [2][355/750]	BT 0.113 (1.200)	DT 0.003 (1.068)	loss 10.349 (10.349)	gnorm 732661.750 (732661.750)	prob 1.348 (1.3478)	GS 30.156 (30.156)	mem 43.542
Train: [2][356/750]	BT 0.105 (1.197)	DT 0.002 (1.065)	loss 9.942 (9.942)	gnorm 711866.188 (711866.188)	prob 1.344 (1.3441)	GS 32.828 (32.828)	mem 43.542
Train: [2][357/750]	BT 0.214 (1.194)	DT 0.011 (1.062)	loss 10.235 (10.235)	gnorm 653878.562 (653878.562)	prob 1.676 (1.6758)	GS 25.406 (25.406)	mem 43.541
Train: [2][358/750]	BT 0.125 (1.191)	DT 0.003 (1.059)	loss 10.225 (10.225)	gnorm 700918.125 (700918.125)	prob 1.270 (1.2696)	GS 31.281 (31.281)	mem 43.559
Train: [2][359/750]	BT 0.125 (1.188)	DT 0.003 (1.056)	loss 10.368 (10.368)	gnorm 656597.750 (656597.750)	prob 0.778 (0.7777)	GS 26.156 (26.156)	mem 43.641
Train: [2][360/750]	BT 0.134 (1.185)	DT 0.003 (1.053)	loss 11.095 (11.095)	gnorm 667980.688 (667980.688)	prob 0.346 (0.3464)	GS 36.062 (36.062)	mem 43.638
Train: [2][361/750]	BT 0.227 (1.182)	DT 0.016 (1.050)	loss 10.217 (10.217)	gnorm 698430.188 (698430.188)	prob 0.922 (0.9223)	GS 34.156 (34.156)	mem 43.604
Train: [2][362/750]	BT 2.579 (1.186)	DT 2.471 (1.054)	loss 10.258 (10.258)	gnorm 631714.688 (631714.688)	prob 0.521 (0.5213)	GS 31.109 (31.109)	mem 43.585
Train: [2][363/750]	BT 0.223 (1.184)	DT 0.002 (1.051)	loss 9.947 (9.947)	gnorm 630957.312 (630957.312)	prob 1.695 (1.6952)	GS 31.719 (31.719)	mem 43.554
Train: [2][364/750]	BT 7.759 (1.202)	DT 7.668 (1.069)	loss 10.318 (10.318)	gnorm 674144.375 (674144.375)	prob 0.736 (0.7357)	GS 34.203 (34.203)	mem 43.502
Train: [2][365/750]	BT 0.101 (1.199)	DT 0.002 (1.066)	loss 9.855 (9.855)	gnorm 650982.125 (650982.125)	prob 1.685 (1.6853)	GS 31.938 (31.938)	mem 43.506
Train: [2][366/750]	BT 0.124 (1.196)	DT 0.002 (1.064)	loss 10.202 (10.202)	gnorm 606060.312 (606060.312)	prob 0.714 (0.7144)	GS 34.203 (34.203)	mem 43.506
Train: [2][367/750]	BT 0.124 (1.193)	DT 0.002 (1.061)	loss 9.955 (9.955)	gnorm 659286.125 (659286.125)	prob 1.495 (1.4953)	GS 32.172 (32.172)	mem 43.517
Train: [2][368/750]	BT 0.144 (1.190)	DT 0.014 (1.058)	loss 10.192 (10.192)	gnorm 646785.312 (646785.312)	prob 1.010 (1.0096)	GS 32.703 (32.703)	mem 43.517
Train: [2][369/750]	BT 0.143 (1.187)	DT 0.016 (1.055)	loss 10.161 (10.161)	gnorm 742210.812 (742210.812)	prob 1.374 (1.3745)	GS 32.484 (32.484)	mem 43.547
Train: [2][370/750]	BT 0.081 (1.184)	DT 0.002 (1.052)	loss 10.025 (10.025)	gnorm 664179.875 (664179.875)	prob 1.236 (1.2360)	GS 32.453 (32.453)	mem 43.688
Train: [2][371/750]	BT 0.304 (1.182)	DT 0.001 (1.049)	loss 10.390 (10.390)	gnorm 667531.312 (667531.312)	prob 1.153 (1.1534)	GS 32.016 (32.016)	mem 43.520
Train: [2][372/750]	BT 0.222 (1.179)	DT 0.013 (1.046)	loss 9.946 (9.946)	gnorm 653972.750 (653972.750)	prob 1.686 (1.6858)	GS 37.172 (37.172)	mem 43.520
Train: [2][373/750]	BT 0.174 (1.177)	DT 0.018 (1.044)	loss 10.067 (10.067)	gnorm 663856.812 (663856.812)	prob 0.798 (0.7978)	GS 25.531 (25.531)	mem 43.527
Train: [2][374/750]	BT 4.436 (1.185)	DT 4.330 (1.053)	loss 10.151 (10.151)	gnorm 732234.312 (732234.312)	prob 0.957 (0.9575)	GS 37.250 (37.250)	mem 43.560
Train: [2][375/750]	BT 0.091 (1.182)	DT 0.003 (1.050)	loss 10.582 (10.582)	gnorm 640073.750 (640073.750)	prob 0.335 (0.3347)	GS 27.984 (27.984)	mem 43.595
Train: [2][376/750]	BT 8.309 (1.201)	DT 8.230 (1.069)	loss 10.196 (10.196)	gnorm 674624.375 (674624.375)	prob 0.720 (0.7196)	GS 34.625 (34.625)	mem 43.641
Train: [2][377/750]	BT 0.089 (1.198)	DT 0.002 (1.066)	loss 10.650 (10.650)	gnorm 688488.125 (688488.125)	prob 0.830 (0.8302)	GS 33.500 (33.500)	mem 43.613
Train: [2][378/750]	BT 0.118 (1.195)	DT 0.002 (1.063)	loss 10.715 (10.715)	gnorm 680908.000 (680908.000)	prob 0.538 (0.5377)	GS 34.562 (34.562)	mem 43.656
Train: [2][379/750]	BT 0.091 (1.193)	DT 0.002 (1.060)	loss 10.402 (10.402)	gnorm 687733.938 (687733.938)	prob 1.031 (1.0313)	GS 28.859 (28.859)	mem 43.582
Train: [2][380/750]	BT 0.095 (1.190)	DT 0.002 (1.058)	loss 10.146 (10.146)	gnorm 653714.562 (653714.562)	prob 1.227 (1.2273)	GS 30.906 (30.906)	mem 43.583
Train: [2][381/750]	BT 0.126 (1.187)	DT 0.002 (1.055)	loss 10.279 (10.279)	gnorm 635266.562 (635266.562)	prob 1.691 (1.6913)	GS 30.344 (30.344)	mem 43.584
Train: [2][382/750]	BT 0.117 (1.184)	DT 0.012 (1.052)	loss 10.006 (10.006)	gnorm 664773.375 (664773.375)	prob 1.651 (1.6506)	GS 34.641 (34.641)	mem 43.585
Train: [2][383/750]	BT 0.140 (1.181)	DT 0.003 (1.049)	loss 9.819 (9.819)	gnorm 696742.938 (696742.938)	prob 2.789 (2.7892)	GS 32.953 (32.953)	mem 43.588
Train: [2][384/750]	BT 0.172 (1.179)	DT 0.002 (1.047)	loss 10.187 (10.187)	gnorm 684224.938 (684224.938)	prob 1.343 (1.3428)	GS 35.812 (35.812)	mem 43.527
Train: [2][385/750]	BT 0.116 (1.176)	DT 0.017 (1.044)	loss 10.032 (10.032)	gnorm 697590.188 (697590.188)	prob 1.469 (1.4690)	GS 30.688 (30.688)	mem 43.527
Train: [2][386/750]	BT 6.006 (1.188)	DT 5.832 (1.056)	loss 10.426 (10.426)	gnorm 709768.250 (709768.250)	prob 1.071 (1.0708)	GS 36.422 (36.422)	mem 43.561
Train: [2][387/750]	BT 0.112 (1.186)	DT 0.001 (1.054)	loss 10.202 (10.202)	gnorm 644885.062 (644885.062)	prob 1.585 (1.5852)	GS 31.641 (31.641)	mem 43.562
Train: [2][388/750]	BT 8.196 (1.204)	DT 8.069 (1.072)	loss 10.337 (10.337)	gnorm 643908.625 (643908.625)	prob 1.102 (1.1021)	GS 34.297 (34.297)	mem 43.641
Train: [2][389/750]	BT 0.149 (1.201)	DT 0.008 (1.069)	loss 10.554 (10.554)	gnorm 675116.688 (675116.688)	prob 0.033 (0.0328)	GS 31.312 (31.312)	mem 43.585
Train: [2][390/750]	BT 0.171 (1.198)	DT 0.002 (1.066)	loss 9.952 (9.952)	gnorm 675277.188 (675277.188)	prob 0.778 (0.7784)	GS 37.078 (37.078)	mem 43.635
Train: [2][391/750]	BT 0.151 (1.196)	DT 0.004 (1.064)	loss 9.750 (9.750)	gnorm 652854.062 (652854.062)	prob 1.319 (1.3189)	GS 32.531 (32.531)	mem 43.704
Train: [2][392/750]	BT 0.082 (1.193)	DT 0.002 (1.061)	loss 9.799 (9.799)	gnorm 660659.250 (660659.250)	prob 0.888 (0.8876)	GS 35.969 (35.969)	mem 43.586
Train: [2][393/750]	BT 0.087 (1.190)	DT 0.002 (1.058)	loss 10.886 (10.886)	gnorm 650410.188 (650410.188)	prob 0.233 (0.2333)	GS 29.859 (29.859)	mem 43.586
Train: [2][394/750]	BT 0.099 (1.187)	DT 0.001 (1.055)	loss 10.012 (10.012)	gnorm 661695.438 (661695.438)	prob 1.475 (1.4751)	GS 34.406 (34.406)	mem 43.586
Train: [2][395/750]	BT 0.236 (1.185)	DT 0.005 (1.053)	loss 10.578 (10.578)	gnorm 715937.500 (715937.500)	prob 1.007 (1.0072)	GS 31.328 (31.328)	mem 43.587
Train: [2][396/750]	BT 0.145 (1.182)	DT 0.002 (1.050)	loss 10.184 (10.184)	gnorm 692421.250 (692421.250)	prob 1.325 (1.3254)	GS 29.688 (29.688)	mem 43.586
Train: [2][397/750]	BT 0.149 (1.180)	DT 0.002 (1.047)	loss 10.381 (10.381)	gnorm 682579.875 (682579.875)	prob 1.161 (1.1612)	GS 29.781 (29.781)	mem 43.619
Train: [2][398/750]	BT 0.169 (1.177)	DT 0.026 (1.045)	loss 9.986 (9.986)	gnorm 643103.500 (643103.500)	prob 1.412 (1.4116)	GS 28.297 (28.297)	mem 43.585
Train: [2][399/750]	BT 0.142 (1.175)	DT 0.012 (1.042)	loss 10.501 (10.501)	gnorm 635008.625 (635008.625)	prob 1.146 (1.1458)	GS 27.047 (27.047)	mem 43.610
Train: [2][400/750]	BT 11.554 (1.200)	DT 11.460 (1.068)	loss 9.884 (9.884)	gnorm 614859.938 (614859.938)	prob 1.692 (1.6923)	GS 33.422 (33.422)	mem 43.461
Train: [2][401/750]	BT 0.084 (1.198)	DT 0.002 (1.066)	loss 10.246 (10.246)	gnorm 644224.250 (644224.250)	prob 1.917 (1.9170)	GS 29.797 (29.797)	mem 43.571
Train: [2][402/750]	BT 0.092 (1.195)	DT 0.002 (1.063)	loss 10.384 (10.384)	gnorm 625690.188 (625690.188)	prob 1.618 (1.6181)	GS 32.844 (32.844)	mem 43.540
Train: [2][403/750]	BT 0.172 (1.192)	DT 0.002 (1.060)	loss 10.154 (10.154)	gnorm 617456.500 (617456.500)	prob 2.175 (2.1753)	GS 28.219 (28.219)	mem 43.584
Train: [2][404/750]	BT 0.133 (1.190)	DT 0.002 (1.058)	loss 10.188 (10.188)	gnorm 654008.312 (654008.312)	prob 2.181 (2.1814)	GS 36.031 (36.031)	mem 43.514
Train: [2][405/750]	BT 0.152 (1.187)	DT 0.016 (1.055)	loss 10.395 (10.395)	gnorm 612082.750 (612082.750)	prob 1.930 (1.9297)	GS 28.656 (28.656)	mem 43.504
Train: [2][406/750]	BT 0.097 (1.185)	DT 0.002 (1.053)	loss 10.767 (10.767)	gnorm 733963.000 (733963.000)	prob 1.017 (1.0166)	GS 33.266 (33.266)	mem 43.579
Train: [2][407/750]	BT 0.167 (1.182)	DT 0.002 (1.050)	loss 10.198 (10.198)	gnorm 646764.000 (646764.000)	prob 2.361 (2.3612)	GS 32.406 (32.406)	mem 43.475
Train: [2][408/750]	BT 0.183 (1.180)	DT 0.024 (1.048)	loss 9.891 (9.891)	gnorm 620182.938 (620182.938)	prob 2.150 (2.1500)	GS 31.094 (31.094)	mem 43.494
Train: [2][409/750]	BT 0.149 (1.177)	DT 0.013 (1.045)	loss 9.750 (9.750)	gnorm 597465.875 (597465.875)	prob 2.318 (2.3180)	GS 43.719 (43.719)	mem 43.486
Train: [2][410/750]	BT 0.728 (1.176)	DT 0.622 (1.044)	loss 9.971 (9.971)	gnorm 661258.188 (661258.188)	prob 2.514 (2.5138)	GS 33.094 (33.094)	mem 43.602
Train: [2][411/750]	BT 0.194 (1.174)	DT 0.026 (1.042)	loss 10.098 (10.098)	gnorm 643853.250 (643853.250)	prob 2.552 (2.5519)	GS 35.406 (35.406)	mem 43.612
Train: [2][412/750]	BT 10.080 (1.195)	DT 9.919 (1.063)	loss 9.662 (9.662)	gnorm 638438.250 (638438.250)	prob 2.458 (2.4580)	GS 32.094 (32.094)	mem 43.594
Train: [2][413/750]	BT 0.185 (1.193)	DT 0.007 (1.061)	loss 10.364 (10.364)	gnorm 625935.312 (625935.312)	prob 1.413 (1.4131)	GS 32.266 (32.266)	mem 43.537
Train: [2][414/750]	BT 0.136 (1.190)	DT 0.019 (1.058)	loss 10.098 (10.098)	gnorm 636349.875 (636349.875)	prob 1.626 (1.6264)	GS 35.953 (35.953)	mem 43.537
Train: [2][415/750]	BT 0.062 (1.187)	DT 0.004 (1.055)	loss 9.620 (9.620)	gnorm 629798.812 (629798.812)	prob 1.941 (1.9411)	GS 30.719 (30.719)	mem 43.538
Train: [2][416/750]	BT 0.109 (1.185)	DT 0.002 (1.053)	loss 9.512 (9.512)	gnorm 577408.562 (577408.562)	prob 1.566 (1.5664)	GS 33.281 (33.281)	mem 43.539
Train: [2][417/750]	BT 0.102 (1.182)	DT 0.003 (1.050)	loss 10.200 (10.200)	gnorm 600564.000 (600564.000)	prob 1.617 (1.6169)	GS 36.812 (36.812)	mem 43.576
Train: [2][418/750]	BT 0.152 (1.180)	DT 0.013 (1.048)	loss 10.248 (10.248)	gnorm 632035.500 (632035.500)	prob 1.613 (1.6132)	GS 33.719 (33.719)	mem 43.681
Train: [2][419/750]	BT 0.173 (1.177)	DT 0.038 (1.046)	loss 10.476 (10.476)	gnorm 663594.250 (663594.250)	prob 1.491 (1.4906)	GS 28.859 (28.859)	mem 43.539
Train: [2][420/750]	BT 0.097 (1.175)	DT 0.002 (1.043)	loss 9.990 (9.990)	gnorm 656500.062 (656500.062)	prob 1.973 (1.9727)	GS 31.875 (31.875)	mem 43.539
Train: [2][421/750]	BT 0.140 (1.172)	DT 0.003 (1.041)	loss 10.489 (10.489)	gnorm 654720.438 (654720.438)	prob 1.514 (1.5139)	GS 29.656 (29.656)	mem 43.558
Train: [2][422/750]	BT 3.091 (1.177)	DT 2.881 (1.045)	loss 10.436 (10.436)	gnorm 675909.688 (675909.688)	prob 1.174 (1.1744)	GS 33.859 (33.859)	mem 43.759
Train: [2][423/750]	BT 0.143 (1.174)	DT 0.012 (1.042)	loss 10.878 (10.878)	gnorm 688946.438 (688946.438)	prob 1.039 (1.0386)	GS 36.156 (36.156)	mem 43.633
Train: [2][424/750]	BT 6.766 (1.188)	DT 6.622 (1.056)	loss 9.906 (9.906)	gnorm 599362.938 (599362.938)	prob 1.696 (1.6962)	GS 34.484 (34.484)	mem 43.602
Train: [2][425/750]	BT 0.199 (1.185)	DT 0.003 (1.053)	loss 9.529 (9.529)	gnorm 622064.250 (622064.250)	prob 2.193 (2.1933)	GS 32.141 (32.141)	mem 43.602
Train: [2][426/750]	BT 1.837 (1.187)	DT 1.742 (1.055)	loss 9.820 (9.820)	gnorm 611247.875 (611247.875)	prob 2.293 (2.2929)	GS 36.844 (36.844)	mem 43.635
Train: [2][427/750]	BT 0.087 (1.184)	DT 0.002 (1.052)	loss 10.568 (10.568)	gnorm 755627.938 (755627.938)	prob 0.616 (0.6156)	GS 36.094 (36.094)	mem 43.635
Train: [2][428/750]	BT 0.131 (1.182)	DT 0.002 (1.050)	loss 10.315 (10.315)	gnorm 692490.375 (692490.375)	prob 1.453 (1.4532)	GS 36.797 (36.797)	mem 43.636
Train: [2][429/750]	BT 0.147 (1.179)	DT 0.003 (1.047)	loss 9.625 (9.625)	gnorm 611767.312 (611767.312)	prob 2.183 (2.1831)	GS 30.312 (30.312)	mem 43.669
Train: [2][430/750]	BT 0.281 (1.177)	DT 0.006 (1.045)	loss 10.180 (10.180)	gnorm 641404.688 (641404.688)	prob 1.074 (1.0737)	GS 35.672 (35.672)	mem 43.769
Train: [2][431/750]	BT 0.169 (1.175)	DT 0.013 (1.043)	loss 10.736 (10.736)	gnorm 708894.812 (708894.812)	prob 0.523 (0.5226)	GS 26.422 (26.422)	mem 43.668
Train: [2][432/750]	BT 0.143 (1.173)	DT 0.008 (1.040)	loss 10.359 (10.359)	gnorm 645288.000 (645288.000)	prob 0.405 (0.4050)	GS 34.531 (34.531)	mem 43.658
Train: [2][433/750]	BT 0.095 (1.170)	DT 0.002 (1.038)	loss 10.636 (10.636)	gnorm 679514.875 (679514.875)	prob 0.548 (0.5482)	GS 31.766 (31.766)	mem 43.636
Train: [2][434/750]	BT 3.454 (1.175)	DT 3.358 (1.043)	loss 10.349 (10.349)	gnorm 585810.375 (585810.375)	prob 0.458 (0.4580)	GS 29.062 (29.062)	mem 43.669
Train: [2][435/750]	BT 0.096 (1.173)	DT 0.003 (1.041)	loss 10.320 (10.320)	gnorm 623294.062 (623294.062)	prob 0.849 (0.8489)	GS 31.812 (31.812)	mem 43.745
Train: [2][436/750]	BT 11.943 (1.198)	DT 11.865 (1.066)	loss 10.071 (10.071)	gnorm 685811.750 (685811.750)	prob 0.647 (0.6472)	GS 33.391 (33.391)	mem 43.703
Train: [2][437/750]	BT 0.107 (1.195)	DT 0.002 (1.063)	loss 10.638 (10.638)	gnorm 632115.750 (632115.750)	prob -0.671 (-0.6707)	GS 29.250 (29.250)	mem 43.683
Train: [2][438/750]	BT 0.098 (1.193)	DT 0.008 (1.061)	loss 9.910 (9.910)	gnorm 627054.312 (627054.312)	prob -0.156 (-0.1557)	GS 33.047 (33.047)	mem 43.684
Train: [2][439/750]	BT 0.086 (1.190)	DT 0.002 (1.058)	loss 9.485 (9.485)	gnorm 637587.062 (637587.062)	prob 0.906 (0.9064)	GS 29.984 (29.984)	mem 43.685
Train: [2][440/750]	BT 0.150 (1.188)	DT 0.048 (1.056)	loss 9.897 (9.897)	gnorm 619604.750 (619604.750)	prob 0.350 (0.3503)	GS 37.219 (37.219)	mem 43.684
Train: [2][441/750]	BT 0.078 (1.185)	DT 0.001 (1.054)	loss 9.942 (9.942)	gnorm 674178.375 (674178.375)	prob 0.242 (0.2417)	GS 34.328 (34.328)	mem 43.684
Train: [2][442/750]	BT 0.187 (1.183)	DT 0.001 (1.051)	loss 10.287 (10.287)	gnorm 631437.625 (631437.625)	prob 0.019 (0.0193)	GS 33.078 (33.078)	mem 43.776
Train: [2][443/750]	BT 0.263 (1.181)	DT 0.011 (1.049)	loss 10.861 (10.861)	gnorm 702846.688 (702846.688)	prob 0.714 (0.7143)	GS 26.281 (26.281)	mem 43.684
Train: [2][444/750]	BT 0.113 (1.178)	DT 0.003 (1.047)	loss 9.853 (9.853)	gnorm 651052.625 (651052.625)	prob 0.943 (0.9429)	GS 34.156 (34.156)	mem 43.726
Train: [2][445/750]	BT 0.160 (1.176)	DT 0.002 (1.044)	loss 10.537 (10.537)	gnorm 671565.062 (671565.062)	prob 1.086 (1.0862)	GS 37.984 (37.984)	mem 43.765
Train: [2][446/750]	BT 0.209 (1.174)	DT 0.003 (1.042)	loss 10.051 (10.051)	gnorm 645866.625 (645866.625)	prob 0.652 (0.6517)	GS 31.688 (31.688)	mem 43.683
Train: [2][447/750]	BT 0.167 (1.172)	DT 0.002 (1.040)	loss 10.379 (10.379)	gnorm 669048.000 (669048.000)	prob 0.870 (0.8702)	GS 32.609 (32.609)	mem 43.738
Train: [2][448/750]	BT 11.845 (1.196)	DT 11.720 (1.063)	loss 10.887 (10.887)	gnorm 720591.938 (720591.938)	prob 0.299 (0.2993)	GS 36.812 (36.812)	mem 43.515
Train: [2][449/750]	BT 0.121 (1.193)	DT 0.002 (1.061)	loss 10.644 (10.644)	gnorm 693558.312 (693558.312)	prob 1.389 (1.3893)	GS 29.453 (29.453)	mem 43.576
Train: [2][450/750]	BT 0.099 (1.191)	DT 0.004 (1.059)	loss 9.692 (9.692)	gnorm 612123.375 (612123.375)	prob 1.659 (1.6591)	GS 35.766 (35.766)	mem 43.485
Train: [2][451/750]	BT 0.144 (1.188)	DT 0.001 (1.056)	loss 10.677 (10.677)	gnorm 658775.438 (658775.438)	prob 0.422 (0.4223)	GS 33.438 (33.438)	mem 43.540
Train: [2][452/750]	BT 0.128 (1.186)	DT 0.013 (1.054)	loss 10.734 (10.734)	gnorm 636130.125 (636130.125)	prob 0.194 (0.1938)	GS 35.812 (35.812)	mem 43.474
Train: [2][453/750]	BT 0.141 (1.184)	DT 0.002 (1.052)	loss 9.998 (9.998)	gnorm 661624.250 (661624.250)	prob 1.100 (1.0996)	GS 31.312 (31.312)	mem 43.475
Train: [2][454/750]	BT 0.182 (1.182)	DT 0.002 (1.049)	loss 10.665 (10.665)	gnorm 653068.688 (653068.688)	prob 0.370 (0.3704)	GS 33.062 (33.062)	mem 43.476
Train: [2][455/750]	BT 0.084 (1.179)	DT 0.003 (1.047)	loss 10.393 (10.393)	gnorm 644102.688 (644102.688)	prob 1.303 (1.3032)	GS 28.094 (28.094)	mem 43.476
Train: [2][456/750]	BT 0.187 (1.177)	DT 0.001 (1.045)	loss 10.441 (10.441)	gnorm 606836.562 (606836.562)	prob 0.264 (0.2639)	GS 32.734 (32.734)	mem 43.476
Train: [2][457/750]	BT 0.140 (1.175)	DT 0.003 (1.043)	loss 9.911 (9.911)	gnorm 612519.938 (612519.938)	prob 0.704 (0.7044)	GS 26.188 (26.188)	mem 43.508
Train: [2][458/750]	BT 0.187 (1.173)	DT 0.006 (1.040)	loss 10.135 (10.135)	gnorm 658090.062 (658090.062)	prob 1.356 (1.3565)	GS 37.641 (37.641)	mem 43.476
Train: [2][459/750]	BT 0.116 (1.170)	DT 0.002 (1.038)	loss 9.899 (9.899)	gnorm 584677.562 (584677.562)	prob 1.033 (1.0327)	GS 31.062 (31.062)	mem 43.477
Train: [2][460/750]	BT 11.172 (1.192)	DT 11.052 (1.060)	loss 10.229 (10.229)	gnorm 637511.062 (637511.062)	prob 1.081 (1.0810)	GS 32.734 (32.734)	mem 43.617
Train: [2][461/750]	BT 0.084 (1.190)	DT 0.001 (1.057)	loss 10.314 (10.314)	gnorm 646735.125 (646735.125)	prob 1.448 (1.4480)	GS 27.312 (27.312)	mem 43.617
Train: [2][462/750]	BT 0.100 (1.187)	DT 0.003 (1.055)	loss 10.557 (10.557)	gnorm 688755.750 (688755.750)	prob 0.297 (0.2968)	GS 31.188 (31.188)	mem 43.616
Train: [2][463/750]	BT 0.089 (1.185)	DT 0.002 (1.053)	loss 10.541 (10.541)	gnorm 639601.500 (639601.500)	prob 0.906 (0.9057)	GS 30.656 (30.656)	mem 43.643
Train: [2][464/750]	BT 0.217 (1.183)	DT 0.021 (1.051)	loss 10.056 (10.056)	gnorm 582910.625 (582910.625)	prob 0.825 (0.8255)	GS 32.406 (32.406)	mem 43.616
Train: [2][465/750]	BT 0.081 (1.180)	DT 0.002 (1.048)	loss 9.796 (9.796)	gnorm 597326.000 (597326.000)	prob 1.338 (1.3377)	GS 30.672 (30.672)	mem 43.629
Train: [2][466/750]	BT 0.194 (1.178)	DT 0.002 (1.046)	loss 10.252 (10.252)	gnorm 627992.125 (627992.125)	prob 0.175 (0.1753)	GS 32.828 (32.828)	mem 43.618
Train: [2][467/750]	BT 0.122 (1.176)	DT 0.011 (1.044)	loss 10.944 (10.944)	gnorm 651977.000 (651977.000)	prob 0.418 (0.4180)	GS 29.422 (29.422)	mem 43.625
Train: [2][468/750]	BT 0.154 (1.174)	DT 0.001 (1.042)	loss 10.193 (10.193)	gnorm 628643.375 (628643.375)	prob 0.619 (0.6189)	GS 33.297 (33.297)	mem 43.620
Train: [2][469/750]	BT 0.160 (1.172)	DT 0.003 (1.040)	loss 10.545 (10.545)	gnorm 640679.812 (640679.812)	prob 0.536 (0.5360)	GS 31.219 (31.219)	mem 43.685
Train: [2][470/750]	BT 0.213 (1.170)	DT 0.003 (1.037)	loss 10.620 (10.620)	gnorm 735114.062 (735114.062)	prob 0.160 (0.1604)	GS 28.469 (28.469)	mem 43.620
Train: [2][471/750]	BT 0.086 (1.167)	DT 0.002 (1.035)	loss 10.330 (10.330)	gnorm 597561.750 (597561.750)	prob 0.466 (0.4664)	GS 32.281 (32.281)	mem 43.621
Train: [2][472/750]	BT 14.979 (1.197)	DT 14.878 (1.064)	loss 9.863 (9.863)	gnorm 634758.000 (634758.000)	prob 1.161 (1.1607)	GS 35.656 (35.656)	mem 43.552
Train: [2][473/750]	BT 0.071 (1.194)	DT 0.001 (1.062)	loss 10.528 (10.528)	gnorm 599620.125 (599620.125)	prob -0.123 (-0.1229)	GS 32.672 (32.672)	mem 43.560
Train: [2][474/750]	BT 0.115 (1.192)	DT 0.002 (1.060)	loss 9.441 (9.441)	gnorm 626151.188 (626151.188)	prob 1.145 (1.1454)	GS 31.281 (31.281)	mem 43.721
Train: [2][475/750]	BT 0.177 (1.190)	DT 0.004 (1.058)	loss 9.422 (9.422)	gnorm 650609.438 (650609.438)	prob 2.192 (2.1921)	GS 31.031 (31.031)	mem 43.816
Train: [2][476/750]	BT 0.213 (1.188)	DT 0.010 (1.056)	loss 10.314 (10.314)	gnorm 652660.062 (652660.062)	prob 0.892 (0.8923)	GS 37.797 (37.797)	mem 43.857
Train: [2][477/750]	BT 0.113 (1.185)	DT 0.002 (1.053)	loss 10.068 (10.068)	gnorm 644802.438 (644802.438)	prob 1.566 (1.5662)	GS 32.250 (32.250)	mem 43.636
Train: [2][478/750]	BT 0.118 (1.183)	DT 0.002 (1.051)	loss 10.232 (10.232)	gnorm 670836.438 (670836.438)	prob 0.796 (0.7960)	GS 33.109 (33.109)	mem 43.593
Train: [2][479/750]	BT 0.135 (1.181)	DT 0.003 (1.049)	loss 10.479 (10.479)	gnorm 645334.000 (645334.000)	prob 0.902 (0.9015)	GS 31.500 (31.500)	mem 43.593
Train: [2][480/750]	BT 0.105 (1.179)	DT 0.012 (1.047)	loss 10.314 (10.314)	gnorm 674082.312 (674082.312)	prob 0.956 (0.9565)	GS 33.625 (33.625)	mem 43.593
Train: [2][481/750]	BT 0.114 (1.177)	DT 0.001 (1.045)	loss 10.053 (10.053)	gnorm 615900.750 (615900.750)	prob 0.789 (0.7887)	GS 29.922 (29.922)	mem 43.592
Train: [2][482/750]	BT 0.075 (1.174)	DT 0.002 (1.042)	loss 10.044 (10.044)	gnorm 589956.375 (589956.375)	prob 0.633 (0.6326)	GS 34.062 (34.062)	mem 43.594
Train: [2][483/750]	BT 0.139 (1.172)	DT 0.001 (1.040)	loss 10.186 (10.186)	gnorm 651299.750 (651299.750)	prob 1.599 (1.5991)	GS 29.188 (29.188)	mem 43.646
Train: [2][484/750]	BT 9.481 (1.189)	DT 9.309 (1.057)	loss 9.858 (9.858)	gnorm 651748.375 (651748.375)	prob 1.915 (1.9149)	GS 37.672 (37.672)	mem 43.600
Train: [2][485/750]	BT 0.085 (1.187)	DT 0.001 (1.055)	loss 9.878 (9.878)	gnorm 593517.125 (593517.125)	prob 1.404 (1.4037)	GS 31.359 (31.359)	mem 43.602
Train: [2][486/750]	BT 0.120 (1.185)	DT 0.004 (1.053)	loss 10.347 (10.347)	gnorm 670402.188 (670402.188)	prob 1.357 (1.3566)	GS 35.547 (35.547)	mem 43.602
Train: [2][487/750]	BT 0.176 (1.183)	DT 0.003 (1.051)	loss 10.120 (10.120)	gnorm 664492.188 (664492.188)	prob 1.470 (1.4701)	GS 30.797 (30.797)	mem 43.657
Train: [2][488/750]	BT 0.217 (1.181)	DT 0.014 (1.049)	loss 10.058 (10.058)	gnorm 626515.125 (626515.125)	prob 1.325 (1.3247)	GS 37.969 (37.969)	mem 43.621
Train: [2][489/750]	BT 0.113 (1.179)	DT 0.029 (1.047)	loss 10.441 (10.441)	gnorm 699250.375 (699250.375)	prob 1.405 (1.4052)	GS 31.312 (31.312)	mem 43.622
Train: [2][490/750]	BT 0.113 (1.176)	DT 0.002 (1.045)	loss 10.344 (10.344)	gnorm 693576.625 (693576.625)	prob 1.445 (1.4446)	GS 33.219 (33.219)	mem 43.622
Train: [2][491/750]	BT 0.155 (1.174)	DT 0.001 (1.042)	loss 9.513 (9.513)	gnorm 621992.125 (621992.125)	prob 2.630 (2.6299)	GS 32.484 (32.484)	mem 43.637
Train: [2][492/750]	BT 0.129 (1.172)	DT 0.002 (1.040)	loss 9.824 (9.824)	gnorm 609817.750 (609817.750)	prob 1.490 (1.4904)	GS 34.188 (34.188)	mem 43.632
Train: [2][493/750]	BT 0.166 (1.170)	DT 0.036 (1.038)	loss 10.603 (10.603)	gnorm 625021.938 (625021.938)	prob 0.884 (0.8840)	GS 27.594 (27.594)	mem 43.610
Train: [2][494/750]	BT 0.104 (1.168)	DT 0.002 (1.036)	loss 10.213 (10.213)	gnorm 712628.750 (712628.750)	prob 1.138 (1.1375)	GS 36.953 (36.953)	mem 43.588
Train: [2][495/750]	BT 0.091 (1.166)	DT 0.003 (1.034)	loss 10.070 (10.070)	gnorm 611548.875 (611548.875)	prob 0.843 (0.8425)	GS 32.078 (32.078)	mem 43.567
Train: [2][496/750]	BT 9.285 (1.182)	DT 9.165 (1.050)	loss 9.663 (9.663)	gnorm 629406.562 (629406.562)	prob 1.667 (1.6667)	GS 34.062 (34.062)	mem 43.642
Train: [2][497/750]	BT 0.174 (1.180)	DT 0.011 (1.048)	loss 10.315 (10.315)	gnorm 662120.312 (662120.312)	prob 0.997 (0.9966)	GS 35.406 (35.406)	mem 43.685
Train: [2][498/750]	BT 0.212 (1.178)	DT 0.002 (1.046)	loss 10.138 (10.138)	gnorm 581965.188 (581965.188)	prob 0.875 (0.8748)	GS 36.344 (36.344)	mem 43.653
Train: [2][499/750]	BT 0.155 (1.176)	DT 0.008 (1.044)	loss 10.078 (10.078)	gnorm 617382.375 (617382.375)	prob 1.290 (1.2902)	GS 30.719 (30.719)	mem 43.624
Train: [2][500/750]	BT 3.611 (1.181)	DT 3.505 (1.049)	loss 10.262 (10.262)	gnorm 638459.562 (638459.562)	prob 1.242 (1.2417)	GS 35.359 (35.359)	mem 43.640
Train: [2][501/750]	BT 0.117 (1.179)	DT 0.004 (1.047)	loss 9.591 (9.591)	gnorm 569032.938 (569032.938)	prob 1.146 (1.1459)	GS 30.469 (30.469)	mem 43.577
Train: [2][502/750]	BT 0.110 (1.177)	DT 0.003 (1.045)	loss 9.865 (9.865)	gnorm 552662.625 (552662.625)	prob 1.739 (1.7388)	GS 33.422 (33.422)	mem 43.620
Train: [2][503/750]	BT 0.118 (1.175)	DT 0.002 (1.043)	loss 9.664 (9.664)	gnorm 605076.938 (605076.938)	prob 1.744 (1.7435)	GS 24.719 (24.719)	mem 43.682
Train: [2][504/750]	BT 0.164 (1.173)	DT 0.002 (1.041)	loss 9.864 (9.864)	gnorm 579220.312 (579220.312)	prob 2.138 (2.1382)	GS 34.766 (34.766)	mem 43.673
Train: [2][505/750]	BT 0.089 (1.171)	DT 0.002 (1.039)	loss 9.952 (9.952)	gnorm 616035.625 (616035.625)	prob 2.294 (2.2938)	GS 33.922 (33.922)	mem 43.580
Train: [2][506/750]	BT 0.101 (1.168)	DT 0.002 (1.037)	loss 10.418 (10.418)	gnorm 613228.000 (613228.000)	prob 1.584 (1.5835)	GS 34.125 (34.125)	mem 43.583
Train: [2][507/750]	BT 0.088 (1.166)	DT 0.001 (1.035)	loss 10.503 (10.503)	gnorm 590989.375 (590989.375)	prob 1.733 (1.7326)	GS 29.281 (29.281)	mem 43.582
Train: [2][508/750]	BT 9.326 (1.182)	DT 9.198 (1.051)	loss 10.340 (10.340)	gnorm 553864.750 (553864.750)	prob 1.591 (1.5910)	GS 31.891 (31.891)	mem 43.660
Train: [2][509/750]	BT 0.368 (1.181)	DT 0.006 (1.049)	loss 10.414 (10.414)	gnorm 598626.188 (598626.188)	prob 1.690 (1.6901)	GS 31.812 (31.812)	mem 43.560
Train: [2][510/750]	BT 0.181 (1.179)	DT 0.007 (1.047)	loss 10.212 (10.212)	gnorm 599726.562 (599726.562)	prob 1.735 (1.7351)	GS 32.297 (32.297)	mem 43.606
Train: [2][511/750]	BT 0.129 (1.177)	DT 0.002 (1.045)	loss 10.505 (10.505)	gnorm 619336.750 (619336.750)	prob 1.729 (1.7289)	GS 36.500 (36.500)	mem 43.538
Train: [2][512/750]	BT 2.024 (1.178)	DT 1.924 (1.046)	loss 10.732 (10.732)	gnorm 618169.250 (618169.250)	prob 0.909 (0.9093)	GS 30.016 (30.016)	mem 43.574
Train: [2][513/750]	BT 0.160 (1.176)	DT 0.001 (1.044)	loss 9.933 (9.933)	gnorm 625376.125 (625376.125)	prob 1.852 (1.8522)	GS 29.734 (29.734)	mem 43.597
Train: [2][514/750]	BT 0.104 (1.174)	DT 0.002 (1.042)	loss 10.052 (10.052)	gnorm 557244.125 (557244.125)	prob 1.798 (1.7977)	GS 30.062 (30.062)	mem 43.435
Train: [2][515/750]	BT 0.134 (1.172)	DT 0.007 (1.040)	loss 10.304 (10.304)	gnorm 595709.375 (595709.375)	prob 1.478 (1.4783)	GS 30.656 (30.656)	mem 43.435
Train: [2][516/750]	BT 0.143 (1.170)	DT 0.002 (1.038)	loss 10.140 (10.140)	gnorm 628150.750 (628150.750)	prob 1.582 (1.5820)	GS 32.125 (32.125)	mem 43.439
Train: [2][517/750]	BT 0.103 (1.168)	DT 0.002 (1.036)	loss 10.337 (10.337)	gnorm 620924.000 (620924.000)	prob 1.157 (1.1572)	GS 28.828 (28.828)	mem 43.439
Train: [2][518/750]	BT 0.075 (1.166)	DT 0.002 (1.034)	loss 10.381 (10.381)	gnorm 597226.375 (597226.375)	prob 0.806 (0.8062)	GS 35.578 (35.578)	mem 43.377
Train: [2][519/750]	BT 0.095 (1.164)	DT 0.002 (1.032)	loss 9.792 (9.792)	gnorm 600413.062 (600413.062)	prob 1.747 (1.7466)	GS 33.391 (33.391)	mem 43.381
Train: [2][520/750]	BT 8.214 (1.178)	DT 8.116 (1.046)	loss 10.189 (10.189)	gnorm 547545.312 (547545.312)	prob 1.265 (1.2646)	GS 31.516 (31.516)	mem 43.532
Train: [2][521/750]	BT 0.167 (1.176)	DT 0.012 (1.044)	loss 10.244 (10.244)	gnorm 576264.000 (576264.000)	prob 1.828 (1.8282)	GS 31.484 (31.484)	mem 43.532
Train: [2][522/750]	BT 0.092 (1.174)	DT 0.002 (1.042)	loss 10.384 (10.384)	gnorm 610158.875 (610158.875)	prob 1.322 (1.3219)	GS 37.953 (37.953)	mem 43.532
Train: [2][523/750]	BT 0.087 (1.172)	DT 0.003 (1.040)	loss 9.983 (9.983)	gnorm 626891.125 (626891.125)	prob 1.848 (1.8476)	GS 28.688 (28.688)	mem 43.532
Train: [2][524/750]	BT 4.205 (1.177)	DT 4.068 (1.046)	loss 10.570 (10.570)	gnorm 653986.000 (653986.000)	prob 0.733 (0.7328)	GS 33.062 (33.062)	mem 43.745
Train: [2][525/750]	BT 0.104 (1.175)	DT 0.008 (1.044)	loss 10.341 (10.341)	gnorm 669567.250 (669567.250)	prob 1.847 (1.8469)	GS 29.500 (29.500)	mem 43.567
Train: [2][526/750]	BT 0.124 (1.173)	DT 0.002 (1.042)	loss 9.947 (9.947)	gnorm 571947.312 (571947.312)	prob 2.267 (2.2665)	GS 35.141 (35.141)	mem 43.567
Train: [2][527/750]	BT 0.146 (1.171)	DT 0.024 (1.040)	loss 9.818 (9.818)	gnorm 602050.688 (602050.688)	prob 2.937 (2.9373)	GS 32.078 (32.078)	mem 43.566
Train: [2][528/750]	BT 0.083 (1.169)	DT 0.014 (1.038)	loss 9.911 (9.911)	gnorm 591644.375 (591644.375)	prob 2.364 (2.3642)	GS 33.391 (33.391)	mem 43.648
Train: [2][529/750]	BT 0.125 (1.167)	DT 0.006 (1.036)	loss 9.960 (9.960)	gnorm 604593.250 (604593.250)	prob 3.162 (3.1620)	GS 36.094 (36.094)	mem 43.567
Train: [2][530/750]	BT 0.210 (1.166)	DT 0.002 (1.034)	loss 9.987 (9.987)	gnorm 571976.250 (571976.250)	prob 2.699 (2.6994)	GS 30.125 (30.125)	mem 43.568
Train: [2][531/750]	BT 0.258 (1.164)	DT 0.002 (1.032)	loss 9.652 (9.652)	gnorm 558953.062 (558953.062)	prob 2.666 (2.6655)	GS 30.484 (30.484)	mem 43.567
Train: [2][532/750]	BT 7.240 (1.175)	DT 7.135 (1.043)	loss 10.389 (10.389)	gnorm 591142.375 (591142.375)	prob 1.659 (1.6591)	GS 32.812 (32.812)	mem 43.536
Train: [2][533/750]	BT 0.090 (1.173)	DT 0.006 (1.042)	loss 10.468 (10.468)	gnorm 644775.438 (644775.438)	prob 2.440 (2.4399)	GS 33.234 (33.234)	mem 43.537
Train: [2][534/750]	BT 0.090 (1.171)	DT 0.003 (1.040)	loss 10.199 (10.199)	gnorm 632088.188 (632088.188)	prob 2.091 (2.0909)	GS 34.312 (34.312)	mem 43.537
Train: [2][535/750]	BT 0.142 (1.169)	DT 0.011 (1.038)	loss 10.242 (10.242)	gnorm 605592.562 (605592.562)	prob 1.906 (1.9064)	GS 26.203 (26.203)	mem 43.554
Train: [2][536/750]	BT 5.562 (1.177)	DT 5.353 (1.046)	loss 9.646 (9.646)	gnorm 608255.438 (608255.438)	prob 2.066 (2.0662)	GS 34.734 (34.734)	mem 43.605
Train: [2][537/750]	BT 0.100 (1.175)	DT 0.012 (1.044)	loss 10.268 (10.268)	gnorm 607482.688 (607482.688)	prob 1.585 (1.5855)	GS 30.062 (30.062)	mem 43.607
Train: [2][538/750]	BT 0.094 (1.173)	DT 0.001 (1.042)	loss 9.935 (9.935)	gnorm 561363.250 (561363.250)	prob 1.620 (1.6199)	GS 34.766 (34.766)	mem 43.607
Train: [2][539/750]	BT 0.083 (1.171)	DT 0.003 (1.040)	loss 10.777 (10.777)	gnorm 658518.812 (658518.812)	prob 0.497 (0.4975)	GS 29.031 (29.031)	mem 43.633
Train: [2][540/750]	BT 0.141 (1.170)	DT 0.003 (1.038)	loss 10.139 (10.139)	gnorm 630560.812 (630560.812)	prob 1.094 (1.0944)	GS 37.500 (37.500)	mem 43.673
Train: [2][541/750]	BT 0.165 (1.168)	DT 0.002 (1.036)	loss 9.480 (9.480)	gnorm 634667.812 (634667.812)	prob 2.105 (2.1054)	GS 29.875 (29.875)	mem 43.654
Train: [2][542/750]	BT 0.181 (1.166)	DT 0.002 (1.034)	loss 9.811 (9.811)	gnorm 567300.375 (567300.375)	prob 2.225 (2.2248)	GS 32.922 (32.922)	mem 43.727
Train: [2][543/750]	BT 0.143 (1.164)	DT 0.002 (1.032)	loss 9.900 (9.900)	gnorm 592154.375 (592154.375)	prob 1.924 (1.9236)	GS 30.438 (30.438)	mem 43.616
Train: [2][544/750]	BT 7.842 (1.176)	DT 7.706 (1.045)	loss 9.879 (9.879)	gnorm 678892.000 (678892.000)	prob 1.617 (1.6172)	GS 39.219 (39.219)	mem 43.572
Train: [2][545/750]	BT 0.149 (1.174)	DT 0.009 (1.043)	loss 9.987 (9.987)	gnorm 588793.375 (588793.375)	prob 1.928 (1.9283)	GS 30.359 (30.359)	mem 43.573
Train: [2][546/750]	BT 0.172 (1.173)	DT 0.013 (1.041)	loss 9.704 (9.704)	gnorm 549229.312 (549229.312)	prob 2.038 (2.0382)	GS 30.781 (30.781)	mem 43.573
Train: [2][547/750]	BT 0.141 (1.171)	DT 0.002 (1.039)	loss 10.159 (10.159)	gnorm 583491.750 (583491.750)	prob 1.330 (1.3304)	GS 28.625 (28.625)	mem 43.574
Train: [2][548/750]	BT 3.130 (1.174)	DT 3.037 (1.042)	loss 10.579 (10.579)	gnorm 618149.938 (618149.938)	prob 0.801 (0.8009)	GS 29.906 (29.906)	mem 43.595
Train: [2][549/750]	BT 0.180 (1.172)	DT 0.002 (1.041)	loss 10.505 (10.505)	gnorm 606918.375 (606918.375)	prob 1.165 (1.1650)	GS 27.453 (27.453)	mem 43.609
Train: [2][550/750]	BT 0.164 (1.171)	DT 0.002 (1.039)	loss 10.742 (10.742)	gnorm 597603.875 (597603.875)	prob 0.124 (0.1245)	GS 32.109 (32.109)	mem 43.676
Train: [2][551/750]	BT 0.206 (1.169)	DT 0.007 (1.037)	loss 9.967 (9.967)	gnorm 641984.500 (641984.500)	prob 1.775 (1.7746)	GS 31.156 (31.156)	mem 43.604
Train: [2][552/750]	BT 0.156 (1.167)	DT 0.002 (1.035)	loss 9.902 (9.902)	gnorm 614816.125 (614816.125)	prob 1.028 (1.0275)	GS 40.359 (40.359)	mem 43.606
Train: [2][553/750]	BT 0.152 (1.165)	DT 0.002 (1.033)	loss 9.932 (9.932)	gnorm 676016.125 (676016.125)	prob 1.647 (1.6465)	GS 28.000 (28.000)	mem 43.604
Train: [2][554/750]	BT 0.090 (1.163)	DT 0.003 (1.031)	loss 10.526 (10.526)	gnorm 628740.500 (628740.500)	prob 0.327 (0.3272)	GS 32.578 (32.578)	mem 43.603
Train: [2][555/750]	BT 0.082 (1.161)	DT 0.002 (1.029)	loss 10.104 (10.104)	gnorm 578403.562 (578403.562)	prob 0.736 (0.7358)	GS 34.250 (34.250)	mem 43.605
Train: [2][556/750]	BT 4.884 (1.168)	DT 4.703 (1.036)	loss 9.552 (9.552)	gnorm 588154.562 (588154.562)	prob 1.636 (1.6361)	GS 32.094 (32.094)	mem 43.652
Train: [2][557/750]	BT 0.243 (1.166)	DT 0.006 (1.034)	loss 10.536 (10.536)	gnorm 597120.000 (597120.000)	prob 0.695 (0.6953)	GS 34.125 (34.125)	mem 43.759
Train: [2][558/750]	BT 0.202 (1.165)	DT 0.002 (1.032)	loss 9.674 (9.674)	gnorm 598067.250 (598067.250)	prob 2.155 (2.1549)	GS 34.938 (34.938)	mem 43.761
Train: [2][559/750]	BT 0.111 (1.163)	DT 0.002 (1.030)	loss 9.877 (9.877)	gnorm 637942.875 (637942.875)	prob 1.303 (1.3034)	GS 31.750 (31.750)	mem 43.762
Train: [2][560/750]	BT 6.949 (1.173)	DT 6.740 (1.041)	loss 10.125 (10.125)	gnorm 590210.625 (590210.625)	prob 1.545 (1.5447)	GS 35.312 (35.312)	mem 43.563
Train: [2][561/750]	BT 0.093 (1.171)	DT 0.002 (1.039)	loss 9.725 (9.725)	gnorm 611849.375 (611849.375)	prob 1.595 (1.5954)	GS 30.172 (30.172)	mem 43.563
Train: [2][562/750]	BT 0.086 (1.169)	DT 0.003 (1.037)	loss 9.450 (9.450)	gnorm 612829.438 (612829.438)	prob 1.802 (1.8017)	GS 33.344 (33.344)	mem 43.563
Train: [2][563/750]	BT 0.159 (1.167)	DT 0.003 (1.035)	loss 10.519 (10.519)	gnorm 633813.938 (633813.938)	prob 1.154 (1.1535)	GS 31.078 (31.078)	mem 43.563
Train: [2][564/750]	BT 0.108 (1.165)	DT 0.003 (1.033)	loss 10.075 (10.075)	gnorm 553061.875 (553061.875)	prob 0.936 (0.9359)	GS 37.156 (37.156)	mem 43.564
Train: [2][565/750]	BT 0.156 (1.164)	DT 0.001 (1.031)	loss 10.423 (10.423)	gnorm 582914.938 (582914.938)	prob 1.336 (1.3364)	GS 29.719 (29.719)	mem 43.565
Train: [2][566/750]	BT 0.236 (1.162)	DT 0.015 (1.030)	loss 10.453 (10.453)	gnorm 652806.812 (652806.812)	prob 1.165 (1.1652)	GS 33.203 (33.203)	mem 43.566
Train: [2][567/750]	BT 0.113 (1.160)	DT 0.002 (1.028)	loss 10.057 (10.057)	gnorm 641879.812 (641879.812)	prob 1.586 (1.5857)	GS 27.688 (27.688)	mem 43.566
Train: [2][568/750]	BT 5.041 (1.167)	DT 4.949 (1.035)	loss 10.984 (10.984)	gnorm 613652.312 (613652.312)	prob 0.052 (0.0522)	GS 34.281 (34.281)	mem 43.560
Train: [2][569/750]	BT 0.141 (1.165)	DT 0.025 (1.033)	loss 10.100 (10.100)	gnorm 614307.750 (614307.750)	prob 1.388 (1.3876)	GS 33.500 (33.500)	mem 43.570
Train: [2][570/750]	BT 0.145 (1.163)	DT 0.003 (1.031)	loss 10.321 (10.321)	gnorm 588062.562 (588062.562)	prob 0.363 (0.3627)	GS 36.219 (36.219)	mem 43.611
Train: [2][571/750]	BT 0.177 (1.162)	DT 0.004 (1.029)	loss 9.945 (9.945)	gnorm 644511.750 (644511.750)	prob 0.651 (0.6514)	GS 31.594 (31.594)	mem 43.612
Train: [2][572/750]	BT 7.869 (1.173)	DT 7.752 (1.041)	loss 9.744 (9.744)	gnorm 601253.250 (601253.250)	prob 1.393 (1.3930)	GS 35.359 (35.359)	mem 43.619
Train: [2][573/750]	BT 0.076 (1.172)	DT 0.002 (1.039)	loss 10.939 (10.939)	gnorm 584132.000 (584132.000)	prob 0.600 (0.6003)	GS 29.750 (29.750)	mem 43.620
Train: [2][574/750]	BT 0.197 (1.170)	DT 0.002 (1.037)	loss 10.487 (10.487)	gnorm 618066.625 (618066.625)	prob 0.654 (0.6537)	GS 35.312 (35.312)	mem 43.630
Train: [2][575/750]	BT 0.111 (1.168)	DT 0.013 (1.036)	loss 9.958 (9.958)	gnorm 629540.688 (629540.688)	prob 1.145 (1.1447)	GS 29.562 (29.562)	mem 43.630
Train: [2][576/750]	BT 0.092 (1.166)	DT 0.003 (1.034)	loss 10.219 (10.219)	gnorm 637564.438 (637564.438)	prob 0.531 (0.5307)	GS 35.641 (35.641)	mem 43.630
Train: [2][577/750]	BT 0.169 (1.164)	DT 0.002 (1.032)	loss 10.536 (10.536)	gnorm 610017.000 (610017.000)	prob 0.303 (0.3035)	GS 33.562 (33.562)	mem 43.630
Train: [2][578/750]	BT 0.235 (1.163)	DT 0.007 (1.030)	loss 10.041 (10.041)	gnorm 616624.250 (616624.250)	prob 1.228 (1.2276)	GS 31.656 (31.656)	mem 43.632
Train: [2][579/750]	BT 0.109 (1.161)	DT 0.010 (1.029)	loss 9.791 (9.791)	gnorm 632872.250 (632872.250)	prob 1.577 (1.5771)	GS 32.344 (32.344)	mem 43.632
Train: [2][580/750]	BT 8.011 (1.173)	DT 7.916 (1.040)	loss 10.395 (10.395)	gnorm 730104.250 (730104.250)	prob 0.635 (0.6348)	GS 36.578 (36.578)	mem 43.640
Train: [2][581/750]	BT 0.136 (1.171)	DT 0.004 (1.039)	loss 10.741 (10.741)	gnorm 714786.250 (714786.250)	prob 1.517 (1.5174)	GS 32.344 (32.344)	mem 43.640
Train: [2][582/750]	BT 0.130 (1.169)	DT 0.004 (1.037)	loss 10.443 (10.443)	gnorm 677983.250 (677983.250)	prob 1.406 (1.4059)	GS 31.594 (31.594)	mem 43.659
Train: [2][583/750]	BT 0.137 (1.167)	DT 0.003 (1.035)	loss 10.548 (10.548)	gnorm 598945.500 (598945.500)	prob 1.357 (1.3574)	GS 33.531 (33.531)	mem 43.688
Train: [2][584/750]	BT 1.655 (1.168)	DT 1.510 (1.036)	loss 9.840 (9.840)	gnorm 646581.062 (646581.062)	prob 2.145 (2.1452)	GS 30.250 (30.250)	mem 43.639
Train: [2][585/750]	BT 0.117 (1.166)	DT 0.002 (1.034)	loss 9.834 (9.834)	gnorm 633093.312 (633093.312)	prob 1.626 (1.6257)	GS 27.625 (27.625)	mem 43.640
Train: [2][586/750]	BT 0.180 (1.165)	DT 0.006 (1.032)	loss 9.785 (9.785)	gnorm 587143.062 (587143.062)	prob 1.916 (1.9157)	GS 28.609 (28.609)	mem 43.640
Train: [2][587/750]	BT 0.119 (1.163)	DT 0.006 (1.031)	loss 10.569 (10.569)	gnorm 572072.375 (572072.375)	prob 1.167 (1.1672)	GS 33.797 (33.797)	mem 43.639
Train: [2][588/750]	BT 0.182 (1.161)	DT 0.003 (1.029)	loss 10.404 (10.404)	gnorm 586629.625 (586629.625)	prob 1.398 (1.3979)	GS 33.938 (33.938)	mem 43.640
Train: [2][589/750]	BT 0.160 (1.160)	DT 0.004 (1.027)	loss 9.998 (9.998)	gnorm 568235.812 (568235.812)	prob 1.810 (1.8097)	GS 32.000 (32.000)	mem 43.665
Train: [2][590/750]	BT 0.125 (1.158)	DT 0.012 (1.025)	loss 10.450 (10.450)	gnorm 573775.500 (573775.500)	prob 1.065 (1.0651)	GS 34.656 (34.656)	mem 43.665
Train: [2][591/750]	BT 0.167 (1.156)	DT 0.004 (1.024)	loss 10.597 (10.597)	gnorm 554102.188 (554102.188)	prob 1.488 (1.4880)	GS 33.562 (33.562)	mem 43.666
Train: [2][592/750]	BT 10.489 (1.172)	DT 10.389 (1.040)	loss 9.882 (9.882)	gnorm 603544.125 (603544.125)	prob 2.303 (2.3026)	GS 35.844 (35.844)	mem 43.677
Train: [2][593/750]	BT 0.115 (1.170)	DT 0.003 (1.038)	loss 10.502 (10.502)	gnorm 567487.250 (567487.250)	prob 2.151 (2.1512)	GS 33.391 (33.391)	mem 43.676
Train: [2][594/750]	BT 0.104 (1.168)	DT 0.003 (1.036)	loss 9.938 (9.938)	gnorm 580575.375 (580575.375)	prob 2.373 (2.3728)	GS 33.453 (33.453)	mem 43.676
Train: [2][595/750]	BT 0.143 (1.167)	DT 0.002 (1.034)	loss 10.270 (10.270)	gnorm 578196.062 (578196.062)	prob 2.238 (2.2380)	GS 29.750 (29.750)	mem 43.676
Train: [2][596/750]	BT 2.951 (1.170)	DT 2.869 (1.037)	loss 9.954 (9.954)	gnorm 600380.812 (600380.812)	prob 1.858 (1.8584)	GS 31.625 (31.625)	mem 43.687
Train: [2][597/750]	BT 0.138 (1.168)	DT 0.002 (1.036)	loss 10.623 (10.623)	gnorm 659072.312 (659072.312)	prob 1.441 (1.4407)	GS 32.031 (32.031)	mem 43.690
Train: [2][598/750]	BT 0.237 (1.166)	DT 0.002 (1.034)	loss 10.264 (10.264)	gnorm 603335.750 (603335.750)	prob 1.450 (1.4497)	GS 39.031 (39.031)	mem 43.689
Train: [2][599/750]	BT 0.249 (1.165)	DT 0.005 (1.032)	loss 10.000 (10.000)	gnorm 620418.875 (620418.875)	prob 2.234 (2.2343)	GS 32.531 (32.531)	mem 43.756
Train: [2][600/750]	BT 0.145 (1.163)	DT 0.005 (1.031)	loss 10.643 (10.643)	gnorm 606298.125 (606298.125)	prob 1.526 (1.5263)	GS 35.578 (35.578)	mem 43.692
Train: [2][601/750]	BT 0.141 (1.161)	DT 0.005 (1.029)	loss 10.314 (10.314)	gnorm 600636.188 (600636.188)	prob 2.187 (2.1868)	GS 31.656 (31.656)	mem 43.772
Train: [2][602/750]	BT 0.151 (1.160)	DT 0.003 (1.027)	loss 10.644 (10.644)	gnorm 615624.625 (615624.625)	prob 1.599 (1.5988)	GS 40.375 (40.375)	mem 43.691
Train: [2][603/750]	BT 0.130 (1.158)	DT 0.003 (1.025)	loss 10.365 (10.365)	gnorm 579436.312 (579436.312)	prob 1.512 (1.5124)	GS 34.172 (34.172)	mem 43.691
Train: [2][604/750]	BT 10.948 (1.174)	DT 10.844 (1.042)	loss 9.746 (9.746)	gnorm 542858.000 (542858.000)	prob 2.220 (2.2200)	GS 34.750 (34.750)	mem 43.691
Train: [2][605/750]	BT 0.090 (1.172)	DT 0.001 (1.040)	loss 10.110 (10.110)	gnorm 608477.375 (608477.375)	prob 2.229 (2.2288)	GS 33.672 (33.672)	mem 43.690
Train: [2][606/750]	BT 0.073 (1.171)	DT 0.002 (1.038)	loss 10.122 (10.122)	gnorm 591231.438 (591231.438)	prob 2.387 (2.3869)	GS 32.906 (32.906)	mem 43.690
Train: [2][607/750]	BT 0.175 (1.169)	DT 0.002 (1.037)	loss 10.575 (10.575)	gnorm 592314.625 (592314.625)	prob 2.127 (2.1269)	GS 29.031 (29.031)	mem 43.690
Train: [2][608/750]	BT 0.795 (1.168)	DT 0.702 (1.036)	loss 10.331 (10.331)	gnorm 621876.125 (621876.125)	prob 1.770 (1.7695)	GS 29.797 (29.797)	mem 43.690
Train: [2][609/750]	BT 0.145 (1.167)	DT 0.001 (1.034)	loss 10.250 (10.250)	gnorm 558849.438 (558849.438)	prob 2.011 (2.0112)	GS 33.234 (33.234)	mem 43.690
Train: [2][610/750]	BT 0.072 (1.165)	DT 0.002 (1.033)	loss 10.554 (10.554)	gnorm 586622.625 (586622.625)	prob 1.629 (1.6295)	GS 36.172 (36.172)	mem 43.740
Train: [2][611/750]	BT 0.097 (1.163)	DT 0.003 (1.031)	loss 10.361 (10.361)	gnorm 577499.688 (577499.688)	prob 2.220 (2.2198)	GS 28.984 (28.984)	mem 43.718
Train: [2][612/750]	BT 0.118 (1.161)	DT 0.002 (1.029)	loss 10.124 (10.124)	gnorm 582886.812 (582886.812)	prob 2.670 (2.6697)	GS 33.562 (33.562)	mem 43.951
Train: [2][613/750]	BT 0.146 (1.160)	DT 0.008 (1.028)	loss 10.125 (10.125)	gnorm 579537.625 (579537.625)	prob 1.646 (1.6456)	GS 31.016 (31.016)	mem 43.879
Train: [2][614/750]	BT 0.103 (1.158)	DT 0.002 (1.026)	loss 9.928 (9.928)	gnorm 595574.812 (595574.812)	prob 1.992 (1.9919)	GS 35.359 (35.359)	mem 43.953
Train: [2][615/750]	BT 0.137 (1.156)	DT 0.002 (1.024)	loss 10.064 (10.064)	gnorm 562614.000 (562614.000)	prob 1.827 (1.8268)	GS 37.641 (37.641)	mem 43.814
Train: [2][616/750]	BT 7.057 (1.166)	DT 6.886 (1.034)	loss 9.503 (9.503)	gnorm 562310.250 (562310.250)	prob 2.860 (2.8598)	GS 30.641 (30.641)	mem 43.770
Train: [2][617/750]	BT 0.080 (1.164)	DT 0.003 (1.032)	loss 10.288 (10.288)	gnorm 588486.875 (588486.875)	prob 2.078 (2.0778)	GS 35.734 (35.734)	mem 43.703
Train: [2][618/750]	BT 0.086 (1.163)	DT 0.003 (1.030)	loss 9.856 (9.856)	gnorm 555059.312 (555059.312)	prob 2.823 (2.8229)	GS 31.812 (31.812)	mem 43.723
Train: [2][619/750]	BT 0.168 (1.161)	DT 0.001 (1.029)	loss 10.387 (10.387)	gnorm 612755.750 (612755.750)	prob 1.947 (1.9475)	GS 31.625 (31.625)	mem 43.802
Train: [2][620/750]	BT 3.019 (1.164)	DT 2.926 (1.032)	loss 10.247 (10.247)	gnorm 638204.812 (638204.812)	prob 1.475 (1.4752)	GS 35.000 (35.000)	mem 43.664
Train: [2][621/750]	BT 0.143 (1.162)	DT 0.002 (1.030)	loss 9.941 (9.941)	gnorm 602520.688 (602520.688)	prob 1.957 (1.9567)	GS 28.547 (28.547)	mem 43.665
Train: [2][622/750]	BT 0.094 (1.161)	DT 0.002 (1.028)	loss 10.593 (10.593)	gnorm 580394.375 (580394.375)	prob 1.153 (1.1529)	GS 33.844 (33.844)	mem 43.666
Train: [2][623/750]	BT 0.124 (1.159)	DT 0.002 (1.027)	loss 10.027 (10.027)	gnorm 586873.062 (586873.062)	prob 1.849 (1.8494)	GS 32.438 (32.438)	mem 43.684
Train: [2][624/750]	BT 0.122 (1.157)	DT 0.002 (1.025)	loss 9.866 (9.866)	gnorm 582660.062 (582660.062)	prob 1.561 (1.5612)	GS 31.922 (31.922)	mem 43.718
Train: [2][625/750]	BT 0.181 (1.156)	DT 0.012 (1.024)	loss 10.261 (10.261)	gnorm 618369.312 (618369.312)	prob 1.793 (1.7928)	GS 34.000 (34.000)	mem 43.672
Train: [2][626/750]	BT 0.097 (1.154)	DT 0.002 (1.022)	loss 10.193 (10.193)	gnorm 577832.188 (577832.188)	prob 1.718 (1.7181)	GS 33.469 (33.469)	mem 43.715
Train: [2][627/750]	BT 0.117 (1.152)	DT 0.002 (1.020)	loss 10.298 (10.298)	gnorm 607037.750 (607037.750)	prob 1.698 (1.6984)	GS 30.094 (30.094)	mem 43.813
Train: [2][628/750]	BT 9.207 (1.165)	DT 9.072 (1.033)	loss 10.202 (10.202)	gnorm 562508.812 (562508.812)	prob 1.720 (1.7204)	GS 33.766 (33.766)	mem 43.689
Train: [2][629/750]	BT 0.137 (1.163)	DT 0.002 (1.031)	loss 9.962 (9.962)	gnorm 584516.312 (584516.312)	prob 2.195 (2.1945)	GS 30.531 (30.531)	mem 43.728
Train: [2][630/750]	BT 0.187 (1.162)	DT 0.034 (1.030)	loss 9.860 (9.860)	gnorm 563981.375 (563981.375)	prob 2.415 (2.4155)	GS 31.438 (31.438)	mem 43.689
Train: [2][631/750]	BT 0.143 (1.160)	DT 0.003 (1.028)	loss 9.904 (9.904)	gnorm 562863.125 (562863.125)	prob 2.052 (2.0516)	GS 32.656 (32.656)	mem 43.699
Train: [2][632/750]	BT 5.639 (1.167)	DT 5.553 (1.035)	loss 10.440 (10.440)	gnorm 601990.438 (601990.438)	prob 0.835 (0.8349)	GS 34.766 (34.766)	mem 43.702
Train: [2][633/750]	BT 0.125 (1.166)	DT 0.006 (1.034)	loss 9.830 (9.830)	gnorm 564796.000 (564796.000)	prob 1.525 (1.5248)	GS 36.047 (36.047)	mem 43.721
Train: [2][634/750]	BT 0.102 (1.164)	DT 0.002 (1.032)	loss 9.835 (9.835)	gnorm 609915.062 (609915.062)	prob 1.509 (1.5094)	GS 35.688 (35.688)	mem 43.813
Train: [2][635/750]	BT 0.127 (1.162)	DT 0.007 (1.031)	loss 10.341 (10.341)	gnorm 624545.875 (624545.875)	prob 0.554 (0.5541)	GS 29.312 (29.312)	mem 43.767
Train: [2][636/750]	BT 0.112 (1.161)	DT 0.002 (1.029)	loss 10.602 (10.602)	gnorm 637695.625 (637695.625)	prob 0.003 (0.0031)	GS 35.016 (35.016)	mem 43.737
Train: [2][637/750]	BT 0.237 (1.159)	DT 0.002 (1.027)	loss 10.975 (10.975)	gnorm 588885.625 (588885.625)	prob 0.492 (0.4925)	GS 27.984 (27.984)	mem 43.700
Train: [2][638/750]	BT 0.185 (1.158)	DT 0.004 (1.026)	loss 10.476 (10.476)	gnorm 662083.688 (662083.688)	prob 0.769 (0.7685)	GS 33.562 (33.562)	mem 43.701
Train: [2][639/750]	BT 0.133 (1.156)	DT 0.017 (1.024)	loss 10.126 (10.126)	gnorm 651458.125 (651458.125)	prob 1.023 (1.0235)	GS 32.344 (32.344)	mem 43.706
Train: [2][640/750]	BT 7.493 (1.166)	DT 7.399 (1.034)	loss 9.771 (9.771)	gnorm 546957.250 (546957.250)	prob 2.143 (2.1435)	GS 36.547 (36.547)	mem 43.723
Train: [2][641/750]	BT 0.136 (1.165)	DT 0.002 (1.033)	loss 10.085 (10.085)	gnorm 594649.812 (594649.812)	prob 1.552 (1.5524)	GS 33.438 (33.438)	mem 43.723
Train: [2][642/750]	BT 0.165 (1.163)	DT 0.011 (1.031)	loss 10.210 (10.210)	gnorm 556153.812 (556153.812)	prob 0.751 (0.7508)	GS 34.000 (34.000)	mem 43.724
Train: [2][643/750]	BT 0.163 (1.161)	DT 0.003 (1.029)	loss 10.048 (10.048)	gnorm 563326.625 (563326.625)	prob 2.069 (2.0687)	GS 37.094 (37.094)	mem 43.724
Train: [2][644/750]	BT 4.979 (1.167)	DT 4.828 (1.035)	loss 10.522 (10.522)	gnorm 536819.500 (536819.500)	prob 1.189 (1.1889)	GS 32.328 (32.328)	mem 43.685
Train: [2][645/750]	BT 0.110 (1.166)	DT 0.003 (1.034)	loss 10.004 (10.004)	gnorm 568846.188 (568846.188)	prob 1.979 (1.9794)	GS 30.234 (30.234)	mem 43.685
Train: [2][646/750]	BT 0.078 (1.164)	DT 0.001 (1.032)	loss 10.918 (10.918)	gnorm 591056.875 (591056.875)	prob 1.220 (1.2197)	GS 34.156 (34.156)	mem 43.685
Train: [2][647/750]	BT 0.110 (1.162)	DT 0.002 (1.030)	loss 9.950 (9.950)	gnorm 564946.312 (564946.312)	prob 2.500 (2.4999)	GS 32.188 (32.188)	mem 43.685
Train: [2][648/750]	BT 0.164 (1.161)	DT 0.003 (1.029)	loss 10.488 (10.488)	gnorm 586429.500 (586429.500)	prob 1.962 (1.9625)	GS 36.750 (36.750)	mem 43.732
Train: [2][649/750]	BT 0.147 (1.159)	DT 0.011 (1.027)	loss 10.277 (10.277)	gnorm 555680.750 (555680.750)	prob 1.912 (1.9120)	GS 27.422 (27.422)	mem 43.687
Train: [2][650/750]	BT 0.193 (1.158)	DT 0.008 (1.026)	loss 9.620 (9.620)	gnorm 516906.125 (516906.125)	prob 2.686 (2.6862)	GS 33.812 (33.812)	mem 43.729
Train: [2][651/750]	BT 0.096 (1.156)	DT 0.001 (1.024)	loss 10.348 (10.348)	gnorm 557096.500 (557096.500)	prob 1.566 (1.5656)	GS 24.375 (24.375)	mem 43.729
Train: [2][652/750]	BT 5.672 (1.163)	DT 5.595 (1.031)	loss 9.740 (9.740)	gnorm 530479.562 (530479.562)	prob 1.686 (1.6858)	GS 31.844 (31.844)	mem 43.637
Train: [2][653/750]	BT 0.216 (1.162)	DT 0.002 (1.030)	loss 10.651 (10.651)	gnorm 529734.062 (529734.062)	prob 1.668 (1.6681)	GS 32.438 (32.438)	mem 43.648
Train: [2][654/750]	BT 0.182 (1.160)	DT 0.005 (1.028)	loss 10.439 (10.439)	gnorm 551831.375 (551831.375)	prob 1.361 (1.3611)	GS 34.250 (34.250)	mem 43.752
Train: [2][655/750]	BT 0.113 (1.159)	DT 0.002 (1.026)	loss 9.804 (9.804)	gnorm 585552.000 (585552.000)	prob 1.498 (1.4980)	GS 30.547 (30.547)	mem 43.697
Train: [2][656/750]	BT 8.082 (1.169)	DT 8.004 (1.037)	loss 10.446 (10.446)	gnorm 555511.500 (555511.500)	prob 0.846 (0.8456)	GS 34.281 (34.281)	mem 43.692
Train: [2][657/750]	BT 0.084 (1.167)	DT 0.002 (1.035)	loss 10.213 (10.213)	gnorm 564744.938 (564744.938)	prob 1.042 (1.0423)	GS 29.078 (29.078)	mem 43.692
Train: [2][658/750]	BT 0.083 (1.166)	DT 0.001 (1.034)	loss 9.818 (9.818)	gnorm 604324.625 (604324.625)	prob 2.365 (2.3650)	GS 35.344 (35.344)	mem 43.692
Train: [2][659/750]	BT 0.145 (1.164)	DT 0.002 (1.032)	loss 10.011 (10.011)	gnorm 646482.375 (646482.375)	prob 2.014 (2.0136)	GS 30.469 (30.469)	mem 43.692
Train: [2][660/750]	BT 0.183 (1.163)	DT 0.010 (1.031)	loss 10.209 (10.209)	gnorm 579790.750 (579790.750)	prob 1.607 (1.6069)	GS 34.234 (34.234)	mem 43.692
Train: [2][661/750]	BT 0.172 (1.161)	DT 0.018 (1.029)	loss 10.043 (10.043)	gnorm 555497.625 (555497.625)	prob 2.282 (2.2819)	GS 32.703 (32.703)	mem 43.760
Train: [2][662/750]	BT 0.131 (1.160)	DT 0.003 (1.028)	loss 9.558 (9.558)	gnorm 631549.312 (631549.312)	prob 2.790 (2.7904)	GS 35.297 (35.297)	mem 43.785
Train: [2][663/750]	BT 0.122 (1.158)	DT 0.002 (1.026)	loss 10.192 (10.192)	gnorm 605353.188 (605353.188)	prob 2.122 (2.1220)	GS 29.656 (29.656)	mem 43.726
Train: [2][664/750]	BT 1.962 (1.159)	DT 1.882 (1.027)	loss 10.210 (10.210)	gnorm 593409.438 (593409.438)	prob 1.250 (1.2499)	GS 34.094 (34.094)	mem 43.679
Train: [2][665/750]	BT 0.175 (1.158)	DT 0.001 (1.026)	loss 10.115 (10.115)	gnorm 577729.375 (577729.375)	prob 2.112 (2.1123)	GS 37.141 (37.141)	mem 43.640
Train: [2][666/750]	BT 0.132 (1.156)	DT 0.002 (1.024)	loss 9.438 (9.438)	gnorm 574124.438 (574124.438)	prob 3.083 (3.0827)	GS 34.188 (34.188)	mem 43.711
Train: [2][667/750]	BT 0.098 (1.155)	DT 0.002 (1.023)	loss 10.259 (10.259)	gnorm 617162.250 (617162.250)	prob 2.057 (2.0574)	GS 29.969 (29.969)	mem 43.683
Train: [2][668/750]	BT 7.010 (1.164)	DT 6.856 (1.032)	loss 10.035 (10.035)	gnorm 559802.375 (559802.375)	prob 1.931 (1.9311)	GS 33.891 (33.891)	mem 43.678
Train: [2][669/750]	BT 0.152 (1.162)	DT 0.011 (1.030)	loss 10.207 (10.207)	gnorm 578373.750 (578373.750)	prob 2.405 (2.4053)	GS 29.547 (29.547)	mem 43.678
Train: [2][670/750]	BT 0.251 (1.161)	DT 0.003 (1.029)	loss 10.020 (10.020)	gnorm 592985.125 (592985.125)	prob 2.029 (2.0290)	GS 30.844 (30.844)	mem 43.677
Train: [2][671/750]	BT 0.132 (1.159)	DT 0.017 (1.027)	loss 10.239 (10.239)	gnorm 546408.312 (546408.312)	prob 2.004 (2.0044)	GS 34.406 (34.406)	mem 43.671
Train: [2][672/750]	BT 0.161 (1.158)	DT 0.003 (1.025)	loss 10.443 (10.443)	gnorm 576415.562 (576415.562)	prob 1.818 (1.8176)	GS 33.922 (33.922)	mem 43.615
Train: [2][673/750]	BT 0.274 (1.156)	DT 0.002 (1.024)	loss 9.986 (9.986)	gnorm 596444.938 (596444.938)	prob 2.150 (2.1495)	GS 31.750 (31.750)	mem 43.615
Train: [2][674/750]	BT 0.169 (1.155)	DT 0.009 (1.022)	loss 9.933 (9.933)	gnorm 590009.625 (590009.625)	prob 1.532 (1.5321)	GS 36.234 (36.234)	mem 43.638
Train: [2][675/750]	BT 0.198 (1.153)	DT 0.012 (1.021)	loss 9.953 (9.953)	gnorm 587344.000 (587344.000)	prob 2.109 (2.1087)	GS 34.125 (34.125)	mem 43.648
Train: [2][676/750]	BT 6.553 (1.161)	DT 6.400 (1.029)	loss 9.931 (9.931)	gnorm 571925.688 (571925.688)	prob 2.079 (2.0788)	GS 33.984 (33.984)	mem 43.584
Train: [2][677/750]	BT 0.075 (1.160)	DT 0.001 (1.027)	loss 10.486 (10.486)	gnorm 563537.062 (563537.062)	prob 1.684 (1.6836)	GS 32.516 (32.516)	mem 43.594
Train: [2][678/750]	BT 0.087 (1.158)	DT 0.002 (1.026)	loss 10.050 (10.050)	gnorm 525574.188 (525574.188)	prob 2.075 (2.0747)	GS 33.938 (33.938)	mem 43.594
Train: [2][679/750]	BT 0.099 (1.157)	DT 0.002 (1.024)	loss 10.555 (10.555)	gnorm 610144.062 (610144.062)	prob 2.059 (2.0590)	GS 31.453 (31.453)	mem 43.594
Train: [2][680/750]	BT 7.415 (1.166)	DT 7.205 (1.033)	loss 10.545 (10.545)	gnorm 587520.938 (587520.938)	prob 1.595 (1.5955)	GS 38.609 (38.609)	mem 43.632
Train: [2][681/750]	BT 0.142 (1.164)	DT 0.025 (1.032)	loss 10.430 (10.430)	gnorm 556109.750 (556109.750)	prob 1.699 (1.6986)	GS 30.078 (30.078)	mem 43.633
Train: [2][682/750]	BT 0.155 (1.163)	DT 0.002 (1.030)	loss 10.318 (10.318)	gnorm 617182.000 (617182.000)	prob 2.241 (2.2413)	GS 35.703 (35.703)	mem 43.632
Train: [2][683/750]	BT 0.128 (1.161)	DT 0.008 (1.029)	loss 9.997 (9.997)	gnorm 579431.125 (579431.125)	prob 2.732 (2.7325)	GS 34.547 (34.547)	mem 43.633
Train: [2][684/750]	BT 0.220 (1.160)	DT 0.002 (1.027)	loss 10.396 (10.396)	gnorm 573769.438 (573769.438)	prob 2.313 (2.3129)	GS 33.984 (33.984)	mem 43.632
Train: [2][685/750]	BT 0.379 (1.159)	DT 0.006 (1.026)	loss 9.524 (9.524)	gnorm 617723.125 (617723.125)	prob 3.163 (3.1628)	GS 34.141 (34.141)	mem 43.639
arpack error, retry= 0
Train: [2][686/750]	BT 0.251 (1.158)	DT 0.010 (1.025)	loss 10.172 (10.172)	gnorm 516822.906 (516822.906)	prob 1.594 (1.5935)	GS 35.844 (35.844)	mem 43.700
Train: [2][687/750]	BT 0.195 (1.156)	DT 0.014 (1.023)	loss 10.361 (10.361)	gnorm 564786.812 (564786.812)	prob 1.364 (1.3642)	GS 34.734 (34.734)	mem 43.633
Train: [2][688/750]	BT 3.883 (1.160)	DT 3.751 (1.027)	loss 10.127 (10.127)	gnorm 568576.312 (568576.312)	prob 2.053 (2.0532)	GS 32.656 (32.656)	mem 43.703
Train: [2][689/750]	BT 0.128 (1.159)	DT 0.002 (1.026)	loss 10.493 (10.493)	gnorm 610952.812 (610952.812)	prob 1.857 (1.8572)	GS 30.234 (30.234)	mem 43.721
Train: [2][690/750]	BT 0.272 (1.157)	DT 0.012 (1.024)	loss 10.591 (10.591)	gnorm 528751.812 (528751.812)	prob 0.962 (0.9616)	GS 33.594 (33.594)	mem 43.797
Train: [2][691/750]	BT 0.250 (1.156)	DT 0.024 (1.023)	loss 10.166 (10.166)	gnorm 593189.125 (593189.125)	prob 1.575 (1.5748)	GS 35.047 (35.047)	mem 43.748
Train: [2][692/750]	BT 5.737 (1.163)	DT 5.590 (1.029)	loss 10.527 (10.527)	gnorm 591573.312 (591573.312)	prob 1.295 (1.2955)	GS 29.781 (29.781)	mem 43.860
Train: [2][693/750]	BT 0.305 (1.161)	DT 0.018 (1.028)	loss 11.058 (11.058)	gnorm 687280.688 (687280.688)	prob 1.479 (1.4787)	GS 56.219 (56.219)	mem 43.667
Train: [2][694/750]	BT 0.185 (1.160)	DT 0.006 (1.026)	loss 9.549 (9.549)	gnorm 569415.875 (569415.875)	prob 2.398 (2.3983)	GS 28.656 (28.656)	mem 43.667
Train: [2][695/750]	BT 0.113 (1.158)	DT 0.005 (1.025)	loss 10.333 (10.333)	gnorm 561210.250 (561210.250)	prob 2.107 (2.1069)	GS 30.750 (30.750)	mem 43.656
Train: [2][696/750]	BT 0.092 (1.157)	DT 0.006 (1.023)	loss 10.484 (10.484)	gnorm 566013.250 (566013.250)	prob 1.925 (1.9248)	GS 36.281 (36.281)	mem 43.619
Train: [2][697/750]	BT 0.087 (1.155)	DT 0.002 (1.022)	loss 9.850 (9.850)	gnorm 572535.062 (572535.062)	prob 2.619 (2.6188)	GS 29.906 (29.906)	mem 43.646
Train: [2][698/750]	BT 0.126 (1.154)	DT 0.003 (1.020)	loss 10.592 (10.592)	gnorm 551934.250 (551934.250)	prob 1.936 (1.9360)	GS 29.375 (29.375)	mem 43.719
Train: [2][699/750]	BT 0.151 (1.152)	DT 0.005 (1.019)	loss 10.192 (10.192)	gnorm 587062.312 (587062.312)	prob 2.072 (2.0721)	GS 36.016 (36.016)	mem 43.601
Train: [2][700/750]	BT 6.299 (1.160)	DT 6.130 (1.026)	loss 10.591 (10.591)	gnorm 608176.562 (608176.562)	prob 1.788 (1.7880)	GS 31.406 (31.406)	mem 43.597
Train: [2][701/750]	BT 0.089 (1.158)	DT 0.004 (1.025)	loss 11.008 (11.008)	gnorm 615905.062 (615905.062)	prob 1.505 (1.5047)	GS 27.797 (27.797)	mem 43.665
Train: [2][702/750]	BT 0.092 (1.157)	DT 0.002 (1.023)	loss 9.877 (9.877)	gnorm 550443.125 (550443.125)	prob 2.865 (2.8653)	GS 34.703 (34.703)	mem 43.778
Train: [2][703/750]	BT 0.144 (1.155)	DT 0.013 (1.022)	loss 10.457 (10.457)	gnorm 581380.188 (581380.188)	prob 2.433 (2.4326)	GS 34.891 (34.891)	mem 43.812
Train: [2][704/750]	BT 7.016 (1.164)	DT 6.838 (1.030)	loss 9.996 (9.996)	gnorm 564410.250 (564410.250)	prob 2.758 (2.7579)	GS 35.688 (35.688)	mem 43.600
Train: [2][705/750]	BT 0.191 (1.162)	DT 0.007 (1.029)	loss 10.655 (10.655)	gnorm 593898.875 (593898.875)	prob 2.650 (2.6505)	GS 32.125 (32.125)	mem 43.601
Train: [2][706/750]	BT 0.158 (1.161)	DT 0.002 (1.027)	loss 9.890 (9.890)	gnorm 553204.938 (553204.938)	prob 2.829 (2.8291)	GS 31.359 (31.359)	mem 43.602
Train: [2][707/750]	BT 0.081 (1.159)	DT 0.002 (1.026)	loss 10.068 (10.068)	gnorm 536158.312 (536158.312)	prob 2.776 (2.7760)	GS 33.922 (33.922)	mem 43.619
Train: [2][708/750]	BT 0.227 (1.158)	DT 0.002 (1.024)	loss 10.228 (10.228)	gnorm 597028.312 (597028.312)	prob 2.367 (2.3670)	GS 35.047 (35.047)	mem 43.612
Train: [2][709/750]	BT 0.098 (1.157)	DT 0.008 (1.023)	loss 10.376 (10.376)	gnorm 531614.688 (531614.688)	prob 1.868 (1.8682)	GS 30.641 (30.641)	mem 43.552
Train: [2][710/750]	BT 0.149 (1.155)	DT 0.001 (1.021)	loss 10.045 (10.045)	gnorm 537547.625 (537547.625)	prob 1.807 (1.8072)	GS 36.453 (36.453)	mem 43.680
Train: [2][711/750]	BT 0.171 (1.154)	DT 0.005 (1.020)	loss 10.544 (10.544)	gnorm 556376.438 (556376.438)	prob 1.743 (1.7432)	GS 31.047 (31.047)	mem 43.739
Train: [2][712/750]	BT 6.268 (1.161)	DT 6.160 (1.027)	loss 9.925 (9.925)	gnorm 575253.875 (575253.875)	prob 2.075 (2.0754)	GS 32.094 (32.094)	mem 43.614
Train: [2][713/750]	BT 0.231 (1.160)	DT 0.001 (1.026)	loss 10.708 (10.708)	gnorm 554583.500 (554583.500)	prob 0.872 (0.8725)	GS 29.812 (29.812)	mem 43.625
Train: [2][714/750]	BT 0.102 (1.158)	DT 0.008 (1.024)	loss 10.019 (10.019)	gnorm 530925.000 (530925.000)	prob 1.369 (1.3694)	GS 33.953 (33.953)	mem 43.626
Train: [2][715/750]	BT 0.147 (1.157)	DT 0.001 (1.023)	loss 10.194 (10.194)	gnorm 550056.000 (550056.000)	prob 1.506 (1.5059)	GS 30.297 (30.297)	mem 43.626
Train: [2][716/750]	BT 5.493 (1.163)	DT 5.286 (1.029)	loss 10.181 (10.181)	gnorm 571170.312 (571170.312)	prob 1.781 (1.7808)	GS 32.781 (32.781)	mem 44.029
Train: [2][717/750]	BT 0.133 (1.161)	DT 0.020 (1.028)	loss 9.981 (9.981)	gnorm 524691.125 (524691.125)	prob 2.711 (2.7114)	GS 29.484 (29.484)	mem 43.996
Train: [2][718/750]	BT 0.125 (1.160)	DT 0.003 (1.026)	loss 10.500 (10.500)	gnorm 555983.188 (555983.188)	prob 1.101 (1.1008)	GS 31.922 (31.922)	mem 43.683
Train: [2][719/750]	BT 0.103 (1.158)	DT 0.006 (1.025)	loss 10.028 (10.028)	gnorm 528894.312 (528894.312)	prob 1.503 (1.5032)	GS 28.641 (28.641)	mem 43.683
Train: [2][720/750]	BT 0.088 (1.157)	DT 0.003 (1.023)	loss 10.005 (10.005)	gnorm 537838.438 (537838.438)	prob 1.404 (1.4037)	GS 35.312 (35.312)	mem 43.682
Train: [2][721/750]	BT 0.123 (1.156)	DT 0.001 (1.022)	loss 10.109 (10.109)	gnorm 519751.562 (519751.562)	prob 1.289 (1.2885)	GS 29.750 (29.750)	mem 43.682
Train: [2][722/750]	BT 0.081 (1.154)	DT 0.002 (1.020)	loss 9.902 (9.902)	gnorm 494937.562 (494937.562)	prob 2.043 (2.0427)	GS 31.859 (31.859)	mem 43.682
Train: [2][723/750]	BT 0.133 (1.153)	DT 0.002 (1.019)	loss 9.878 (9.878)	gnorm 547529.438 (547529.438)	prob 1.167 (1.1669)	GS 29.812 (29.812)	mem 43.880
Train: [2][724/750]	BT 4.526 (1.157)	DT 4.424 (1.024)	loss 9.654 (9.654)	gnorm 544457.625 (544457.625)	prob 1.096 (1.0960)	GS 27.172 (27.172)	mem 43.728
Train: [2][725/750]	BT 0.174 (1.156)	DT 0.002 (1.022)	loss 10.002 (10.002)	gnorm 573145.312 (573145.312)	prob 0.690 (0.6901)	GS 30.156 (30.156)	mem 43.786
Train: [2][726/750]	BT 0.203 (1.155)	DT 0.002 (1.021)	loss 10.168 (10.168)	gnorm 526052.750 (526052.750)	prob 0.318 (0.3183)	GS 33.172 (33.172)	mem 43.853
Train: [2][727/750]	BT 0.131 (1.153)	DT 0.012 (1.020)	loss 10.090 (10.090)	gnorm 543007.562 (543007.562)	prob 0.530 (0.5295)	GS 32.109 (32.109)	mem 43.726
Train: [2][728/750]	BT 4.670 (1.158)	DT 4.562 (1.024)	loss 9.630 (9.630)	gnorm 550494.750 (550494.750)	prob 1.365 (1.3649)	GS 34.375 (34.375)	mem 43.466
Train: [2][729/750]	BT 0.155 (1.157)	DT 0.002 (1.023)	loss 10.129 (10.129)	gnorm 557294.000 (557294.000)	prob 1.161 (1.1607)	GS 30.094 (30.094)	mem 43.466
Train: [2][730/750]	BT 0.101 (1.155)	DT 0.001 (1.022)	loss 10.391 (10.391)	gnorm 549931.062 (549931.062)	prob 0.171 (0.1713)	GS 34.312 (34.312)	mem 43.465
Train: [2][731/750]	BT 0.099 (1.154)	DT 0.004 (1.020)	loss 10.261 (10.261)	gnorm 543840.938 (543840.938)	prob 1.453 (1.4528)	GS 28.828 (28.828)	mem 43.329
Train: [2][732/750]	BT 0.196 (1.152)	DT 0.001 (1.019)	loss 9.901 (9.901)	gnorm 480321.594 (480321.594)	prob 0.965 (0.9647)	GS 34.469 (34.469)	mem 43.330
Train: [2][733/750]	BT 0.145 (1.151)	DT 0.008 (1.017)	loss 10.188 (10.188)	gnorm 566327.750 (566327.750)	prob 1.163 (1.1633)	GS 27.484 (27.484)	mem 43.330
Train: [2][734/750]	BT 0.143 (1.150)	DT 0.003 (1.016)	loss 10.302 (10.302)	gnorm 604681.688 (604681.688)	prob 0.789 (0.7889)	GS 33.641 (33.641)	mem 43.403
Train: [2][735/750]	BT 0.127 (1.148)	DT 0.002 (1.015)	loss 10.447 (10.447)	gnorm 546262.438 (546262.438)	prob 1.873 (1.8729)	GS 32.188 (32.188)	mem 43.267
Train: [2][736/750]	BT 4.560 (1.153)	DT 4.468 (1.019)	loss 10.024 (10.024)	gnorm 563672.688 (563672.688)	prob 1.795 (1.7949)	GS 33.234 (33.234)	mem 23.047
Train: [2][737/750]	BT 0.084 (1.151)	DT 0.001 (1.018)	loss 10.258 (10.258)	gnorm 544897.438 (544897.438)	prob 0.879 (0.8794)	GS 26.297 (26.297)	mem 23.048
Train: [2][738/750]	BT 0.071 (1.150)	DT 0.002 (1.017)	loss 9.707 (9.707)	gnorm 527204.688 (527204.688)	prob 1.928 (1.9284)	GS 31.250 (31.250)	mem 23.048
Train: [2][739/750]	BT 0.081 (1.149)	DT 0.001 (1.015)	loss 10.263 (10.263)	gnorm 588623.938 (588623.938)	prob 1.748 (1.7481)	GS 30.266 (30.266)	mem 23.048
Train: [2][740/750]	BT 2.462 (1.150)	DT 2.391 (1.017)	loss 9.957 (9.957)	gnorm 552036.438 (552036.438)	prob 2.155 (2.1546)	GS 27.844 (27.844)	mem 14.325
Train: [2][741/750]	BT 0.075 (1.149)	DT 0.001 (1.016)	loss 10.630 (10.630)	gnorm 601230.562 (601230.562)	prob 1.582 (1.5818)	GS 29.984 (29.984)	mem 14.325
Train: [2][742/750]	BT 0.066 (1.147)	DT 0.002 (1.014)	loss 10.557 (10.557)	gnorm 513620.906 (513620.906)	prob 1.049 (1.0491)	GS 29.547 (29.547)	mem 14.325
Train: [2][743/750]	BT 0.084 (1.146)	DT 0.002 (1.013)	loss 10.271 (10.271)	gnorm 556831.562 (556831.562)	prob 2.021 (2.0212)	GS 29.422 (29.422)	mem 14.325
Train: [2][744/750]	BT 0.078 (1.145)	DT 0.002 (1.012)	loss 10.291 (10.291)	gnorm 558688.312 (558688.312)	prob 1.765 (1.7653)	GS 32.531 (32.531)	mem 14.325
Train: [2][745/750]	BT 0.073 (1.143)	DT 0.002 (1.010)	loss 10.001 (10.001)	gnorm 749965.125 (749965.125)	prob 2.243 (2.2433)	GS 30.250 (30.250)	mem 14.432
Train: [2][746/750]	BT 0.124 (1.142)	DT 0.002 (1.009)	loss 10.772 (10.772)	gnorm 878390.000 (878390.000)	prob 1.332 (1.3324)	GS 35.125 (35.125)	mem 14.463
Train: [2][747/750]	BT 0.076 (1.140)	DT 0.002 (1.008)	loss 10.143 (10.143)	gnorm 761997.375 (761997.375)	prob 2.368 (2.3676)	GS 39.406 (39.406)	mem 14.325
Train: [2][748/750]	BT 0.412 (1.139)	DT 0.329 (1.007)	loss 9.725 (9.725)	gnorm 796819.312 (796819.312)	prob 2.645 (2.6451)	GS 37.312 (37.312)	mem 14.297
Train: [2][749/750]	BT 0.067 (1.138)	DT 0.001 (1.005)	loss 9.895 (9.895)	gnorm 695872.625 (695872.625)	prob 3.194 (3.1944)	GS 28.438 (28.438)	mem 14.297
Train: [2][750/750]	BT 0.078 (1.137)	DT 0.001 (1.004)	loss 10.017 (10.017)	gnorm 773631.125 (773631.125)	prob 3.054 (3.0540)	GS 34.469 (34.469)	mem 14.296
Train: [2][751/750]	BT 0.074 (1.135)	DT 0.002 (1.003)	loss 10.069 (10.069)	gnorm 810530.062 (810530.062)	prob 2.695 (2.6950)	GS 38.812 (38.812)	mem 14.297
Train: [2][752/750]	BT 1.270 (1.135)	DT 1.206 (1.003)	loss 9.708 (9.708)	gnorm 806725.250 (806725.250)	prob 2.951 (2.9515)	GS 38.625 (38.625)	mem 11.356
Train: [2][753/750]	BT 0.069 (1.134)	DT 0.002 (1.002)	loss 9.880 (9.880)	gnorm 715560.625 (715560.625)	prob 2.829 (2.8294)	GS 27.656 (27.656)	mem 11.356
Train: [2][754/750]	BT 0.062 (1.132)	DT 0.002 (1.000)	loss 10.007 (10.007)	gnorm 747583.188 (747583.188)	prob 3.344 (3.3442)	GS 34.875 (34.875)	mem 11.356
Train: [2][755/750]	BT 0.065 (1.131)	DT 0.001 (0.999)	loss 9.909 (9.909)	gnorm 747782.938 (747782.938)	prob 2.977 (2.9774)	GS 24.062 (24.062)	mem 11.356
Train: [2][756/750]	BT 0.071 (1.130)	DT 0.001 (0.998)	loss 10.558 (10.558)	gnorm 788162.125 (788162.125)	prob 1.988 (1.9878)	GS 34.438 (34.438)	mem 11.356
epoch 2, total time 854.21
==> Saving...
==> Saving...
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [3][1/750]	BT 23.426 (23.426)	DT 23.227 (23.227)	loss 9.929 (9.929)	gnorm 568243.750 (568243.750)	prob 2.597 (2.5970)	GS 31.281 (31.281)	mem 42.354
Train: [3][2/750]	BT 0.350 (11.888)	DT 0.234 (11.731)	loss 10.310 (10.310)	gnorm 583346.062 (583346.062)	prob 1.937 (1.9368)	GS 33.422 (33.422)	mem 42.462
Train: [3][3/750]	BT 0.291 (8.022)	DT 0.015 (7.825)	loss 10.076 (10.076)	gnorm 507855.594 (507855.594)	prob 2.176 (2.1763)	GS 31.094 (31.094)	mem 42.377
Train: [3][4/750]	BT 0.470 (6.134)	DT 0.324 (5.950)	loss 10.596 (10.596)	gnorm 604409.562 (604409.562)	prob 1.932 (1.9325)	GS 36.297 (36.297)	mem 42.382
Train: [3][5/750]	BT 0.131 (4.933)	DT 0.002 (4.760)	loss 9.851 (9.851)	gnorm 574302.750 (574302.750)	prob 2.190 (2.1897)	GS 31.609 (31.609)	mem 42.384
Train: [3][6/750]	BT 0.226 (4.149)	DT 0.011 (3.969)	loss 10.125 (10.125)	gnorm 541270.438 (541270.438)	prob 2.271 (2.2711)	GS 33.844 (33.844)	mem 42.385
Train: [3][7/750]	BT 0.194 (3.584)	DT 0.003 (3.402)	loss 10.364 (10.364)	gnorm 579751.625 (579751.625)	prob 1.789 (1.7893)	GS 31.406 (31.406)	mem 42.386
Train: [3][8/750]	BT 0.149 (3.155)	DT 0.055 (2.984)	loss 10.268 (10.268)	gnorm 611577.438 (611577.438)	prob 1.066 (1.0664)	GS 32.859 (32.859)	mem 42.388
Train: [3][9/750]	BT 0.155 (2.821)	DT 0.001 (2.652)	loss 10.659 (10.659)	gnorm 587493.312 (587493.312)	prob 0.948 (0.9478)	GS 34.188 (34.188)	mem 42.435
Train: [3][10/750]	BT 0.109 (2.550)	DT 0.004 (2.388)	loss 10.235 (10.235)	gnorm 513319.969 (513319.969)	prob 0.985 (0.9850)	GS 35.469 (35.469)	mem 42.570
Train: [3][11/750]	BT 0.188 (2.335)	DT 0.020 (2.172)	loss 10.008 (10.008)	gnorm 538129.062 (538129.062)	prob 2.136 (2.1364)	GS 36.453 (36.453)	mem 42.558
Train: [3][12/750]	BT 0.119 (2.151)	DT 0.002 (1.991)	loss 10.438 (10.438)	gnorm 544628.125 (544628.125)	prob 1.091 (1.0906)	GS 36.469 (36.469)	mem 42.581
Train: [3][13/750]	BT 6.851 (2.512)	DT 6.765 (2.359)	loss 9.986 (9.986)	gnorm 554054.250 (554054.250)	prob 2.671 (2.6708)	GS 30.000 (30.000)	mem 42.483
Train: [3][14/750]	BT 3.704 (2.597)	DT 3.554 (2.444)	loss 9.739 (9.739)	gnorm 543928.125 (543928.125)	prob 1.907 (1.9067)	GS 30.984 (30.984)	mem 42.546
Train: [3][15/750]	BT 0.128 (2.433)	DT 0.004 (2.281)	loss 9.568 (9.568)	gnorm 537326.812 (537326.812)	prob 2.894 (2.8935)	GS 31.109 (31.109)	mem 42.609
Train: [3][16/750]	BT 2.889 (2.461)	DT 2.756 (2.311)	loss 9.821 (9.821)	gnorm 522994.625 (522994.625)	prob 1.658 (1.6581)	GS 33.062 (33.062)	mem 42.571
Train: [3][17/750]	BT 0.123 (2.324)	DT 0.003 (2.175)	loss 10.280 (10.280)	gnorm 592363.875 (592363.875)	prob 1.331 (1.3310)	GS 26.281 (26.281)	mem 42.571
Train: [3][18/750]	BT 0.333 (2.213)	DT 0.220 (2.067)	loss 10.345 (10.345)	gnorm 546505.000 (546505.000)	prob 1.070 (1.0695)	GS 30.359 (30.359)	mem 42.628
Train: [3][19/750]	BT 0.202 (2.107)	DT 0.003 (1.958)	loss 10.571 (10.571)	gnorm 578415.500 (578415.500)	prob 0.763 (0.7630)	GS 30.500 (30.500)	mem 42.588
Train: [3][20/750]	BT 3.244 (2.164)	DT 3.073 (2.014)	loss 10.137 (10.137)	gnorm 506318.438 (506318.438)	prob 0.567 (0.5673)	GS 36.312 (36.312)	mem 42.625
Train: [3][21/750]	BT 0.084 (2.065)	DT 0.002 (1.918)	loss 10.214 (10.214)	gnorm 529226.000 (529226.000)	prob 0.883 (0.8831)	GS 27.453 (27.453)	mem 42.625
Train: [3][22/750]	BT 0.160 (1.978)	DT 0.002 (1.831)	loss 10.606 (10.606)	gnorm 583261.500 (583261.500)	prob 0.654 (0.6536)	GS 29.594 (29.594)	mem 42.639
Train: [3][23/750]	BT 0.162 (1.899)	DT 0.006 (1.752)	loss 10.043 (10.043)	gnorm 535218.500 (535218.500)	prob 1.405 (1.4048)	GS 30.516 (30.516)	mem 42.639
Train: [3][24/750]	BT 0.184 (1.828)	DT 0.006 (1.679)	loss 10.123 (10.123)	gnorm 575886.312 (575886.312)	prob 1.063 (1.0630)	GS 33.984 (33.984)	mem 42.640
Train: [3][25/750]	BT 0.159 (1.761)	DT 0.003 (1.612)	loss 10.482 (10.482)	gnorm 569711.562 (569711.562)	prob 1.094 (1.0936)	GS 31.828 (31.828)	mem 42.663
Train: [3][26/750]	BT 5.585 (1.908)	DT 5.450 (1.759)	loss 10.435 (10.435)	gnorm 566812.875 (566812.875)	prob 0.988 (0.9878)	GS 31.828 (31.828)	mem 42.656
Train: [3][27/750]	BT 0.260 (1.847)	DT 0.010 (1.695)	loss 10.180 (10.180)	gnorm 556413.438 (556413.438)	prob 1.909 (1.9095)	GS 32.031 (32.031)	mem 42.656
Train: [3][28/750]	BT 1.453 (1.833)	DT 1.358 (1.683)	loss 9.808 (9.808)	gnorm 569111.250 (569111.250)	prob 1.659 (1.6588)	GS 32.688 (32.688)	mem 42.895
Train: [3][29/750]	BT 0.103 (1.773)	DT 0.002 (1.625)	loss 10.252 (10.252)	gnorm 548527.688 (548527.688)	prob 1.572 (1.5720)	GS 32.359 (32.359)	mem 42.832
Train: [3][30/750]	BT 0.196 (1.721)	DT 0.014 (1.571)	loss 9.167 (9.167)	gnorm 484567.438 (484567.438)	prob 2.371 (2.3712)	GS 34.203 (34.203)	mem 42.660
Train: [3][31/750]	BT 0.113 (1.669)	DT 0.006 (1.520)	loss 10.257 (10.257)	gnorm 538577.062 (538577.062)	prob 1.204 (1.2041)	GS 31.969 (31.969)	mem 42.659
Train: [3][32/750]	BT 4.362 (1.753)	DT 4.226 (1.605)	loss 10.470 (10.470)	gnorm 508647.594 (508647.594)	prob 0.722 (0.7219)	GS 30.016 (30.016)	mem 42.672
Train: [3][33/750]	BT 0.140 (1.704)	DT 0.011 (1.557)	loss 10.751 (10.751)	gnorm 541519.312 (541519.312)	prob 0.752 (0.7520)	GS 27.969 (27.969)	mem 42.745
Train: [3][34/750]	BT 0.116 (1.658)	DT 0.002 (1.511)	loss 10.454 (10.454)	gnorm 527770.750 (527770.750)	prob 0.702 (0.7023)	GS 31.547 (31.547)	mem 42.673
Train: [3][35/750]	BT 0.197 (1.616)	DT 0.015 (1.468)	loss 10.169 (10.169)	gnorm 514870.188 (514870.188)	prob 1.069 (1.0688)	GS 30.797 (30.797)	mem 42.675
Train: [3][36/750]	BT 0.159 (1.575)	DT 0.002 (1.428)	loss 10.340 (10.340)	gnorm 583062.688 (583062.688)	prob 1.103 (1.1033)	GS 32.641 (32.641)	mem 42.677
Train: [3][37/750]	BT 0.123 (1.536)	DT 0.010 (1.389)	loss 10.915 (10.915)	gnorm 580421.875 (580421.875)	prob 0.422 (0.4218)	GS 28.625 (28.625)	mem 42.677
Train: [3][38/750]	BT 8.207 (1.712)	DT 8.056 (1.565)	loss 10.085 (10.085)	gnorm 650710.625 (650710.625)	prob 1.481 (1.4808)	GS 36.875 (36.875)	mem 42.747
Train: [3][39/750]	BT 0.167 (1.672)	DT 0.002 (1.525)	loss 10.026 (10.026)	gnorm 550553.500 (550553.500)	prob 1.469 (1.4692)	GS 27.156 (27.156)	mem 42.720
Train: [3][40/750]	BT 0.090 (1.633)	DT 0.002 (1.486)	loss 10.476 (10.476)	gnorm 627288.875 (627288.875)	prob 0.949 (0.9488)	GS 33.875 (33.875)	mem 42.757
Train: [3][41/750]	BT 0.110 (1.595)	DT 0.002 (1.450)	loss 10.517 (10.517)	gnorm 561547.000 (561547.000)	prob 1.644 (1.6441)	GS 31.266 (31.266)	mem 42.722
Train: [3][42/750]	BT 0.226 (1.563)	DT 0.005 (1.416)	loss 10.104 (10.104)	gnorm 511413.844 (511413.844)	prob 1.798 (1.7977)	GS 33.938 (33.938)	mem 42.771
Train: [3][43/750]	BT 0.184 (1.531)	DT 0.007 (1.383)	loss 10.350 (10.350)	gnorm 547419.812 (547419.812)	prob 2.017 (2.0168)	GS 33.859 (33.859)	mem 42.882
Train: [3][44/750]	BT 5.847 (1.629)	DT 5.671 (1.481)	loss 9.424 (9.424)	gnorm 517109.906 (517109.906)	prob 2.476 (2.4763)	GS 31.938 (31.938)	mem 42.655
Train: [3][45/750]	BT 0.089 (1.595)	DT 0.002 (1.448)	loss 10.734 (10.734)	gnorm 545714.250 (545714.250)	prob 0.797 (0.7970)	GS 30.375 (30.375)	mem 42.656
Train: [3][46/750]	BT 0.082 (1.562)	DT 0.002 (1.416)	loss 9.467 (9.467)	gnorm 528638.375 (528638.375)	prob 2.109 (2.1086)	GS 36.297 (36.297)	mem 42.714
Train: [3][47/750]	BT 0.162 (1.532)	DT 0.003 (1.386)	loss 10.349 (10.349)	gnorm 574135.375 (574135.375)	prob 2.282 (2.2821)	GS 33.609 (33.609)	mem 42.658
Train: [3][48/750]	BT 0.080 (1.502)	DT 0.002 (1.357)	loss 10.815 (10.815)	gnorm 587836.812 (587836.812)	prob 1.517 (1.5172)	GS 30.188 (30.188)	mem 42.721
Train: [3][49/750]	BT 0.118 (1.473)	DT 0.001 (1.330)	loss 10.685 (10.685)	gnorm 595520.625 (595520.625)	prob 1.636 (1.6355)	GS 31.250 (31.250)	mem 42.660
Train: [3][50/750]	BT 5.774 (1.559)	DT 5.691 (1.417)	loss 10.121 (10.121)	gnorm 535051.438 (535051.438)	prob 2.036 (2.0358)	GS 32.125 (32.125)	mem 42.716
Train: [3][51/750]	BT 0.132 (1.531)	DT 0.002 (1.389)	loss 10.141 (10.141)	gnorm 535637.000 (535637.000)	prob 1.701 (1.7013)	GS 27.672 (27.672)	mem 42.676
Train: [3][52/750]	BT 0.275 (1.507)	DT 0.016 (1.363)	loss 9.883 (9.883)	gnorm 528128.000 (528128.000)	prob 1.185 (1.1850)	GS 32.750 (32.750)	mem 42.676
Train: [3][53/750]	BT 0.088 (1.481)	DT 0.002 (1.337)	loss 10.147 (10.147)	gnorm 533813.250 (533813.250)	prob 1.713 (1.7127)	GS 36.406 (36.406)	mem 42.677
Train: [3][54/750]	BT 0.245 (1.458)	DT 0.002 (1.312)	loss 10.093 (10.093)	gnorm 509170.031 (509170.031)	prob 1.544 (1.5436)	GS 32.156 (32.156)	mem 42.679
Train: [3][55/750]	BT 0.124 (1.433)	DT 0.021 (1.289)	loss 10.920 (10.920)	gnorm 525416.500 (525416.500)	prob 0.777 (0.7772)	GS 36.203 (36.203)	mem 42.763
Train: [3][56/750]	BT 7.618 (1.544)	DT 7.454 (1.399)	loss 10.206 (10.206)	gnorm 560504.562 (560504.562)	prob 1.691 (1.6905)	GS 35.375 (35.375)	mem 42.767
Train: [3][57/750]	BT 0.166 (1.520)	DT 0.008 (1.375)	loss 10.233 (10.233)	gnorm 547576.312 (547576.312)	prob 1.457 (1.4574)	GS 34.828 (34.828)	mem 42.766
Train: [3][58/750]	BT 0.098 (1.495)	DT 0.009 (1.351)	loss 10.512 (10.512)	gnorm 527880.125 (527880.125)	prob 1.285 (1.2851)	GS 34.078 (34.078)	mem 42.792
Train: [3][59/750]	BT 0.112 (1.472)	DT 0.012 (1.328)	loss 10.776 (10.776)	gnorm 552640.812 (552640.812)	prob 0.909 (0.9086)	GS 34.828 (34.828)	mem 42.810
Train: [3][60/750]	BT 0.262 (1.452)	DT 0.003 (1.306)	loss 10.546 (10.546)	gnorm 566069.500 (566069.500)	prob 1.205 (1.2048)	GS 34.125 (34.125)	mem 42.894
Train: [3][61/750]	BT 0.103 (1.429)	DT 0.002 (1.285)	loss 10.201 (10.201)	gnorm 545508.438 (545508.438)	prob 2.390 (2.3904)	GS 32.609 (32.609)	mem 42.777
Train: [3][62/750]	BT 5.555 (1.496)	DT 5.370 (1.351)	loss 10.014 (10.014)	gnorm 538686.062 (538686.062)	prob 2.034 (2.0344)	GS 31.859 (31.859)	mem 42.947
Train: [3][63/750]	BT 0.180 (1.475)	DT 0.028 (1.330)	loss 10.090 (10.090)	gnorm 605421.250 (605421.250)	prob 2.241 (2.2412)	GS 27.266 (27.266)	mem 42.821
Train: [3][64/750]	BT 0.126 (1.454)	DT 0.002 (1.309)	loss 10.268 (10.268)	gnorm 536773.062 (536773.062)	prob 1.946 (1.9456)	GS 28.578 (28.578)	mem 42.821
Train: [3][65/750]	BT 0.151 (1.434)	DT 0.014 (1.289)	loss 10.206 (10.206)	gnorm 544388.875 (544388.875)	prob 1.680 (1.6802)	GS 35.234 (35.234)	mem 42.846
Train: [3][66/750]	BT 0.213 (1.415)	DT 0.022 (1.270)	loss 9.705 (9.705)	gnorm 614123.000 (614123.000)	prob 2.180 (2.1804)	GS 36.750 (36.750)	mem 42.855
Train: [3][67/750]	BT 0.084 (1.396)	DT 0.006 (1.251)	loss 10.476 (10.476)	gnorm 523501.000 (523501.000)	prob 1.375 (1.3747)	GS 30.000 (30.000)	mem 42.949
Train: [3][68/750]	BT 7.050 (1.479)	DT 6.930 (1.335)	loss 10.293 (10.293)	gnorm 535597.625 (535597.625)	prob 1.856 (1.8557)	GS 33.047 (33.047)	mem 42.803
Train: [3][69/750]	BT 0.191 (1.460)	DT 0.018 (1.315)	loss 9.987 (9.987)	gnorm 586279.875 (586279.875)	prob 1.061 (1.0608)	GS 33.141 (33.141)	mem 42.777
Train: [3][70/750]	BT 0.127 (1.441)	DT 0.002 (1.297)	loss 10.105 (10.105)	gnorm 513786.625 (513786.625)	prob 1.439 (1.4393)	GS 35.875 (35.875)	mem 42.822
Train: [3][71/750]	BT 0.160 (1.423)	DT 0.022 (1.279)	loss 9.939 (9.939)	gnorm 522353.750 (522353.750)	prob 1.346 (1.3461)	GS 30.297 (30.297)	mem 42.774
Train: [3][72/750]	BT 0.103 (1.405)	DT 0.002 (1.261)	loss 10.455 (10.455)	gnorm 553093.375 (553093.375)	prob 1.452 (1.4522)	GS 36.062 (36.062)	mem 42.774
Train: [3][73/750]	BT 0.162 (1.388)	DT 0.003 (1.244)	loss 9.995 (9.995)	gnorm 547863.875 (547863.875)	prob 2.061 (2.0607)	GS 35.328 (35.328)	mem 42.775
Train: [3][74/750]	BT 5.513 (1.443)	DT 5.336 (1.299)	loss 10.500 (10.500)	gnorm 529369.375 (529369.375)	prob 1.241 (1.2406)	GS 32.750 (32.750)	mem 43.026
Train: [3][75/750]	BT 0.107 (1.426)	DT 0.009 (1.282)	loss 10.016 (10.016)	gnorm 546716.562 (546716.562)	prob 1.974 (1.9743)	GS 30.781 (30.781)	mem 42.813
Train: [3][76/750]	BT 0.089 (1.408)	DT 0.005 (1.265)	loss 10.569 (10.569)	gnorm 569661.250 (569661.250)	prob 1.324 (1.3245)	GS 33.062 (33.062)	mem 42.972
Train: [3][77/750]	BT 0.133 (1.391)	DT 0.001 (1.249)	loss 10.095 (10.095)	gnorm 546386.125 (546386.125)	prob 1.532 (1.5317)	GS 33.531 (33.531)	mem 42.939
Train: [3][78/750]	BT 0.101 (1.375)	DT 0.009 (1.233)	loss 10.562 (10.562)	gnorm 510377.500 (510377.500)	prob 1.567 (1.5670)	GS 36.125 (36.125)	mem 42.937
Train: [3][79/750]	BT 0.089 (1.359)	DT 0.003 (1.217)	loss 9.841 (9.841)	gnorm 521765.750 (521765.750)	prob 2.174 (2.1741)	GS 34.484 (34.484)	mem 42.794
Train: [3][80/750]	BT 3.892 (1.390)	DT 3.769 (1.249)	loss 9.814 (9.814)	gnorm 531225.000 (531225.000)	prob 2.217 (2.2174)	GS 34.062 (34.062)	mem 42.744
Train: [3][81/750]	BT 0.097 (1.374)	DT 0.011 (1.234)	loss 10.151 (10.151)	gnorm 533005.000 (533005.000)	prob 1.331 (1.3312)	GS 29.734 (29.734)	mem 42.729
Train: [3][82/750]	BT 0.158 (1.359)	DT 0.002 (1.219)	loss 9.983 (9.983)	gnorm 540850.438 (540850.438)	prob 2.002 (2.0016)	GS 27.250 (27.250)	mem 42.729
Train: [3][83/750]	BT 0.167 (1.345)	DT 0.018 (1.204)	loss 9.873 (9.873)	gnorm 525929.000 (525929.000)	prob 2.250 (2.2503)	GS 34.297 (34.297)	mem 42.729
Train: [3][84/750]	BT 0.125 (1.331)	DT 0.003 (1.190)	loss 9.767 (9.767)	gnorm 527342.250 (527342.250)	prob 1.738 (1.7378)	GS 33.750 (33.750)	mem 42.765
Train: [3][85/750]	BT 0.235 (1.318)	DT 0.020 (1.176)	loss 9.795 (9.795)	gnorm 520858.656 (520858.656)	prob 2.036 (2.0357)	GS 28.469 (28.469)	mem 42.731
Train: [3][86/750]	BT 11.721 (1.439)	DT 11.604 (1.297)	loss 10.028 (10.028)	gnorm 525058.562 (525058.562)	prob 1.204 (1.2036)	GS 34.844 (34.844)	mem 42.884
Train: [3][87/750]	BT 0.270 (1.425)	DT 0.048 (1.283)	loss 10.425 (10.425)	gnorm 552970.625 (552970.625)	prob 1.136 (1.1357)	GS 34.438 (34.438)	mem 42.884
Train: [3][88/750]	BT 0.104 (1.410)	DT 0.017 (1.269)	loss 10.168 (10.168)	gnorm 525621.375 (525621.375)	prob 1.649 (1.6492)	GS 35.266 (35.266)	mem 42.884
Train: [3][89/750]	BT 0.144 (1.396)	DT 0.018 (1.255)	loss 9.741 (9.741)	gnorm 523438.938 (523438.938)	prob 1.628 (1.6277)	GS 33.672 (33.672)	mem 42.816
Train: [3][90/750]	BT 0.126 (1.382)	DT 0.015 (1.241)	loss 9.630 (9.630)	gnorm 507728.969 (507728.969)	prob 1.987 (1.9868)	GS 31.797 (31.797)	mem 42.816
Train: [3][91/750]	BT 0.109 (1.368)	DT 0.001 (1.227)	loss 10.615 (10.615)	gnorm 513421.250 (513421.250)	prob 1.264 (1.2642)	GS 31.281 (31.281)	mem 42.817
Train: [3][92/750]	BT 2.010 (1.375)	DT 1.919 (1.235)	loss 10.319 (10.319)	gnorm 527412.812 (527412.812)	prob 1.135 (1.1355)	GS 34.250 (34.250)	mem 42.857
Train: [3][93/750]	BT 0.120 (1.361)	DT 0.002 (1.222)	loss 10.226 (10.226)	gnorm 572256.438 (572256.438)	prob 0.905 (0.9047)	GS 31.438 (31.438)	mem 42.825
Train: [3][94/750]	BT 0.147 (1.348)	DT 0.001 (1.209)	loss 10.715 (10.715)	gnorm 592988.625 (592988.625)	prob 0.082 (0.0822)	GS 34.750 (34.750)	mem 42.825
Train: [3][95/750]	BT 0.130 (1.336)	DT 0.002 (1.196)	loss 10.451 (10.451)	gnorm 609948.938 (609948.938)	prob 1.054 (1.0539)	GS 32.438 (32.438)	mem 42.826
Train: [3][96/750]	BT 0.113 (1.323)	DT 0.002 (1.183)	loss 9.822 (9.822)	gnorm 536382.312 (536382.312)	prob 0.982 (0.9824)	GS 35.922 (35.922)	mem 42.826
Train: [3][97/750]	BT 0.116 (1.310)	DT 0.002 (1.171)	loss 10.581 (10.581)	gnorm 538401.438 (538401.438)	prob 1.123 (1.1232)	GS 34.406 (34.406)	mem 42.852
Train: [3][98/750]	BT 8.233 (1.381)	DT 8.142 (1.242)	loss 9.741 (9.741)	gnorm 551613.000 (551613.000)	prob 2.123 (2.1231)	GS 33.516 (33.516)	mem 42.764
Train: [3][99/750]	BT 0.108 (1.368)	DT 0.004 (1.230)	loss 10.147 (10.147)	gnorm 578356.062 (578356.062)	prob 1.761 (1.7608)	GS 39.125 (39.125)	mem 42.788
Train: [3][100/750]	BT 0.100 (1.356)	DT 0.002 (1.218)	loss 10.610 (10.610)	gnorm 504810.125 (504810.125)	prob 1.762 (1.7615)	GS 36.125 (36.125)	mem 42.767
Train: [3][101/750]	BT 0.207 (1.344)	DT 0.002 (1.206)	loss 10.556 (10.556)	gnorm 568573.188 (568573.188)	prob 1.446 (1.4465)	GS 34.078 (34.078)	mem 42.755
Train: [3][102/750]	BT 0.093 (1.332)	DT 0.001 (1.194)	loss 9.988 (9.988)	gnorm 535887.688 (535887.688)	prob 1.800 (1.8001)	GS 35.312 (35.312)	mem 42.790
Train: [3][103/750]	BT 0.155 (1.320)	DT 0.008 (1.182)	loss 9.672 (9.672)	gnorm 512939.594 (512939.594)	prob 2.232 (2.2321)	GS 32.203 (32.203)	mem 42.785
Train: [3][104/750]	BT 2.749 (1.334)	DT 2.449 (1.194)	loss 9.854 (9.854)	gnorm 499812.906 (499812.906)	prob 1.403 (1.4026)	GS 34.125 (34.125)	mem 42.863
Train: [3][105/750]	BT 0.087 (1.322)	DT 0.003 (1.183)	loss 9.923 (9.923)	gnorm 543290.688 (543290.688)	prob 1.574 (1.5738)	GS 30.875 (30.875)	mem 42.739
Train: [3][106/750]	BT 0.148 (1.311)	DT 0.002 (1.172)	loss 10.365 (10.365)	gnorm 537604.500 (537604.500)	prob 0.636 (0.6360)	GS 32.469 (32.469)	mem 42.739
Train: [3][107/750]	BT 0.148 (1.300)	DT 0.003 (1.161)	loss 10.381 (10.381)	gnorm 533407.000 (533407.000)	prob 0.873 (0.8733)	GS 29.016 (29.016)	mem 42.756
Train: [3][108/750]	BT 0.145 (1.290)	DT 0.003 (1.150)	loss 10.024 (10.024)	gnorm 562669.625 (562669.625)	prob 0.696 (0.6963)	GS 33.016 (33.016)	mem 42.770
Train: [3][109/750]	BT 0.091 (1.279)	DT 0.001 (1.140)	loss 9.157 (9.157)	gnorm 537526.938 (537526.938)	prob 2.262 (2.2621)	GS 31.328 (31.328)	mem 42.820
Train: [3][110/750]	BT 10.204 (1.360)	DT 10.087 (1.221)	loss 10.218 (10.218)	gnorm 537863.688 (537863.688)	prob 0.393 (0.3928)	GS 32.719 (32.719)	mem 42.864
Train: [3][111/750]	BT 0.090 (1.348)	DT 0.003 (1.210)	loss 10.368 (10.368)	gnorm 484652.438 (484652.438)	prob -0.032 (-0.0317)	GS 26.656 (26.656)	mem 42.892
Train: [3][112/750]	BT 0.151 (1.338)	DT 0.002 (1.199)	loss 9.656 (9.656)	gnorm 561833.062 (561833.062)	prob 0.864 (0.8636)	GS 35.312 (35.312)	mem 42.865
Train: [3][113/750]	BT 0.074 (1.327)	DT 0.002 (1.189)	loss 10.408 (10.408)	gnorm 511892.656 (511892.656)	prob -0.370 (-0.3696)	GS 30.688 (30.688)	mem 42.865
Train: [3][114/750]	BT 0.079 (1.316)	DT 0.001 (1.178)	loss 10.223 (10.223)	gnorm 526351.875 (526351.875)	prob -0.030 (-0.0300)	GS 30.250 (30.250)	mem 42.865
Train: [3][115/750]	BT 0.103 (1.305)	DT 0.002 (1.168)	loss 10.202 (10.202)	gnorm 567992.688 (567992.688)	prob 0.524 (0.5236)	GS 32.875 (32.875)	mem 42.865
Train: [3][116/750]	BT 0.575 (1.299)	DT 0.449 (1.162)	loss 10.240 (10.240)	gnorm 506298.781 (506298.781)	prob 0.196 (0.1960)	GS 32.156 (32.156)	mem 42.972
Train: [3][117/750]	BT 0.250 (1.290)	DT 0.035 (1.152)	loss 10.490 (10.490)	gnorm 576446.250 (576446.250)	prob 0.264 (0.2638)	GS 35.984 (35.984)	mem 42.866
Train: [3][118/750]	BT 0.181 (1.280)	DT 0.014 (1.143)	loss 10.546 (10.546)	gnorm 536711.250 (536711.250)	prob 0.258 (0.2582)	GS 35.172 (35.172)	mem 42.866
Train: [3][119/750]	BT 0.220 (1.271)	DT 0.009 (1.133)	loss 10.158 (10.158)	gnorm 515608.438 (515608.438)	prob 1.335 (1.3349)	GS 31.234 (31.234)	mem 42.867
Train: [3][120/750]	BT 0.166 (1.262)	DT 0.003 (1.124)	loss 10.286 (10.286)	gnorm 514694.281 (514694.281)	prob 0.966 (0.9659)	GS 32.906 (32.906)	mem 42.867
Train: [3][121/750]	BT 0.135 (1.253)	DT 0.002 (1.114)	loss 10.440 (10.440)	gnorm 526181.062 (526181.062)	prob 1.386 (1.3865)	GS 29.844 (29.844)	mem 42.868
Train: [3][122/750]	BT 10.725 (1.331)	DT 10.613 (1.192)	loss 10.120 (10.120)	gnorm 533671.188 (533671.188)	prob 1.598 (1.5984)	GS 31.766 (31.766)	mem 42.896
Train: [3][123/750]	BT 0.150 (1.321)	DT 0.009 (1.183)	loss 10.095 (10.095)	gnorm 508545.781 (508545.781)	prob 1.415 (1.4151)	GS 31.750 (31.750)	mem 42.948
Train: [3][124/750]	BT 0.102 (1.311)	DT 0.002 (1.173)	loss 10.190 (10.190)	gnorm 481659.750 (481659.750)	prob 1.495 (1.4949)	GS 32.984 (32.984)	mem 42.899
Train: [3][125/750]	BT 0.132 (1.302)	DT 0.002 (1.164)	loss 10.234 (10.234)	gnorm 478998.750 (478998.750)	prob 1.192 (1.1918)	GS 26.750 (26.750)	mem 42.899
Train: [3][126/750]	BT 0.141 (1.292)	DT 0.024 (1.155)	loss 10.339 (10.339)	gnorm 514767.844 (514767.844)	prob 1.442 (1.4424)	GS 32.125 (32.125)	mem 42.898
Train: [3][127/750]	BT 0.077 (1.283)	DT 0.002 (1.146)	loss 10.417 (10.417)	gnorm 541602.938 (541602.938)	prob 1.983 (1.9830)	GS 32.906 (32.906)	mem 42.898
Train: [3][128/750]	BT 3.996 (1.304)	DT 3.889 (1.167)	loss 9.989 (9.989)	gnorm 546614.000 (546614.000)	prob 1.357 (1.3574)	GS 35.344 (35.344)	mem 42.907
Train: [3][129/750]	BT 0.138 (1.295)	DT 0.029 (1.158)	loss 10.191 (10.191)	gnorm 539716.625 (539716.625)	prob 1.114 (1.1139)	GS 32.625 (32.625)	mem 42.908
Train: [3][130/750]	BT 0.108 (1.286)	DT 0.002 (1.149)	loss 10.119 (10.119)	gnorm 537033.562 (537033.562)	prob 1.290 (1.2898)	GS 37.484 (37.484)	mem 42.908
Train: [3][131/750]	BT 0.110 (1.277)	DT 0.006 (1.141)	loss 9.820 (9.820)	gnorm 517162.844 (517162.844)	prob 1.649 (1.6488)	GS 35.656 (35.656)	mem 42.909
Train: [3][132/750]	BT 0.102 (1.268)	DT 0.008 (1.132)	loss 10.273 (10.273)	gnorm 471389.156 (471389.156)	prob 1.188 (1.1879)	GS 33.844 (33.844)	mem 42.923
Train: [3][133/750]	BT 0.167 (1.260)	DT 0.002 (1.124)	loss 9.897 (9.897)	gnorm 508906.438 (508906.438)	prob 1.289 (1.2892)	GS 32.703 (32.703)	mem 42.966
Train: [3][134/750]	BT 9.262 (1.320)	DT 9.107 (1.183)	loss 10.209 (10.209)	gnorm 545806.250 (545806.250)	prob 0.692 (0.6923)	GS 34.375 (34.375)	mem 42.868
Train: [3][135/750]	BT 0.099 (1.310)	DT 0.018 (1.174)	loss 10.147 (10.147)	gnorm 504197.125 (504197.125)	prob 0.947 (0.9470)	GS 32.891 (32.891)	mem 42.867
Train: [3][136/750]	BT 0.111 (1.302)	DT 0.001 (1.166)	loss 9.813 (9.813)	gnorm 538006.562 (538006.562)	prob 0.814 (0.8137)	GS 36.766 (36.766)	mem 42.868
Train: [3][137/750]	BT 0.148 (1.293)	DT 0.050 (1.158)	loss 10.155 (10.155)	gnorm 563470.750 (563470.750)	prob 0.983 (0.9833)	GS 32.625 (32.625)	mem 42.878
Train: [3][138/750]	BT 0.178 (1.285)	DT 0.002 (1.149)	loss 10.007 (10.007)	gnorm 583369.000 (583369.000)	prob 1.231 (1.2308)	GS 35.578 (35.578)	mem 42.961
Train: [3][139/750]	BT 0.170 (1.277)	DT 0.037 (1.141)	loss 10.559 (10.559)	gnorm 504823.625 (504823.625)	prob 0.893 (0.8928)	GS 31.406 (31.406)	mem 42.870
Train: [3][140/750]	BT 3.975 (1.296)	DT 3.870 (1.161)	loss 10.722 (10.722)	gnorm 537732.250 (537732.250)	prob 0.143 (0.1433)	GS 33.406 (33.406)	mem 42.929
Train: [3][141/750]	BT 0.200 (1.289)	DT 0.002 (1.153)	loss 9.833 (9.833)	gnorm 543411.750 (543411.750)	prob 1.700 (1.7001)	GS 33.875 (33.875)	mem 42.930
Train: [3][142/750]	BT 0.217 (1.281)	DT 0.020 (1.145)	loss 10.358 (10.358)	gnorm 489151.031 (489151.031)	prob 1.121 (1.1205)	GS 34.844 (34.844)	mem 42.931
Train: [3][143/750]	BT 0.114 (1.273)	DT 0.016 (1.137)	loss 10.515 (10.515)	gnorm 496839.125 (496839.125)	prob 1.445 (1.4455)	GS 30.297 (30.297)	mem 42.931
Train: [3][144/750]	BT 0.121 (1.265)	DT 0.002 (1.129)	loss 9.887 (9.887)	gnorm 523261.031 (523261.031)	prob 1.973 (1.9727)	GS 32.812 (32.812)	mem 42.930
Train: [3][145/750]	BT 0.081 (1.257)	DT 0.001 (1.121)	loss 9.522 (9.522)	gnorm 526074.250 (526074.250)	prob 2.215 (2.2155)	GS 35.219 (35.219)	mem 42.931
Train: [3][146/750]	BT 5.967 (1.289)	DT 5.674 (1.152)	loss 9.647 (9.647)	gnorm 522247.875 (522247.875)	prob 2.231 (2.2311)	GS 32.875 (32.875)	mem 42.994
Train: [3][147/750]	BT 0.192 (1.282)	DT 0.009 (1.144)	loss 10.333 (10.333)	gnorm 507088.469 (507088.469)	prob 2.398 (2.3983)	GS 30.844 (30.844)	mem 42.936
Train: [3][148/750]	BT 0.158 (1.274)	DT 0.006 (1.137)	loss 9.780 (9.780)	gnorm 519714.156 (519714.156)	prob 2.954 (2.9542)	GS 35.922 (35.922)	mem 42.936
Train: [3][149/750]	BT 0.242 (1.267)	DT 0.002 (1.129)	loss 10.065 (10.065)	gnorm 549224.125 (549224.125)	prob 2.007 (2.0073)	GS 29.703 (29.703)	mem 42.936
Train: [3][150/750]	BT 0.166 (1.260)	DT 0.002 (1.122)	loss 9.723 (9.723)	gnorm 492585.688 (492585.688)	prob 2.093 (2.0929)	GS 31.516 (31.516)	mem 42.937
Train: [3][151/750]	BT 0.092 (1.252)	DT 0.003 (1.114)	loss 10.062 (10.062)	gnorm 500171.312 (500171.312)	prob 1.640 (1.6396)	GS 28.500 (28.500)	mem 42.939
Train: [3][152/750]	BT 5.626 (1.281)	DT 5.419 (1.143)	loss 9.780 (9.780)	gnorm 490290.906 (490290.906)	prob 1.770 (1.7700)	GS 31.703 (31.703)	mem 42.966
Train: [3][153/750]	BT 0.191 (1.274)	DT 0.002 (1.135)	loss 9.881 (9.881)	gnorm 521607.250 (521607.250)	prob 1.806 (1.8060)	GS 37.594 (37.594)	mem 43.009
Train: [3][154/750]	BT 0.145 (1.266)	DT 0.002 (1.128)	loss 10.470 (10.470)	gnorm 514687.500 (514687.500)	prob 0.869 (0.8691)	GS 29.266 (29.266)	mem 42.952
Train: [3][155/750]	BT 0.139 (1.259)	DT 0.002 (1.121)	loss 10.239 (10.239)	gnorm 499850.438 (499850.438)	prob 0.856 (0.8562)	GS 32.172 (32.172)	mem 42.952
Train: [3][156/750]	BT 0.199 (1.252)	DT 0.016 (1.113)	loss 10.116 (10.116)	gnorm 519806.531 (519806.531)	prob 1.258 (1.2578)	GS 28.281 (28.281)	mem 43.049
Train: [3][157/750]	BT 0.160 (1.245)	DT 0.011 (1.106)	loss 10.359 (10.359)	gnorm 535494.000 (535494.000)	prob 1.376 (1.3757)	GS 36.031 (36.031)	mem 42.954
Train: [3][158/750]	BT 7.631 (1.286)	DT 7.541 (1.147)	loss 10.416 (10.416)	gnorm 514225.000 (514225.000)	prob 0.618 (0.6184)	GS 31.969 (31.969)	mem 43.076
Train: [3][159/750]	BT 0.089 (1.278)	DT 0.002 (1.140)	loss 10.529 (10.529)	gnorm 500834.312 (500834.312)	prob 0.763 (0.7629)	GS 29.422 (29.422)	mem 43.076
Train: [3][160/750]	BT 0.103 (1.271)	DT 0.001 (1.133)	loss 10.038 (10.038)	gnorm 508624.938 (508624.938)	prob 1.432 (1.4317)	GS 34.422 (34.422)	mem 42.967
Train: [3][161/750]	BT 0.120 (1.264)	DT 0.004 (1.126)	loss 9.836 (9.836)	gnorm 540691.312 (540691.312)	prob 1.838 (1.8378)	GS 29.703 (29.703)	mem 42.967
Train: [3][162/750]	BT 0.086 (1.256)	DT 0.001 (1.119)	loss 10.501 (10.501)	gnorm 505322.594 (505322.594)	prob 0.678 (0.6781)	GS 32.656 (32.656)	mem 42.968
Train: [3][163/750]	BT 0.119 (1.249)	DT 0.002 (1.112)	loss 10.462 (10.462)	gnorm 543179.625 (543179.625)	prob 1.338 (1.3377)	GS 31.844 (31.844)	mem 42.968
Train: [3][164/750]	BT 2.309 (1.256)	DT 2.206 (1.119)	loss 10.145 (10.145)	gnorm 523416.406 (523416.406)	prob 1.241 (1.2409)	GS 29.703 (29.703)	mem 43.013
Train: [3][165/750]	BT 0.123 (1.249)	DT 0.002 (1.112)	loss 10.434 (10.434)	gnorm 534885.000 (534885.000)	prob 1.447 (1.4465)	GS 33.703 (33.703)	mem 43.013
Train: [3][166/750]	BT 0.174 (1.243)	DT 0.002 (1.105)	loss 9.873 (9.873)	gnorm 518321.906 (518321.906)	prob 1.510 (1.5104)	GS 34.516 (34.516)	mem 43.062
Train: [3][167/750]	BT 0.161 (1.236)	DT 0.031 (1.099)	loss 10.135 (10.135)	gnorm 543728.500 (543728.500)	prob 1.663 (1.6628)	GS 34.969 (34.969)	mem 43.097
Train: [3][168/750]	BT 0.131 (1.229)	DT 0.003 (1.092)	loss 9.964 (9.964)	gnorm 451550.281 (451550.281)	prob 1.500 (1.5004)	GS 32.094 (32.094)	mem 43.200
Train: [3][169/750]	BT 0.086 (1.223)	DT 0.003 (1.086)	loss 9.982 (9.982)	gnorm 536021.500 (536021.500)	prob 1.684 (1.6836)	GS 30.750 (30.750)	mem 43.390
Train: [3][170/750]	BT 11.207 (1.281)	DT 11.084 (1.145)	loss 10.220 (10.220)	gnorm 502260.688 (502260.688)	prob 1.960 (1.9605)	GS 30.125 (30.125)	mem 43.394
Train: [3][171/750]	BT 0.177 (1.275)	DT 0.035 (1.138)	loss 10.879 (10.879)	gnorm 600551.688 (600551.688)	prob 0.988 (0.9884)	GS 30.203 (30.203)	mem 43.349
Train: [3][172/750]	BT 0.145 (1.268)	DT 0.002 (1.132)	loss 10.065 (10.065)	gnorm 560654.312 (560654.312)	prob 1.618 (1.6179)	GS 33.703 (33.703)	mem 43.212
Train: [3][173/750]	BT 0.105 (1.262)	DT 0.003 (1.125)	loss 10.139 (10.139)	gnorm 553302.125 (553302.125)	prob 1.943 (1.9432)	GS 34.766 (34.766)	mem 43.213
Train: [3][174/750]	BT 0.161 (1.255)	DT 0.004 (1.119)	loss 9.642 (9.642)	gnorm 536811.438 (536811.438)	prob 2.596 (2.5962)	GS 35.188 (35.188)	mem 43.249
Train: [3][175/750]	BT 0.141 (1.249)	DT 0.010 (1.112)	loss 10.432 (10.432)	gnorm 490782.969 (490782.969)	prob 1.894 (1.8941)	GS 30.266 (30.266)	mem 43.259
Train: [3][176/750]	BT 1.675 (1.251)	DT 1.570 (1.115)	loss 9.887 (9.887)	gnorm 535560.562 (535560.562)	prob 2.242 (2.2416)	GS 38.328 (38.328)	mem 43.292
Train: [3][177/750]	BT 0.142 (1.245)	DT 0.012 (1.109)	loss 10.430 (10.430)	gnorm 535043.250 (535043.250)	prob 2.316 (2.3162)	GS 39.438 (39.438)	mem 43.186
Train: [3][178/750]	BT 0.201 (1.239)	DT 0.007 (1.102)	loss 10.106 (10.106)	gnorm 496584.750 (496584.750)	prob 2.386 (2.3859)	GS 34.562 (34.562)	mem 43.226
Train: [3][179/750]	BT 0.179 (1.233)	DT 0.002 (1.096)	loss 9.812 (9.812)	gnorm 537245.312 (537245.312)	prob 2.829 (2.8286)	GS 29.469 (29.469)	mem 43.233
Train: [3][180/750]	BT 0.105 (1.227)	DT 0.005 (1.090)	loss 9.957 (9.957)	gnorm 508557.406 (508557.406)	prob 2.226 (2.2259)	GS 35.531 (35.531)	mem 43.191
Train: [3][181/750]	BT 0.136 (1.221)	DT 0.001 (1.084)	loss 10.112 (10.112)	gnorm 532475.562 (532475.562)	prob 1.649 (1.6487)	GS 30.781 (30.781)	mem 43.191
Train: [3][182/750]	BT 9.114 (1.264)	DT 8.942 (1.127)	loss 10.211 (10.211)	gnorm 522623.688 (522623.688)	prob 2.204 (2.2041)	GS 31.000 (31.000)	mem 43.140
Train: [3][183/750]	BT 0.099 (1.258)	DT 0.004 (1.121)	loss 10.160 (10.160)	gnorm 495960.844 (495960.844)	prob 1.493 (1.4926)	GS 32.141 (32.141)	mem 43.105
Train: [3][184/750]	BT 0.090 (1.252)	DT 0.002 (1.115)	loss 9.365 (9.365)	gnorm 543076.062 (543076.062)	prob 1.901 (1.9009)	GS 31.562 (31.562)	mem 43.105
Train: [3][185/750]	BT 0.138 (1.246)	DT 0.009 (1.109)	loss 10.285 (10.285)	gnorm 510306.969 (510306.969)	prob 1.528 (1.5283)	GS 30.516 (30.516)	mem 43.242
Train: [3][186/750]	BT 0.193 (1.240)	DT 0.001 (1.103)	loss 10.663 (10.663)	gnorm 575842.375 (575842.375)	prob 0.376 (0.3756)	GS 32.438 (32.438)	mem 43.241
Train: [3][187/750]	BT 0.087 (1.234)	DT 0.002 (1.097)	loss 10.337 (10.337)	gnorm 551802.125 (551802.125)	prob 1.299 (1.2988)	GS 31.531 (31.531)	mem 43.124
Train: [3][188/750]	BT 6.590 (1.262)	DT 6.517 (1.126)	loss 10.302 (10.302)	gnorm 512252.219 (512252.219)	prob 1.408 (1.4079)	GS 32.719 (32.719)	mem 43.112
Train: [3][189/750]	BT 0.086 (1.256)	DT 0.002 (1.120)	loss 10.085 (10.085)	gnorm 491941.938 (491941.938)	prob 1.409 (1.4092)	GS 32.391 (32.391)	mem 43.112
Train: [3][190/750]	BT 0.349 (1.251)	DT 0.002 (1.114)	loss 10.008 (10.008)	gnorm 533928.438 (533928.438)	prob 1.526 (1.5263)	GS 36.281 (36.281)	mem 43.256
Train: [3][191/750]	BT 0.116 (1.245)	DT 0.025 (1.109)	loss 10.579 (10.579)	gnorm 499872.594 (499872.594)	prob 0.470 (0.4703)	GS 30.359 (30.359)	mem 43.341
Train: [3][192/750]	BT 0.105 (1.239)	DT 0.001 (1.103)	loss 10.590 (10.590)	gnorm 488552.000 (488552.000)	prob 1.092 (1.0919)	GS 32.359 (32.359)	mem 43.449
Train: [3][193/750]	BT 0.142 (1.234)	DT 0.004 (1.097)	loss 10.160 (10.160)	gnorm 509428.469 (509428.469)	prob 1.693 (1.6925)	GS 29.109 (29.109)	mem 43.382
Train: [3][194/750]	BT 6.394 (1.260)	DT 6.293 (1.124)	loss 10.114 (10.114)	gnorm 530666.812 (530666.812)	prob 1.297 (1.2973)	GS 33.375 (33.375)	mem 43.127
Train: [3][195/750]	BT 0.088 (1.254)	DT 0.002 (1.118)	loss 10.184 (10.184)	gnorm 498089.656 (498089.656)	prob 0.476 (0.4763)	GS 31.078 (31.078)	mem 43.126
Train: [3][196/750]	BT 0.077 (1.248)	DT 0.002 (1.112)	loss 9.798 (9.798)	gnorm 502642.125 (502642.125)	prob 1.183 (1.1829)	GS 31.797 (31.797)	mem 43.127
Train: [3][197/750]	BT 0.129 (1.243)	DT 0.002 (1.107)	loss 10.632 (10.632)	gnorm 540718.938 (540718.938)	prob 0.300 (0.2997)	GS 29.000 (29.000)	mem 43.127
Train: [3][198/750]	BT 0.124 (1.237)	DT 0.003 (1.101)	loss 9.851 (9.851)	gnorm 487841.438 (487841.438)	prob 0.871 (0.8706)	GS 33.812 (33.812)	mem 43.127
Train: [3][199/750]	BT 0.160 (1.232)	DT 0.002 (1.096)	loss 10.463 (10.463)	gnorm 518612.469 (518612.469)	prob 0.495 (0.4951)	GS 32.078 (32.078)	mem 43.127
Train: [3][200/750]	BT 1.682 (1.234)	DT 1.374 (1.097)	loss 9.448 (9.448)	gnorm 518247.781 (518247.781)	prob 1.947 (1.9471)	GS 30.828 (30.828)	mem 43.300
Train: [3][201/750]	BT 0.176 (1.229)	DT 0.002 (1.092)	loss 10.232 (10.232)	gnorm 510683.344 (510683.344)	prob 1.483 (1.4825)	GS 31.625 (31.625)	mem 43.178
Train: [3][202/750]	BT 0.078 (1.223)	DT 0.003 (1.086)	loss 10.127 (10.127)	gnorm 542590.125 (542590.125)	prob 1.164 (1.1641)	GS 32.859 (32.859)	mem 43.070
Train: [3][203/750]	BT 0.092 (1.217)	DT 0.002 (1.081)	loss 10.422 (10.422)	gnorm 472989.688 (472989.688)	prob 0.774 (0.7742)	GS 32.844 (32.844)	mem 43.072
Train: [3][204/750]	BT 0.082 (1.212)	DT 0.002 (1.076)	loss 10.361 (10.361)	gnorm 526520.688 (526520.688)	prob 1.062 (1.0622)	GS 35.141 (35.141)	mem 43.188
Train: [3][205/750]	BT 0.255 (1.207)	DT 0.002 (1.070)	loss 10.333 (10.333)	gnorm 500862.656 (500862.656)	prob 1.049 (1.0493)	GS 30.531 (30.531)	mem 43.284
Train: [3][206/750]	BT 8.563 (1.243)	DT 8.458 (1.106)	loss 9.812 (9.812)	gnorm 485171.594 (485171.594)	prob 1.766 (1.7657)	GS 31.156 (31.156)	mem 43.203
Train: [3][207/750]	BT 0.092 (1.237)	DT 0.008 (1.101)	loss 10.522 (10.522)	gnorm 529468.812 (529468.812)	prob 1.101 (1.1007)	GS 30.891 (30.891)	mem 43.204
Train: [3][208/750]	BT 0.080 (1.232)	DT 0.002 (1.096)	loss 9.926 (9.926)	gnorm 506092.719 (506092.719)	prob 1.676 (1.6755)	GS 32.953 (32.953)	mem 43.341
Train: [3][209/750]	BT 0.254 (1.227)	DT 0.010 (1.091)	loss 10.470 (10.470)	gnorm 547668.250 (547668.250)	prob 0.525 (0.5247)	GS 33.656 (33.656)	mem 43.353
Train: [3][210/750]	BT 0.104 (1.222)	DT 0.006 (1.085)	loss 9.959 (9.959)	gnorm 555599.125 (555599.125)	prob 1.008 (1.0077)	GS 36.641 (36.641)	mem 43.207
Train: [3][211/750]	BT 0.100 (1.216)	DT 0.001 (1.080)	loss 10.201 (10.201)	gnorm 567776.250 (567776.250)	prob 0.909 (0.9090)	GS 29.578 (29.578)	mem 43.212
Train: [3][212/750]	BT 3.566 (1.227)	DT 3.439 (1.091)	loss 10.078 (10.078)	gnorm 510781.906 (510781.906)	prob 0.935 (0.9346)	GS 33.359 (33.359)	mem 43.298
Train: [3][213/750]	BT 0.124 (1.222)	DT 0.017 (1.086)	loss 10.727 (10.727)	gnorm 555272.438 (555272.438)	prob 1.128 (1.1283)	GS 31.422 (31.422)	mem 43.172
Train: [3][214/750]	BT 0.114 (1.217)	DT 0.002 (1.081)	loss 10.022 (10.022)	gnorm 517967.344 (517967.344)	prob 2.117 (2.1174)	GS 32.438 (32.438)	mem 43.180
Train: [3][215/750]	BT 0.142 (1.212)	DT 0.003 (1.076)	loss 10.236 (10.236)	gnorm 541070.375 (541070.375)	prob 1.497 (1.4973)	GS 35.297 (35.297)	mem 43.188
Train: [3][216/750]	BT 0.087 (1.207)	DT 0.002 (1.071)	loss 10.271 (10.271)	gnorm 558407.125 (558407.125)	prob 1.228 (1.2281)	GS 38.000 (38.000)	mem 43.188
Train: [3][217/750]	BT 0.076 (1.202)	DT 0.002 (1.066)	loss 10.306 (10.306)	gnorm 568115.750 (568115.750)	prob 1.646 (1.6461)	GS 28.922 (28.922)	mem 43.188
Train: [3][218/750]	BT 5.564 (1.222)	DT 5.365 (1.086)	loss 9.756 (9.756)	gnorm 447106.500 (447106.500)	prob 1.630 (1.6302)	GS 34.906 (34.906)	mem 43.164
Train: [3][219/750]	BT 0.177 (1.217)	DT 0.004 (1.081)	loss 9.796 (9.796)	gnorm 511786.719 (511786.719)	prob 2.732 (2.7316)	GS 36.406 (36.406)	mem 43.200
Train: [3][220/750]	BT 0.090 (1.212)	DT 0.003 (1.076)	loss 9.957 (9.957)	gnorm 494747.844 (494747.844)	prob 2.242 (2.2419)	GS 34.797 (34.797)	mem 43.185
Train: [3][221/750]	BT 0.116 (1.207)	DT 0.002 (1.071)	loss 10.401 (10.401)	gnorm 522618.688 (522618.688)	prob 1.982 (1.9816)	GS 31.734 (31.734)	mem 43.210
Train: [3][222/750]	BT 0.101 (1.202)	DT 0.002 (1.066)	loss 9.607 (9.607)	gnorm 519962.719 (519962.719)	prob 2.796 (2.7962)	GS 30.469 (30.469)	mem 43.331
Train: [3][223/750]	BT 0.261 (1.198)	DT 0.003 (1.062)	loss 10.003 (10.003)	gnorm 488792.750 (488792.750)	prob 2.895 (2.8951)	GS 28.969 (28.969)	mem 43.318
Train: [3][224/750]	BT 5.604 (1.217)	DT 5.489 (1.081)	loss 10.071 (10.071)	gnorm 500889.000 (500889.000)	prob 1.847 (1.8466)	GS 35.969 (35.969)	mem 43.301
Train: [3][225/750]	BT 0.095 (1.212)	DT 0.001 (1.077)	loss 10.223 (10.223)	gnorm 535699.938 (535699.938)	prob 2.139 (2.1388)	GS 33.484 (33.484)	mem 43.301
Train: [3][226/750]	BT 0.139 (1.208)	DT 0.007 (1.072)	loss 10.482 (10.482)	gnorm 554098.062 (554098.062)	prob 2.063 (2.0633)	GS 33.422 (33.422)	mem 43.301
Train: [3][227/750]	BT 0.078 (1.203)	DT 0.002 (1.067)	loss 10.174 (10.174)	gnorm 555569.812 (555569.812)	prob 2.659 (2.6591)	GS 30.219 (30.219)	mem 43.300
Train: [3][228/750]	BT 0.184 (1.198)	DT 0.001 (1.063)	loss 10.008 (10.008)	gnorm 507009.281 (507009.281)	prob 2.003 (2.0025)	GS 36.219 (36.219)	mem 43.300
Train: [3][229/750]	BT 0.111 (1.193)	DT 0.002 (1.058)	loss 9.900 (9.900)	gnorm 516356.875 (516356.875)	prob 2.365 (2.3649)	GS 33.500 (33.500)	mem 43.301
Train: [3][230/750]	BT 6.073 (1.215)	DT 5.980 (1.079)	loss 10.008 (10.008)	gnorm 561455.062 (561455.062)	prob 1.588 (1.5876)	GS 32.641 (32.641)	mem 43.522
Train: [3][231/750]	BT 0.094 (1.210)	DT 0.002 (1.075)	loss 10.803 (10.803)	gnorm 567402.125 (567402.125)	prob 1.099 (1.0987)	GS 28.875 (28.875)	mem 43.491
Train: [3][232/750]	BT 0.174 (1.205)	DT 0.003 (1.070)	loss 10.189 (10.189)	gnorm 519448.625 (519448.625)	prob 1.184 (1.1843)	GS 32.391 (32.391)	mem 43.572
Train: [3][233/750]	BT 0.152 (1.201)	DT 0.002 (1.065)	loss 9.779 (9.779)	gnorm 546630.312 (546630.312)	prob 1.848 (1.8483)	GS 32.031 (32.031)	mem 43.573
Train: [3][234/750]	BT 0.169 (1.196)	DT 0.019 (1.061)	loss 10.113 (10.113)	gnorm 495278.156 (495278.156)	prob 1.519 (1.5186)	GS 33.531 (33.531)	mem 43.467
Train: [3][235/750]	BT 0.164 (1.192)	DT 0.002 (1.056)	loss 10.506 (10.506)	gnorm 536493.938 (536493.938)	prob 1.674 (1.6738)	GS 29.719 (29.719)	mem 43.506
Train: [3][236/750]	BT 5.738 (1.211)	DT 5.615 (1.076)	loss 10.065 (10.065)	gnorm 505399.625 (505399.625)	prob 2.098 (2.0981)	GS 33.703 (33.703)	mem 43.508
Train: [3][237/750]	BT 0.270 (1.207)	DT 0.034 (1.071)	loss 9.842 (9.842)	gnorm 480020.188 (480020.188)	prob 2.055 (2.0551)	GS 31.000 (31.000)	mem 43.509
Train: [3][238/750]	BT 0.153 (1.203)	DT 0.005 (1.067)	loss 10.251 (10.251)	gnorm 530531.812 (530531.812)	prob 1.590 (1.5900)	GS 30.562 (30.562)	mem 43.509
Train: [3][239/750]	BT 0.152 (1.198)	DT 0.011 (1.063)	loss 9.548 (9.548)	gnorm 490891.562 (490891.562)	prob 1.702 (1.7022)	GS 29.969 (29.969)	mem 43.510
Train: [3][240/750]	BT 0.155 (1.194)	DT 0.008 (1.058)	loss 10.064 (10.064)	gnorm 486499.969 (486499.969)	prob 0.954 (0.9538)	GS 35.719 (35.719)	mem 43.509
Train: [3][241/750]	BT 0.150 (1.190)	DT 0.002 (1.054)	loss 9.906 (9.906)	gnorm 520224.250 (520224.250)	prob 1.057 (1.0569)	GS 33.312 (33.312)	mem 43.511
Train: [3][242/750]	BT 1.539 (1.191)	DT 1.341 (1.055)	loss 9.971 (9.971)	gnorm 478024.062 (478024.062)	prob 1.440 (1.4397)	GS 32.312 (32.312)	mem 43.515
Train: [3][243/750]	BT 0.112 (1.187)	DT 0.002 (1.051)	loss 9.675 (9.675)	gnorm 526123.438 (526123.438)	prob 2.258 (2.2578)	GS 40.031 (40.031)	mem 43.515
Train: [3][244/750]	BT 1.339 (1.187)	DT 1.213 (1.051)	loss 9.977 (9.977)	gnorm 475028.219 (475028.219)	prob 1.559 (1.5592)	GS 37.516 (37.516)	mem 43.647
Train: [3][245/750]	BT 0.090 (1.183)	DT 0.002 (1.047)	loss 10.823 (10.823)	gnorm 539256.375 (539256.375)	prob 0.143 (0.1433)	GS 31.891 (31.891)	mem 43.622
Train: [3][246/750]	BT 0.089 (1.178)	DT 0.003 (1.043)	loss 10.048 (10.048)	gnorm 552278.500 (552278.500)	prob 1.379 (1.3789)	GS 33.484 (33.484)	mem 43.622
Train: [3][247/750]	BT 0.090 (1.174)	DT 0.003 (1.039)	loss 10.029 (10.029)	gnorm 518999.156 (518999.156)	prob 1.849 (1.8492)	GS 33.969 (33.969)	mem 43.517
Train: [3][248/750]	BT 10.441 (1.211)	DT 10.312 (1.076)	loss 10.499 (10.499)	gnorm 557240.375 (557240.375)	prob 1.248 (1.2478)	GS 33.109 (33.109)	mem 43.532
Train: [3][249/750]	BT 0.098 (1.207)	DT 0.002 (1.072)	loss 10.194 (10.194)	gnorm 505674.531 (505674.531)	prob 1.530 (1.5298)	GS 29.375 (29.375)	mem 43.551
Train: [3][250/750]	BT 0.087 (1.202)	DT 0.003 (1.067)	loss 10.421 (10.421)	gnorm 521624.688 (521624.688)	prob 0.728 (0.7279)	GS 25.125 (25.125)	mem 43.468
Train: [3][251/750]	BT 0.171 (1.198)	DT 0.014 (1.063)	loss 10.327 (10.327)	gnorm 576104.188 (576104.188)	prob 1.530 (1.5302)	GS 29.797 (29.797)	mem 43.467
Train: [3][252/750]	BT 0.076 (1.194)	DT 0.002 (1.059)	loss 10.703 (10.703)	gnorm 519487.438 (519487.438)	prob 0.020 (0.0196)	GS 34.094 (34.094)	mem 43.468
Train: [3][253/750]	BT 0.077 (1.189)	DT 0.002 (1.055)	loss 10.097 (10.097)	gnorm 499783.625 (499783.625)	prob 1.450 (1.4499)	GS 30.641 (30.641)	mem 43.468
Train: [3][254/750]	BT 0.086 (1.185)	DT 0.002 (1.051)	loss 10.104 (10.104)	gnorm 523081.844 (523081.844)	prob 1.154 (1.1544)	GS 32.719 (32.719)	mem 43.468
Train: [3][255/750]	BT 0.122 (1.181)	DT 0.002 (1.046)	loss 10.841 (10.841)	gnorm 528826.312 (528826.312)	prob 0.703 (0.7033)	GS 28.703 (28.703)	mem 43.483
Train: [3][256/750]	BT 2.251 (1.185)	DT 2.060 (1.050)	loss 10.685 (10.685)	gnorm 514554.656 (514554.656)	prob 0.777 (0.7768)	GS 35.656 (35.656)	mem 43.597
Train: [3][257/750]	BT 0.108 (1.181)	DT 0.006 (1.046)	loss 10.289 (10.289)	gnorm 508608.250 (508608.250)	prob 1.701 (1.7015)	GS 38.375 (38.375)	mem 43.597
Train: [3][258/750]	BT 0.124 (1.177)	DT 0.014 (1.042)	loss 9.647 (9.647)	gnorm 488798.844 (488798.844)	prob 2.424 (2.4244)	GS 33.891 (33.891)	mem 43.513
Train: [3][259/750]	BT 0.218 (1.173)	DT 0.005 (1.038)	loss 10.366 (10.366)	gnorm 482069.875 (482069.875)	prob 1.438 (1.4376)	GS 30.297 (30.297)	mem 43.520
Train: [3][260/750]	BT 12.865 (1.218)	DT 12.737 (1.083)	loss 10.067 (10.067)	gnorm 518934.938 (518934.938)	prob 1.060 (1.0603)	GS 35.328 (35.328)	mem 43.540
Train: [3][261/750]	BT 0.071 (1.214)	DT 0.003 (1.079)	loss 9.570 (9.570)	gnorm 472635.219 (472635.219)	prob 1.587 (1.5873)	GS 31.328 (31.328)	mem 43.540
Train: [3][262/750]	BT 0.079 (1.209)	DT 0.002 (1.075)	loss 9.849 (9.849)	gnorm 547220.938 (547220.938)	prob 1.387 (1.3865)	GS 36.766 (36.766)	mem 43.588
Train: [3][263/750]	BT 0.130 (1.205)	DT 0.002 (1.071)	loss 10.101 (10.101)	gnorm 515203.750 (515203.750)	prob 0.941 (0.9406)	GS 29.391 (29.391)	mem 43.589
Train: [3][264/750]	BT 0.165 (1.201)	DT 0.008 (1.067)	loss 9.616 (9.616)	gnorm 530764.688 (530764.688)	prob 1.496 (1.4955)	GS 32.609 (32.609)	mem 43.659
Train: [3][265/750]	BT 0.091 (1.197)	DT 0.024 (1.063)	loss 10.007 (10.007)	gnorm 503412.562 (503412.562)	prob 2.050 (2.0496)	GS 29.266 (29.266)	mem 43.590
Train: [3][266/750]	BT 0.095 (1.193)	DT 0.002 (1.059)	loss 9.925 (9.925)	gnorm 480017.188 (480017.188)	prob 2.349 (2.3494)	GS 32.547 (32.547)	mem 43.707
Train: [3][267/750]	BT 0.183 (1.189)	DT 0.007 (1.055)	loss 10.417 (10.417)	gnorm 552193.250 (552193.250)	prob 1.679 (1.6791)	GS 30.859 (30.859)	mem 43.635
Train: [3][268/750]	BT 2.295 (1.193)	DT 2.069 (1.059)	loss 9.955 (9.955)	gnorm 526734.438 (526734.438)	prob 2.508 (2.5083)	GS 31.500 (31.500)	mem 43.577
Train: [3][269/750]	BT 0.093 (1.189)	DT 0.014 (1.055)	loss 10.446 (10.446)	gnorm 488807.500 (488807.500)	prob 1.858 (1.8585)	GS 31.312 (31.312)	mem 43.546
Train: [3][270/750]	BT 0.139 (1.185)	DT 0.002 (1.051)	loss 10.510 (10.510)	gnorm 516085.469 (516085.469)	prob 1.844 (1.8437)	GS 30.797 (30.797)	mem 43.597
Train: [3][271/750]	BT 0.094 (1.181)	DT 0.003 (1.047)	loss 9.988 (9.988)	gnorm 507085.656 (507085.656)	prob 2.298 (2.2978)	GS 30.609 (30.609)	mem 43.578
Train: [3][272/750]	BT 9.252 (1.211)	DT 9.144 (1.077)	loss 9.918 (9.918)	gnorm 459025.125 (459025.125)	prob 1.827 (1.8268)	GS 33.703 (33.703)	mem 43.529
Train: [3][273/750]	BT 0.201 (1.207)	DT 0.024 (1.073)	loss 9.970 (9.970)	gnorm 483975.000 (483975.000)	prob 2.325 (2.3246)	GS 32.609 (32.609)	mem 43.626
Train: [3][274/750]	BT 0.148 (1.203)	DT 0.003 (1.069)	loss 10.075 (10.075)	gnorm 500697.312 (500697.312)	prob 2.348 (2.3484)	GS 31.734 (31.734)	mem 43.531
Train: [3][275/750]	BT 0.112 (1.199)	DT 0.001 (1.065)	loss 10.696 (10.696)	gnorm 486060.156 (486060.156)	prob 1.638 (1.6380)	GS 28.969 (28.969)	mem 43.532
Train: [3][276/750]	BT 0.149 (1.196)	DT 0.002 (1.062)	loss 10.242 (10.242)	gnorm 492175.125 (492175.125)	prob 1.913 (1.9128)	GS 31.766 (31.766)	mem 43.531
Train: [3][277/750]	BT 0.148 (1.192)	DT 0.001 (1.058)	loss 10.157 (10.157)	gnorm 469508.469 (469508.469)	prob 2.303 (2.3032)	GS 31.750 (31.750)	mem 43.530
Train: [3][278/750]	BT 0.176 (1.188)	DT 0.002 (1.054)	loss 10.012 (10.012)	gnorm 443252.844 (443252.844)	prob 2.237 (2.2371)	GS 32.219 (32.219)	mem 43.530
Train: [3][279/750]	BT 0.165 (1.185)	DT 0.002 (1.050)	loss 9.965 (9.965)	gnorm 460001.625 (460001.625)	prob 2.053 (2.0528)	GS 33.516 (33.516)	mem 43.530
Train: [3][280/750]	BT 0.150 (1.181)	DT 0.048 (1.047)	loss 9.947 (9.947)	gnorm 454694.594 (454694.594)	prob 1.955 (1.9551)	GS 31.281 (31.281)	mem 43.530
Train: [3][281/750]	BT 0.151 (1.177)	DT 0.003 (1.043)	loss 10.129 (10.129)	gnorm 478601.281 (478601.281)	prob 2.132 (2.1315)	GS 29.375 (29.375)	mem 43.531
Train: [3][282/750]	BT 0.093 (1.173)	DT 0.002 (1.039)	loss 10.264 (10.264)	gnorm 470817.625 (470817.625)	prob 1.996 (1.9964)	GS 34.234 (34.234)	mem 43.530
Train: [3][283/750]	BT 0.111 (1.170)	DT 0.002 (1.035)	loss 10.504 (10.504)	gnorm 515058.156 (515058.156)	prob 1.847 (1.8470)	GS 34.438 (34.438)	mem 43.583
Train: [3][284/750]	BT 12.599 (1.210)	DT 12.487 (1.076)	loss 9.758 (9.758)	gnorm 493635.688 (493635.688)	prob 2.278 (2.2779)	GS 34.828 (34.828)	mem 43.545
Train: [3][285/750]	BT 0.091 (1.206)	DT 0.003 (1.072)	loss 10.217 (10.217)	gnorm 553579.875 (553579.875)	prob 1.997 (1.9969)	GS 27.781 (27.781)	mem 43.580
Train: [3][286/750]	BT 0.084 (1.202)	DT 0.003 (1.068)	loss 10.302 (10.302)	gnorm 536396.375 (536396.375)	prob 1.604 (1.6037)	GS 35.984 (35.984)	mem 43.742
Train: [3][287/750]	BT 0.218 (1.199)	DT 0.017 (1.065)	loss 10.241 (10.241)	gnorm 521091.812 (521091.812)	prob 1.314 (1.3144)	GS 30.141 (30.141)	mem 43.671
Train: [3][288/750]	BT 0.208 (1.195)	DT 0.012 (1.061)	loss 10.509 (10.509)	gnorm 523640.375 (523640.375)	prob 0.627 (0.6265)	GS 34.281 (34.281)	mem 43.622
Train: [3][289/750]	BT 0.122 (1.191)	DT 0.006 (1.057)	loss 9.960 (9.960)	gnorm 547057.312 (547057.312)	prob 1.039 (1.0387)	GS 30.062 (30.062)	mem 43.631
Train: [3][290/750]	BT 0.125 (1.188)	DT 0.010 (1.054)	loss 9.916 (9.916)	gnorm 482776.062 (482776.062)	prob 0.728 (0.7283)	GS 34.812 (34.812)	mem 43.591
Train: [3][291/750]	BT 0.093 (1.184)	DT 0.002 (1.050)	loss 10.079 (10.079)	gnorm 536385.750 (536385.750)	prob 0.950 (0.9505)	GS 30.672 (30.672)	mem 43.547
Train: [3][292/750]	BT 3.256 (1.191)	DT 3.093 (1.057)	loss 10.671 (10.671)	gnorm 528138.938 (528138.938)	prob 0.212 (0.2118)	GS 34.531 (34.531)	mem 43.491
Train: [3][293/750]	BT 0.118 (1.187)	DT 0.011 (1.054)	loss 10.470 (10.470)	gnorm 512155.812 (512155.812)	prob 1.158 (1.1579)	GS 31.016 (31.016)	mem 43.491
Train: [3][294/750]	BT 0.112 (1.184)	DT 0.002 (1.050)	loss 9.413 (9.413)	gnorm 513568.969 (513568.969)	prob 1.229 (1.2295)	GS 31.062 (31.062)	mem 43.493
Train: [3][295/750]	BT 0.141 (1.180)	DT 0.010 (1.046)	loss 11.118 (11.118)	gnorm 563676.625 (563676.625)	prob 0.545 (0.5449)	GS 33.750 (33.750)	mem 43.590
Train: [3][296/750]	BT 9.222 (1.207)	DT 9.113 (1.074)	loss 10.456 (10.456)	gnorm 521693.562 (521693.562)	prob 1.226 (1.2258)	GS 34.594 (34.594)	mem 43.501
Train: [3][297/750]	BT 0.076 (1.204)	DT 0.002 (1.070)	loss 10.355 (10.355)	gnorm 579560.375 (579560.375)	prob 1.050 (1.0497)	GS 35.406 (35.406)	mem 43.500
Train: [3][298/750]	BT 0.107 (1.200)	DT 0.001 (1.066)	loss 10.483 (10.483)	gnorm 515327.000 (515327.000)	prob 0.653 (0.6534)	GS 34.594 (34.594)	mem 43.501
Train: [3][299/750]	BT 0.104 (1.196)	DT 0.006 (1.063)	loss 10.135 (10.135)	gnorm 449124.156 (449124.156)	prob 1.048 (1.0479)	GS 34.828 (34.828)	mem 43.500
Train: [3][300/750]	BT 0.096 (1.193)	DT 0.002 (1.059)	loss 9.990 (9.990)	gnorm 511305.312 (511305.312)	prob 1.231 (1.2310)	GS 35.484 (35.484)	mem 43.502
Train: [3][301/750]	BT 0.140 (1.189)	DT 0.001 (1.056)	loss 10.105 (10.105)	gnorm 478164.250 (478164.250)	prob 0.829 (0.8290)	GS 32.312 (32.312)	mem 43.502
Train: [3][302/750]	BT 0.103 (1.185)	DT 0.002 (1.052)	loss 9.682 (9.682)	gnorm 478757.688 (478757.688)	prob 2.093 (2.0931)	GS 33.984 (33.984)	mem 43.501
Train: [3][303/750]	BT 0.143 (1.182)	DT 0.005 (1.049)	loss 9.704 (9.704)	gnorm 461485.000 (461485.000)	prob 1.258 (1.2578)	GS 28.297 (28.297)	mem 43.501
Train: [3][304/750]	BT 2.637 (1.187)	DT 2.391 (1.053)	loss 10.480 (10.480)	gnorm 482337.750 (482337.750)	prob 0.730 (0.7299)	GS 34.234 (34.234)	mem 43.557
Train: [3][305/750]	BT 0.144 (1.183)	DT 0.011 (1.050)	loss 9.700 (9.700)	gnorm 496866.938 (496866.938)	prob 1.597 (1.5971)	GS 34.516 (34.516)	mem 43.558
Train: [3][306/750]	BT 0.122 (1.180)	DT 0.004 (1.047)	loss 9.715 (9.715)	gnorm 472526.188 (472526.188)	prob 1.271 (1.2714)	GS 32.109 (32.109)	mem 43.589
Train: [3][307/750]	BT 0.103 (1.176)	DT 0.007 (1.043)	loss 10.623 (10.623)	gnorm 512273.906 (512273.906)	prob 0.690 (0.6903)	GS 32.234 (32.234)	mem 43.559
Train: [3][308/750]	BT 7.697 (1.198)	DT 7.521 (1.064)	loss 9.943 (9.943)	gnorm 522333.875 (522333.875)	prob 0.650 (0.6501)	GS 31.469 (31.469)	mem 43.590
Train: [3][309/750]	BT 0.124 (1.194)	DT 0.006 (1.061)	loss 10.461 (10.461)	gnorm 514624.938 (514624.938)	prob 0.556 (0.5556)	GS 29.719 (29.719)	mem 43.590
Train: [3][310/750]	BT 0.098 (1.191)	DT 0.001 (1.057)	loss 10.116 (10.116)	gnorm 498932.406 (498932.406)	prob 1.076 (1.0758)	GS 39.781 (39.781)	mem 43.590
Train: [3][311/750]	BT 0.263 (1.188)	DT 0.002 (1.054)	loss 10.466 (10.466)	gnorm 493756.562 (493756.562)	prob 1.456 (1.4561)	GS 32.562 (32.562)	mem 43.641
Train: [3][312/750]	BT 0.081 (1.184)	DT 0.001 (1.051)	loss 10.126 (10.126)	gnorm 468029.812 (468029.812)	prob 1.241 (1.2410)	GS 35.047 (35.047)	mem 43.685
Train: [3][313/750]	BT 0.159 (1.181)	DT 0.002 (1.047)	loss 10.434 (10.434)	gnorm 462706.062 (462706.062)	prob 1.414 (1.4136)	GS 29.984 (29.984)	mem 43.591
Train: [3][314/750]	BT 0.131 (1.177)	DT 0.003 (1.044)	loss 10.787 (10.787)	gnorm 498506.531 (498506.531)	prob 1.151 (1.1505)	GS 36.109 (36.109)	mem 43.592
Train: [3][315/750]	BT 0.187 (1.174)	DT 0.020 (1.041)	loss 10.306 (10.306)	gnorm 506987.125 (506987.125)	prob 1.710 (1.7095)	GS 31.109 (31.109)	mem 43.593
Train: [3][316/750]	BT 1.973 (1.177)	DT 1.866 (1.043)	loss 10.290 (10.290)	gnorm 506816.875 (506816.875)	prob 1.706 (1.7061)	GS 31.734 (31.734)	mem 43.618
Train: [3][317/750]	BT 0.186 (1.174)	DT 0.002 (1.040)	loss 10.400 (10.400)	gnorm 478733.156 (478733.156)	prob 1.009 (1.0091)	GS 35.594 (35.594)	mem 43.622
Train: [3][318/750]	BT 0.092 (1.170)	DT 0.002 (1.037)	loss 10.524 (10.524)	gnorm 488168.781 (488168.781)	prob 0.827 (0.8267)	GS 30.938 (30.938)	mem 43.717
Train: [3][319/750]	BT 0.105 (1.167)	DT 0.002 (1.033)	loss 10.012 (10.012)	gnorm 500916.344 (500916.344)	prob 1.265 (1.2652)	GS 30.719 (30.719)	mem 43.789
Train: [3][320/750]	BT 12.300 (1.202)	DT 12.133 (1.068)	loss 9.965 (9.965)	gnorm 501662.781 (501662.781)	prob 1.222 (1.2222)	GS 32.281 (32.281)	mem 43.566
Train: [3][321/750]	BT 0.135 (1.198)	DT 0.001 (1.065)	loss 10.119 (10.119)	gnorm 498209.875 (498209.875)	prob 0.712 (0.7121)	GS 32.641 (32.641)	mem 43.566
Train: [3][322/750]	BT 0.230 (1.195)	DT 0.002 (1.062)	loss 10.123 (10.123)	gnorm 479603.188 (479603.188)	prob 0.606 (0.6056)	GS 30.375 (30.375)	mem 43.693
Train: [3][323/750]	BT 0.137 (1.192)	DT 0.008 (1.058)	loss 10.365 (10.365)	gnorm 481205.188 (481205.188)	prob 1.061 (1.0613)	GS 27.594 (27.594)	mem 43.584
Train: [3][324/750]	BT 0.180 (1.189)	DT 0.001 (1.055)	loss 10.105 (10.105)	gnorm 470907.469 (470907.469)	prob 0.930 (0.9304)	GS 33.094 (33.094)	mem 43.586
Train: [3][325/750]	BT 0.112 (1.186)	DT 0.003 (1.052)	loss 9.886 (9.886)	gnorm 461739.438 (461739.438)	prob 1.039 (1.0389)	GS 34.875 (34.875)	mem 43.586
Train: [3][326/750]	BT 0.212 (1.183)	DT 0.007 (1.049)	loss 10.089 (10.089)	gnorm 477408.906 (477408.906)	prob 1.250 (1.2503)	GS 34.875 (34.875)	mem 43.611
Train: [3][327/750]	BT 0.179 (1.180)	DT 0.006 (1.045)	loss 9.397 (9.397)	gnorm 510135.562 (510135.562)	prob 1.571 (1.5710)	GS 25.422 (25.422)	mem 43.665
Train: [3][328/750]	BT 0.753 (1.178)	DT 0.606 (1.044)	loss 9.762 (9.762)	gnorm 475224.125 (475224.125)	prob 1.563 (1.5631)	GS 33.672 (33.672)	mem 43.587
Train: [3][329/750]	BT 0.279 (1.176)	DT 0.002 (1.041)	loss 10.240 (10.240)	gnorm 509694.406 (509694.406)	prob 0.942 (0.9420)	GS 34.281 (34.281)	mem 43.642
Train: [3][330/750]	BT 0.119 (1.172)	DT 0.009 (1.038)	loss 9.694 (9.694)	gnorm 457860.281 (457860.281)	prob 0.907 (0.9065)	GS 31.797 (31.797)	mem 43.591
Train: [3][331/750]	BT 0.122 (1.169)	DT 0.002 (1.035)	loss 10.584 (10.584)	gnorm 568519.188 (568519.188)	prob 0.168 (0.1680)	GS 49.688 (49.688)	mem 43.592
Train: [3][332/750]	BT 10.983 (1.199)	DT 10.865 (1.064)	loss 10.213 (10.213)	gnorm 483908.438 (483908.438)	prob 0.535 (0.5350)	GS 30.766 (30.766)	mem 43.652
Train: [3][333/750]	BT 0.123 (1.196)	DT 0.002 (1.061)	loss 10.579 (10.579)	gnorm 483790.500 (483790.500)	prob 0.284 (0.2839)	GS 29.953 (29.953)	mem 43.641
Train: [3][334/750]	BT 0.093 (1.192)	DT 0.001 (1.058)	loss 9.651 (9.651)	gnorm 446554.844 (446554.844)	prob 1.278 (1.2781)	GS 30.719 (30.719)	mem 43.546
Train: [3][335/750]	BT 0.131 (1.189)	DT 0.004 (1.055)	loss 10.281 (10.281)	gnorm 526748.250 (526748.250)	prob 0.806 (0.8060)	GS 31.188 (31.188)	mem 43.546
Train: [3][336/750]	BT 0.144 (1.186)	DT 0.007 (1.052)	loss 10.402 (10.402)	gnorm 472156.438 (472156.438)	prob 0.664 (0.6638)	GS 36.609 (36.609)	mem 43.547
Train: [3][337/750]	BT 0.108 (1.183)	DT 0.002 (1.048)	loss 10.091 (10.091)	gnorm 515104.031 (515104.031)	prob 0.957 (0.9572)	GS 31.125 (31.125)	mem 43.552
Train: [3][338/750]	BT 0.104 (1.180)	DT 0.002 (1.045)	loss 10.575 (10.575)	gnorm 552904.562 (552904.562)	prob 0.673 (0.6725)	GS 32.844 (32.844)	mem 43.586
Train: [3][339/750]	BT 0.170 (1.177)	DT 0.002 (1.042)	loss 9.889 (9.889)	gnorm 506416.312 (506416.312)	prob 0.520 (0.5198)	GS 27.953 (27.953)	mem 43.561
Train: [3][340/750]	BT 2.137 (1.179)	DT 2.026 (1.045)	loss 10.525 (10.525)	gnorm 490726.531 (490726.531)	prob -0.524 (-0.5241)	GS 34.484 (34.484)	mem 43.617
Train: [3][341/750]	BT 0.141 (1.176)	DT 0.005 (1.042)	loss 10.101 (10.101)	gnorm 527021.625 (527021.625)	prob 0.197 (0.1966)	GS 24.969 (24.969)	mem 43.580
Train: [3][342/750]	BT 0.121 (1.173)	DT 0.001 (1.039)	loss 9.980 (9.980)	gnorm 489629.375 (489629.375)	prob 0.018 (0.0181)	GS 35.781 (35.781)	mem 43.580
Train: [3][343/750]	BT 0.113 (1.170)	DT 0.001 (1.036)	loss 10.381 (10.381)	gnorm 484439.875 (484439.875)	prob -0.668 (-0.6678)	GS 27.000 (27.000)	mem 43.581
Train: [3][344/750]	BT 9.512 (1.194)	DT 9.429 (1.060)	loss 10.523 (10.523)	gnorm 456764.719 (456764.719)	prob -0.826 (-0.8264)	GS 35.922 (35.922)	mem 43.609
Train: [3][345/750]	BT 0.083 (1.191)	DT 0.002 (1.057)	loss 10.158 (10.158)	gnorm 509925.281 (509925.281)	prob 0.420 (0.4201)	GS 32.906 (32.906)	mem 43.610
Train: [3][346/750]	BT 0.165 (1.188)	DT 0.001 (1.054)	loss 9.888 (9.888)	gnorm 489778.969 (489778.969)	prob 0.424 (0.4242)	GS 32.891 (32.891)	mem 43.715
Train: [3][347/750]	BT 0.095 (1.185)	DT 0.002 (1.051)	loss 10.811 (10.811)	gnorm 519998.125 (519998.125)	prob 0.277 (0.2773)	GS 33.906 (33.906)	mem 43.715
Train: [3][348/750]	BT 0.094 (1.182)	DT 0.009 (1.048)	loss 9.582 (9.582)	gnorm 497843.406 (497843.406)	prob 1.700 (1.7004)	GS 35.391 (35.391)	mem 43.616
Train: [3][349/750]	BT 0.092 (1.179)	DT 0.002 (1.045)	loss 10.136 (10.136)	gnorm 533251.250 (533251.250)	prob 1.373 (1.3733)	GS 29.438 (29.438)	mem 43.616
Train: [3][350/750]	BT 0.084 (1.176)	DT 0.002 (1.042)	loss 9.824 (9.824)	gnorm 463145.219 (463145.219)	prob 1.386 (1.3859)	GS 35.000 (35.000)	mem 43.660
Train: [3][351/750]	BT 0.130 (1.173)	DT 0.002 (1.039)	loss 9.586 (9.586)	gnorm 469383.281 (469383.281)	prob 2.081 (2.0810)	GS 29.641 (29.641)	mem 43.710
Train: [3][352/750]	BT 0.756 (1.172)	DT 0.593 (1.038)	loss 9.460 (9.460)	gnorm 444246.312 (444246.312)	prob 2.341 (2.3414)	GS 34.828 (34.828)	mem 43.637
Train: [3][353/750]	BT 0.135 (1.169)	DT 0.002 (1.035)	loss 10.142 (10.142)	gnorm 520979.188 (520979.188)	prob 1.986 (1.9860)	GS 31.203 (31.203)	mem 43.637
Train: [3][354/750]	BT 0.213 (1.166)	DT 0.002 (1.032)	loss 10.481 (10.481)	gnorm 524253.219 (524253.219)	prob 0.692 (0.6923)	GS 30.172 (30.172)	mem 43.637
Train: [3][355/750]	BT 0.100 (1.163)	DT 0.001 (1.029)	loss 10.381 (10.381)	gnorm 473653.719 (473653.719)	prob 0.679 (0.6787)	GS 33.484 (33.484)	mem 43.637
Train: [3][356/750]	BT 9.003 (1.185)	DT 8.872 (1.051)	loss 9.668 (9.668)	gnorm 455694.156 (455694.156)	prob 1.725 (1.7247)	GS 33.688 (33.688)	mem 44.040
Train: [3][357/750]	BT 0.168 (1.182)	DT 0.011 (1.048)	loss 9.980 (9.980)	gnorm 474730.281 (474730.281)	prob 1.447 (1.4469)	GS 25.469 (25.469)	mem 43.808
Train: [3][358/750]	BT 0.134 (1.179)	DT 0.006 (1.046)	loss 9.930 (9.930)	gnorm 447104.594 (447104.594)	prob 1.493 (1.4931)	GS 33.203 (33.203)	mem 43.539
Train: [3][359/750]	BT 0.120 (1.176)	DT 0.002 (1.043)	loss 9.913 (9.913)	gnorm 456314.469 (456314.469)	prob 1.804 (1.8040)	GS 27.500 (27.500)	mem 43.540
Train: [3][360/750]	BT 0.074 (1.173)	DT 0.002 (1.040)	loss 9.797 (9.797)	gnorm 435528.688 (435528.688)	prob 1.689 (1.6895)	GS 34.328 (34.328)	mem 43.540
Train: [3][361/750]	BT 0.090 (1.170)	DT 0.001 (1.037)	loss 10.384 (10.384)	gnorm 461495.188 (461495.188)	prob 1.479 (1.4791)	GS 30.703 (30.703)	mem 43.554
Train: [3][362/750]	BT 0.142 (1.167)	DT 0.004 (1.034)	loss 9.916 (9.916)	gnorm 471375.719 (471375.719)	prob 1.935 (1.9345)	GS 35.062 (35.062)	mem 43.559
Train: [3][363/750]	BT 0.096 (1.164)	DT 0.004 (1.031)	loss 10.187 (10.187)	gnorm 464222.625 (464222.625)	prob 1.900 (1.9002)	GS 31.109 (31.109)	mem 43.559
Train: [3][364/750]	BT 0.639 (1.163)	DT 0.525 (1.030)	loss 9.837 (9.837)	gnorm 417643.312 (417643.312)	prob 2.470 (2.4702)	GS 33.875 (33.875)	mem 43.557
Train: [3][365/750]	BT 0.099 (1.160)	DT 0.002 (1.027)	loss 10.150 (10.150)	gnorm 451902.812 (451902.812)	prob 1.488 (1.4883)	GS 31.000 (31.000)	mem 43.572
Train: [3][366/750]	BT 0.193 (1.157)	DT 0.004 (1.024)	loss 10.036 (10.036)	gnorm 506622.281 (506622.281)	prob 1.829 (1.8285)	GS 30.344 (30.344)	mem 43.590
Train: [3][367/750]	BT 0.099 (1.155)	DT 0.002 (1.021)	loss 9.429 (9.429)	gnorm 470283.438 (470283.438)	prob 2.016 (2.0165)	GS 31.641 (31.641)	mem 43.590
Train: [3][368/750]	BT 11.390 (1.182)	DT 11.312 (1.049)	loss 10.166 (10.166)	gnorm 462907.469 (462907.469)	prob 1.193 (1.1931)	GS 33.266 (33.266)	mem 43.639
Train: [3][369/750]	BT 0.065 (1.179)	DT 0.001 (1.047)	loss 10.129 (10.129)	gnorm 474836.812 (474836.812)	prob 1.690 (1.6896)	GS 30.609 (30.609)	mem 43.639
Train: [3][370/750]	BT 0.077 (1.176)	DT 0.001 (1.044)	loss 10.078 (10.078)	gnorm 536689.250 (536689.250)	prob 1.539 (1.5391)	GS 32.859 (32.859)	mem 43.647
Train: [3][371/750]	BT 0.214 (1.174)	DT 0.002 (1.041)	loss 9.474 (9.474)	gnorm 470097.875 (470097.875)	prob 2.206 (2.2055)	GS 30.953 (30.953)	mem 43.682
Train: [3][372/750]	BT 0.143 (1.171)	DT 0.002 (1.038)	loss 9.981 (9.981)	gnorm 487882.719 (487882.719)	prob 1.344 (1.3444)	GS 35.906 (35.906)	mem 43.688
Train: [3][373/750]	BT 0.077 (1.168)	DT 0.002 (1.035)	loss 10.145 (10.145)	gnorm 490809.000 (490809.000)	prob 1.295 (1.2948)	GS 31.031 (31.031)	mem 43.771
Train: [3][374/750]	BT 0.130 (1.165)	DT 0.006 (1.033)	loss 9.938 (9.938)	gnorm 503784.438 (503784.438)	prob 1.671 (1.6711)	GS 31.078 (31.078)	mem 43.640
Train: [3][375/750]	BT 0.096 (1.162)	DT 0.001 (1.030)	loss 9.965 (9.965)	gnorm 503097.656 (503097.656)	prob 1.519 (1.5187)	GS 31.094 (31.094)	mem 43.673
Train: [3][376/750]	BT 1.768 (1.164)	DT 1.684 (1.032)	loss 10.118 (10.118)	gnorm 479211.625 (479211.625)	prob 1.154 (1.1539)	GS 34.797 (34.797)	mem 43.757
Train: [3][377/750]	BT 0.093 (1.161)	DT 0.003 (1.029)	loss 9.577 (9.577)	gnorm 474353.344 (474353.344)	prob 1.583 (1.5833)	GS 30.266 (30.266)	mem 43.958
Train: [3][378/750]	BT 0.091 (1.158)	DT 0.001 (1.026)	loss 10.025 (10.025)	gnorm 493903.594 (493903.594)	prob 1.294 (1.2935)	GS 32.953 (32.953)	mem 43.955
Train: [3][379/750]	BT 0.095 (1.156)	DT 0.002 (1.023)	loss 10.569 (10.569)	gnorm 505268.844 (505268.844)	prob 0.806 (0.8063)	GS 31.812 (31.812)	mem 43.955
Train: [3][380/750]	BT 11.717 (1.183)	DT 11.612 (1.051)	loss 9.934 (9.934)	gnorm 507898.875 (507898.875)	prob 1.529 (1.5289)	GS 33.562 (33.562)	mem 43.640
Train: [3][381/750]	BT 0.075 (1.180)	DT 0.002 (1.049)	loss 9.917 (9.917)	gnorm 493728.781 (493728.781)	prob 1.113 (1.1127)	GS 31.922 (31.922)	mem 43.639
Train: [3][382/750]	BT 0.095 (1.178)	DT 0.002 (1.046)	loss 10.399 (10.399)	gnorm 451675.625 (451675.625)	prob 0.683 (0.6828)	GS 33.172 (33.172)	mem 43.639
Train: [3][383/750]	BT 0.092 (1.175)	DT 0.003 (1.043)	loss 10.195 (10.195)	gnorm 464209.719 (464209.719)	prob 1.030 (1.0299)	GS 29.047 (29.047)	mem 43.639
Train: [3][384/750]	BT 0.121 (1.172)	DT 0.001 (1.040)	loss 9.640 (9.640)	gnorm 448694.312 (448694.312)	prob 2.122 (2.1217)	GS 33.375 (33.375)	mem 43.639
Train: [3][385/750]	BT 0.140 (1.169)	DT 0.010 (1.038)	loss 10.103 (10.103)	gnorm 433054.750 (433054.750)	prob 1.998 (1.9980)	GS 32.906 (32.906)	mem 43.687
Train: [3][386/750]	BT 0.100 (1.167)	DT 0.004 (1.035)	loss 9.910 (9.910)	gnorm 476569.594 (476569.594)	prob 2.712 (2.7116)	GS 32.781 (32.781)	mem 43.804
Train: [3][387/750]	BT 0.203 (1.164)	DT 0.002 (1.032)	loss 10.273 (10.273)	gnorm 466357.438 (466357.438)	prob 1.649 (1.6487)	GS 29.922 (29.922)	mem 43.800
Train: [3][388/750]	BT 0.127 (1.161)	DT 0.006 (1.030)	loss 10.035 (10.035)	gnorm 481013.156 (481013.156)	prob 1.606 (1.6059)	GS 34.453 (34.453)	mem 43.638
Train: [3][389/750]	BT 0.088 (1.159)	DT 0.002 (1.027)	loss 10.261 (10.261)	gnorm 465949.844 (465949.844)	prob 1.979 (1.9790)	GS 26.688 (26.688)	mem 43.638
Train: [3][390/750]	BT 0.199 (1.156)	DT 0.005 (1.024)	loss 10.074 (10.074)	gnorm 462584.750 (462584.750)	prob 2.042 (2.0423)	GS 33.250 (33.250)	mem 43.639
Train: [3][391/750]	BT 0.081 (1.153)	DT 0.001 (1.022)	loss 10.164 (10.164)	gnorm 481229.719 (481229.719)	prob 1.599 (1.5989)	GS 29.359 (29.359)	mem 43.639
Train: [3][392/750]	BT 13.715 (1.185)	DT 13.624 (1.054)	loss 10.622 (10.622)	gnorm 495819.375 (495819.375)	prob 0.668 (0.6681)	GS 34.531 (34.531)	mem 43.796
Train: [3][393/750]	BT 0.095 (1.183)	DT 0.002 (1.051)	loss 9.962 (9.962)	gnorm 463773.375 (463773.375)	prob 1.522 (1.5221)	GS 30.562 (30.562)	mem 43.695
Train: [3][394/750]	BT 0.104 (1.180)	DT 0.002 (1.049)	loss 10.718 (10.718)	gnorm 470300.031 (470300.031)	prob 0.151 (0.1507)	GS 35.172 (35.172)	mem 43.695
Train: [3][395/750]	BT 0.074 (1.177)	DT 0.002 (1.046)	loss 10.045 (10.045)	gnorm 481559.781 (481559.781)	prob 0.210 (0.2098)	GS 30.531 (30.531)	mem 43.553
Train: [3][396/750]	BT 0.086 (1.174)	DT 0.001 (1.043)	loss 9.903 (9.903)	gnorm 468862.875 (468862.875)	prob 0.990 (0.9897)	GS 32.734 (32.734)	mem 43.550
Train: [3][397/750]	BT 0.133 (1.172)	DT 0.002 (1.041)	loss 10.152 (10.152)	gnorm 485264.688 (485264.688)	prob 1.456 (1.4558)	GS 28.891 (28.891)	mem 43.550
Train: [3][398/750]	BT 0.080 (1.169)	DT 0.003 (1.038)	loss 10.263 (10.263)	gnorm 506286.781 (506286.781)	prob 1.154 (1.1538)	GS 38.484 (38.484)	mem 43.550
Train: [3][399/750]	BT 0.184 (1.167)	DT 0.009 (1.035)	loss 9.652 (9.652)	gnorm 437534.844 (437534.844)	prob 1.894 (1.8944)	GS 35.891 (35.891)	mem 43.551
Train: [3][400/750]	BT 1.007 (1.166)	DT 0.770 (1.035)	loss 10.592 (10.592)	gnorm 486621.250 (486621.250)	prob 0.581 (0.5814)	GS 36.891 (36.891)	mem 43.626
Train: [3][401/750]	BT 0.145 (1.164)	DT 0.002 (1.032)	loss 9.664 (9.664)	gnorm 490209.969 (490209.969)	prob 1.583 (1.5829)	GS 35.797 (35.797)	mem 43.768
Train: [3][402/750]	BT 0.195 (1.161)	DT 0.011 (1.030)	loss 10.215 (10.215)	gnorm 452904.469 (452904.469)	prob -0.007 (-0.0072)	GS 32.875 (32.875)	mem 43.843
Train: [3][403/750]	BT 0.127 (1.159)	DT 0.002 (1.027)	loss 9.952 (9.952)	gnorm 460350.281 (460350.281)	prob 0.440 (0.4399)	GS 29.969 (29.969)	mem 43.607
Train: [3][404/750]	BT 10.129 (1.181)	DT 9.992 (1.049)	loss 10.578 (10.578)	gnorm 488620.312 (488620.312)	prob -0.744 (-0.7441)	GS 31.219 (31.219)	mem 43.598
Train: [3][405/750]	BT 0.195 (1.178)	DT 0.025 (1.047)	loss 10.240 (10.240)	gnorm 526440.062 (526440.062)	prob -0.073 (-0.0734)	GS 31.203 (31.203)	mem 43.598
Train: [3][406/750]	BT 0.093 (1.176)	DT 0.001 (1.044)	loss 10.462 (10.462)	gnorm 485156.688 (485156.688)	prob -0.961 (-0.9610)	GS 33.516 (33.516)	mem 43.603
Train: [3][407/750]	BT 0.089 (1.173)	DT 0.004 (1.042)	loss 10.230 (10.230)	gnorm 530144.375 (530144.375)	prob -0.270 (-0.2698)	GS 28.766 (28.766)	mem 43.603
Train: [3][408/750]	BT 0.105 (1.170)	DT 0.002 (1.039)	loss 10.138 (10.138)	gnorm 485069.500 (485069.500)	prob -0.204 (-0.2035)	GS 32.719 (32.719)	mem 43.603
Train: [3][409/750]	BT 0.158 (1.168)	DT 0.005 (1.037)	loss 10.020 (10.020)	gnorm 495576.750 (495576.750)	prob 0.105 (0.1052)	GS 32.469 (32.469)	mem 43.603
Train: [3][410/750]	BT 0.149 (1.165)	DT 0.002 (1.034)	loss 10.250 (10.250)	gnorm 512715.125 (512715.125)	prob -0.289 (-0.2885)	GS 31.875 (31.875)	mem 43.604
Train: [3][411/750]	BT 0.103 (1.163)	DT 0.007 (1.032)	loss 10.431 (10.431)	gnorm 506580.656 (506580.656)	prob -0.171 (-0.1711)	GS 34.594 (34.594)	mem 43.604
Train: [3][412/750]	BT 0.784 (1.162)	DT 0.677 (1.031)	loss 10.015 (10.015)	gnorm 462529.875 (462529.875)	prob 0.533 (0.5333)	GS 36.312 (36.312)	mem 43.538
Train: [3][413/750]	BT 0.155 (1.160)	DT 0.001 (1.028)	loss 9.511 (9.511)	gnorm 460554.469 (460554.469)	prob 1.181 (1.1812)	GS 32.250 (32.250)	mem 43.550
Train: [3][414/750]	BT 0.128 (1.157)	DT 0.004 (1.026)	loss 10.062 (10.062)	gnorm 485086.406 (485086.406)	prob 0.349 (0.3488)	GS 32.109 (32.109)	mem 43.523
Train: [3][415/750]	BT 0.105 (1.155)	DT 0.009 (1.023)	loss 10.575 (10.575)	gnorm 531895.250 (531895.250)	prob 0.708 (0.7082)	GS 27.891 (27.891)	mem 43.525
Train: [3][416/750]	BT 9.399 (1.174)	DT 9.281 (1.043)	loss 9.921 (9.921)	gnorm 499346.938 (499346.938)	prob 0.847 (0.8473)	GS 30.922 (30.922)	mem 43.570
Train: [3][417/750]	BT 0.087 (1.172)	DT 0.004 (1.041)	loss 9.899 (9.899)	gnorm 492963.344 (492963.344)	prob 0.787 (0.7865)	GS 32.172 (32.172)	mem 43.587
Train: [3][418/750]	BT 0.155 (1.169)	DT 0.002 (1.038)	loss 9.815 (9.815)	gnorm 457512.625 (457512.625)	prob 0.830 (0.8303)	GS 32.609 (32.609)	mem 43.613
Train: [3][419/750]	BT 0.205 (1.167)	DT 0.018 (1.036)	loss 10.039 (10.039)	gnorm 495994.281 (495994.281)	prob 1.137 (1.1370)	GS 36.078 (36.078)	mem 43.614
Train: [3][420/750]	BT 0.126 (1.164)	DT 0.007 (1.033)	loss 10.228 (10.228)	gnorm 483089.562 (483089.562)	prob 0.448 (0.4482)	GS 32.938 (32.938)	mem 43.641
Train: [3][421/750]	BT 0.124 (1.162)	DT 0.002 (1.031)	loss 10.534 (10.534)	gnorm 476403.906 (476403.906)	prob 0.184 (0.1838)	GS 30.188 (30.188)	mem 43.686
Train: [3][422/750]	BT 0.085 (1.159)	DT 0.003 (1.028)	loss 9.810 (9.810)	gnorm 456141.062 (456141.062)	prob 0.147 (0.1469)	GS 31.594 (31.594)	mem 43.621
Train: [3][423/750]	BT 0.084 (1.157)	DT 0.001 (1.026)	loss 10.244 (10.244)	gnorm 466156.344 (466156.344)	prob 0.744 (0.7439)	GS 30.109 (30.109)	mem 43.666
Train: [3][424/750]	BT 3.374 (1.162)	DT 3.173 (1.031)	loss 10.112 (10.112)	gnorm 438938.312 (438938.312)	prob 0.489 (0.4886)	GS 32.984 (32.984)	mem 43.769
Train: [3][425/750]	BT 0.333 (1.160)	DT 0.024 (1.029)	loss 9.679 (9.679)	gnorm 438521.344 (438521.344)	prob 1.224 (1.2239)	GS 35.844 (35.844)	mem 43.744
Train: [3][426/750]	BT 0.179 (1.158)	DT 0.003 (1.026)	loss 10.405 (10.405)	gnorm 484748.500 (484748.500)	prob 0.594 (0.5939)	GS 36.516 (36.516)	mem 43.635
Train: [3][427/750]	BT 0.216 (1.156)	DT 0.005 (1.024)	loss 9.904 (9.904)	gnorm 456212.312 (456212.312)	prob 1.062 (1.0617)	GS 33.250 (33.250)	mem 43.755
Train: [3][428/750]	BT 7.271 (1.170)	DT 7.185 (1.038)	loss 9.729 (9.729)	gnorm 448222.438 (448222.438)	prob 1.860 (1.8603)	GS 29.359 (29.359)	mem 43.653
Train: [3][429/750]	BT 0.146 (1.168)	DT 0.012 (1.036)	loss 10.663 (10.663)	gnorm 471394.844 (471394.844)	prob 0.528 (0.5278)	GS 26.078 (26.078)	mem 43.654
Train: [3][430/750]	BT 0.149 (1.165)	DT 0.009 (1.033)	loss 10.214 (10.214)	gnorm 468449.281 (468449.281)	prob 0.605 (0.6054)	GS 34.844 (34.844)	mem 43.580
Train: [3][431/750]	BT 0.079 (1.163)	DT 0.002 (1.031)	loss 9.879 (9.879)	gnorm 481288.125 (481288.125)	prob 0.717 (0.7174)	GS 26.828 (26.828)	mem 43.585
Train: [3][432/750]	BT 0.088 (1.160)	DT 0.002 (1.029)	loss 9.758 (9.758)	gnorm 465453.219 (465453.219)	prob 1.681 (1.6813)	GS 36.484 (36.484)	mem 43.585
Train: [3][433/750]	BT 0.074 (1.158)	DT 0.002 (1.026)	loss 10.708 (10.708)	gnorm 471011.562 (471011.562)	prob 0.465 (0.4655)	GS 35.172 (35.172)	mem 43.624
Train: [3][434/750]	BT 0.126 (1.155)	DT 0.001 (1.024)	loss 10.095 (10.095)	gnorm 512561.188 (512561.188)	prob 1.441 (1.4414)	GS 32.406 (32.406)	mem 43.694
Train: [3][435/750]	BT 0.090 (1.153)	DT 0.003 (1.022)	loss 10.080 (10.080)	gnorm 490074.438 (490074.438)	prob 0.817 (0.8167)	GS 32.578 (32.578)	mem 43.623
Train: [3][436/750]	BT 5.131 (1.162)	DT 4.970 (1.031)	loss 9.834 (9.834)	gnorm 440782.281 (440782.281)	prob 1.028 (1.0275)	GS 37.531 (37.531)	mem 43.654
Train: [3][437/750]	BT 0.116 (1.160)	DT 0.004 (1.028)	loss 9.729 (9.729)	gnorm 488336.000 (488336.000)	prob 1.213 (1.2133)	GS 31.156 (31.156)	mem 43.574
Train: [3][438/750]	BT 0.088 (1.157)	DT 0.002 (1.026)	loss 10.473 (10.473)	gnorm 483795.781 (483795.781)	prob 0.387 (0.3871)	GS 34.094 (34.094)	mem 43.625
Train: [3][439/750]	BT 0.217 (1.155)	DT 0.002 (1.024)	loss 9.951 (9.951)	gnorm 452221.781 (452221.781)	prob 1.129 (1.1294)	GS 30.922 (30.922)	mem 43.581
Train: [3][440/750]	BT 4.711 (1.163)	DT 4.511 (1.032)	loss 9.940 (9.940)	gnorm 410469.531 (410469.531)	prob 0.728 (0.7281)	GS 33.031 (33.031)	mem 43.527
Train: [3][441/750]	BT 0.152 (1.161)	DT 0.002 (1.029)	loss 10.032 (10.032)	gnorm 424689.906 (424689.906)	prob 0.882 (0.8815)	GS 39.922 (39.922)	mem 43.527
Train: [3][442/750]	BT 0.197 (1.159)	DT 0.011 (1.027)	loss 9.644 (9.644)	gnorm 441431.625 (441431.625)	prob 1.802 (1.8018)	GS 33.734 (33.734)	mem 43.548
Train: [3][443/750]	BT 0.137 (1.156)	DT 0.008 (1.025)	loss 9.747 (9.747)	gnorm 462843.688 (462843.688)	prob 2.507 (2.5069)	GS 32.516 (32.516)	mem 43.548
Train: [3][444/750]	BT 1.058 (1.156)	DT 0.972 (1.025)	loss 9.776 (9.776)	gnorm 435805.344 (435805.344)	prob 2.045 (2.0452)	GS 35.047 (35.047)	mem 43.569
Train: [3][445/750]	BT 0.141 (1.154)	DT 0.002 (1.022)	loss 10.100 (10.100)	gnorm 466729.875 (466729.875)	prob 1.111 (1.1111)	GS 31.875 (31.875)	mem 43.685
Train: [3][446/750]	BT 0.220 (1.152)	DT 0.007 (1.020)	loss 9.735 (9.735)	gnorm 453745.125 (453745.125)	prob 0.884 (0.8841)	GS 33.312 (33.312)	mem 43.651
Train: [3][447/750]	BT 0.193 (1.150)	DT 0.011 (1.018)	loss 9.953 (9.953)	gnorm 458608.562 (458608.562)	prob 0.781 (0.7806)	GS 30.109 (30.109)	mem 43.559
Train: [3][448/750]	BT 6.518 (1.162)	DT 6.372 (1.030)	loss 9.821 (9.821)	gnorm 459664.844 (459664.844)	prob 1.109 (1.1090)	GS 31.531 (31.531)	mem 43.559
Train: [3][449/750]	BT 0.104 (1.159)	DT 0.002 (1.027)	loss 10.072 (10.072)	gnorm 487133.812 (487133.812)	prob 0.002 (0.0022)	GS 32.719 (32.719)	mem 43.613
Train: [3][450/750]	BT 0.108 (1.157)	DT 0.007 (1.025)	loss 10.007 (10.007)	gnorm 447577.062 (447577.062)	prob -0.057 (-0.0566)	GS 34.031 (34.031)	mem 43.560
Train: [3][451/750]	BT 0.157 (1.155)	DT 0.002 (1.023)	loss 10.345 (10.345)	gnorm 468146.344 (468146.344)	prob 0.168 (0.1677)	GS 34.625 (34.625)	mem 43.670
Train: [3][452/750]	BT 4.601 (1.162)	DT 4.488 (1.030)	loss 9.800 (9.800)	gnorm 487065.469 (487065.469)	prob 1.036 (1.0362)	GS 33.234 (33.234)	mem 43.546
Train: [3][453/750]	BT 0.142 (1.160)	DT 0.025 (1.028)	loss 10.093 (10.093)	gnorm 500900.594 (500900.594)	prob 0.750 (0.7499)	GS 39.359 (39.359)	mem 43.605
Train: [3][454/750]	BT 3.114 (1.164)	DT 3.021 (1.033)	loss 10.131 (10.131)	gnorm 432303.750 (432303.750)	prob 0.129 (0.1292)	GS 34.375 (34.375)	mem 43.580
Train: [3][455/750]	BT 0.130 (1.162)	DT 0.001 (1.030)	loss 10.093 (10.093)	gnorm 471894.438 (471894.438)	prob 0.160 (0.1598)	GS 33.266 (33.266)	mem 43.579
Train: [3][456/750]	BT 0.099 (1.160)	DT 0.002 (1.028)	loss 9.304 (9.304)	gnorm 429319.719 (429319.719)	prob 0.919 (0.9187)	GS 34.250 (34.250)	mem 43.638
Train: [3][457/750]	BT 0.323 (1.158)	DT 0.044 (1.026)	loss 10.364 (10.364)	gnorm 459429.594 (459429.594)	prob 0.664 (0.6644)	GS 33.109 (33.109)	mem 43.598
Train: [3][458/750]	BT 0.170 (1.156)	DT 0.030 (1.024)	loss 9.610 (9.610)	gnorm 437085.188 (437085.188)	prob 1.329 (1.3288)	GS 33.250 (33.250)	mem 43.637
Train: [3][459/750]	BT 0.178 (1.154)	DT 0.016 (1.022)	loss 10.083 (10.083)	gnorm 446863.938 (446863.938)	prob 0.568 (0.5675)	GS 29.750 (29.750)	mem 43.641
Train: [3][460/750]	BT 4.036 (1.160)	DT 3.857 (1.028)	loss 9.836 (9.836)	gnorm 449979.656 (449979.656)	prob 0.905 (0.9051)	GS 32.500 (32.500)	mem 43.803
Train: [3][461/750]	BT 0.317 (1.158)	DT 0.013 (1.026)	loss 9.674 (9.674)	gnorm 476664.812 (476664.812)	prob 0.866 (0.8655)	GS 28.594 (28.594)	mem 43.805
Train: [3][462/750]	BT 0.115 (1.156)	DT 0.002 (1.023)	loss 10.145 (10.145)	gnorm 465256.625 (465256.625)	prob 0.378 (0.3782)	GS 34.250 (34.250)	mem 43.675
Train: [3][463/750]	BT 0.096 (1.154)	DT 0.001 (1.021)	loss 10.119 (10.119)	gnorm 493942.625 (493942.625)	prob 0.549 (0.5492)	GS 27.031 (27.031)	mem 43.675
Train: [3][464/750]	BT 0.105 (1.151)	DT 0.008 (1.019)	loss 9.758 (9.758)	gnorm 447125.312 (447125.312)	prob 0.888 (0.8880)	GS 30.547 (30.547)	mem 43.676
Train: [3][465/750]	BT 0.111 (1.149)	DT 0.006 (1.017)	loss 10.492 (10.492)	gnorm 497747.188 (497747.188)	prob 0.232 (0.2325)	GS 31.562 (31.562)	mem 43.676
Train: [3][466/750]	BT 7.900 (1.163)	DT 7.730 (1.031)	loss 10.726 (10.726)	gnorm 538192.125 (538192.125)	prob -0.263 (-0.2630)	GS 31.297 (31.297)	mem 43.596
Train: [3][467/750]	BT 0.223 (1.161)	DT 0.005 (1.029)	loss 10.500 (10.500)	gnorm 468797.969 (468797.969)	prob 0.318 (0.3178)	GS 30.250 (30.250)	mem 43.658
Train: [3][468/750]	BT 0.188 (1.159)	DT 0.010 (1.027)	loss 9.806 (9.806)	gnorm 433646.344 (433646.344)	prob 0.953 (0.9525)	GS 31.828 (31.828)	mem 43.597
Train: [3][469/750]	BT 0.112 (1.157)	DT 0.007 (1.025)	loss 9.970 (9.970)	gnorm 491782.250 (491782.250)	prob 0.873 (0.8729)	GS 30.781 (30.781)	mem 43.606
Train: [3][470/750]	BT 0.137 (1.155)	DT 0.002 (1.022)	loss 9.692 (9.692)	gnorm 446994.719 (446994.719)	prob 1.180 (1.1797)	GS 38.078 (38.078)	mem 43.597
Train: [3][471/750]	BT 0.146 (1.153)	DT 0.002 (1.020)	loss 10.254 (10.254)	gnorm 465761.531 (465761.531)	prob 0.208 (0.2084)	GS 30.516 (30.516)	mem 43.597
Train: [3][472/750]	BT 5.831 (1.163)	DT 5.685 (1.030)	loss 9.864 (9.864)	gnorm 442588.781 (442588.781)	prob 0.447 (0.4472)	GS 33.875 (33.875)	mem 43.605
Train: [3][473/750]	BT 0.107 (1.161)	DT 0.014 (1.028)	loss 9.961 (9.961)	gnorm 503324.625 (503324.625)	prob 0.871 (0.8707)	GS 30.000 (30.000)	mem 43.606
Train: [3][474/750]	BT 0.105 (1.158)	DT 0.002 (1.026)	loss 10.084 (10.084)	gnorm 475822.125 (475822.125)	prob 0.889 (0.8893)	GS 31.672 (31.672)	mem 43.640
Train: [3][475/750]	BT 0.117 (1.156)	DT 0.011 (1.024)	loss 9.861 (9.861)	gnorm 423986.188 (423986.188)	prob 1.076 (1.0759)	GS 41.375 (41.375)	mem 43.686
Train: [3][476/750]	BT 0.120 (1.154)	DT 0.002 (1.022)	loss 9.625 (9.625)	gnorm 459494.969 (459494.969)	prob 1.434 (1.4340)	GS 33.047 (33.047)	mem 43.605
Train: [3][477/750]	BT 0.129 (1.152)	DT 0.002 (1.019)	loss 10.621 (10.621)	gnorm 482839.062 (482839.062)	prob 0.009 (0.0087)	GS 30.828 (30.828)	mem 43.606
Train: [3][478/750]	BT 8.674 (1.168)	DT 8.521 (1.035)	loss 10.065 (10.065)	gnorm 410039.938 (410039.938)	prob 0.580 (0.5795)	GS 34.547 (34.547)	mem 43.769
Train: [3][479/750]	BT 0.169 (1.165)	DT 0.005 (1.033)	loss 10.020 (10.020)	gnorm 487997.219 (487997.219)	prob 0.797 (0.7975)	GS 42.453 (42.453)	mem 43.771
Train: [3][480/750]	BT 0.277 (1.164)	DT 0.020 (1.031)	loss 10.708 (10.708)	gnorm 450729.312 (450729.312)	prob 0.329 (0.3288)	GS 34.047 (34.047)	mem 43.812
Train: [3][481/750]	BT 0.150 (1.161)	DT 0.027 (1.029)	loss 10.072 (10.072)	gnorm 440990.375 (440990.375)	prob 0.958 (0.9577)	GS 30.172 (30.172)	mem 43.666
Train: [3][482/750]	BT 0.160 (1.159)	DT 0.003 (1.027)	loss 9.442 (9.442)	gnorm 436492.500 (436492.500)	prob 1.385 (1.3847)	GS 31.016 (31.016)	mem 43.800
Train: [3][483/750]	BT 0.146 (1.157)	DT 0.001 (1.025)	loss 10.298 (10.298)	gnorm 495316.312 (495316.312)	prob 0.283 (0.2835)	GS 29.172 (29.172)	mem 43.778
Train: [3][484/750]	BT 4.099 (1.163)	DT 3.973 (1.031)	loss 9.827 (9.827)	gnorm 472080.750 (472080.750)	prob 0.236 (0.2357)	GS 31.531 (31.531)	mem 43.668
Train: [3][485/750]	BT 0.221 (1.161)	DT 0.021 (1.029)	loss 10.680 (10.680)	gnorm 493396.375 (493396.375)	prob -0.061 (-0.0614)	GS 28.547 (28.547)	mem 43.691
Train: [3][486/750]	BT 0.151 (1.159)	DT 0.027 (1.027)	loss 9.786 (9.786)	gnorm 488624.250 (488624.250)	prob 0.920 (0.9200)	GS 35.000 (35.000)	mem 43.739
Train: [3][487/750]	BT 0.092 (1.157)	DT 0.003 (1.024)	loss 9.172 (9.172)	gnorm 453579.188 (453579.188)	prob 1.949 (1.9486)	GS 32.859 (32.859)	mem 43.764
Train: [3][488/750]	BT 0.099 (1.155)	DT 0.002 (1.022)	loss 9.725 (9.725)	gnorm 423544.312 (423544.312)	prob 1.072 (1.0718)	GS 34.281 (34.281)	mem 43.690
Train: [3][489/750]	BT 0.084 (1.153)	DT 0.002 (1.020)	loss 10.323 (10.323)	gnorm 450047.375 (450047.375)	prob 0.336 (0.3363)	GS 29.438 (29.438)	mem 43.725
Train: [3][490/750]	BT 7.561 (1.166)	DT 7.397 (1.033)	loss 10.414 (10.414)	gnorm 487415.844 (487415.844)	prob 0.436 (0.4360)	GS 34.578 (34.578)	mem 43.650
Train: [3][491/750]	BT 0.129 (1.164)	DT 0.010 (1.031)	loss 9.578 (9.578)	gnorm 432326.250 (432326.250)	prob 1.603 (1.6031)	GS 29.719 (29.719)	mem 43.686
Train: [3][492/750]	BT 0.117 (1.162)	DT 0.002 (1.029)	loss 10.473 (10.473)	gnorm 479080.844 (479080.844)	prob 0.713 (0.7132)	GS 34.188 (34.188)	mem 43.651
Train: [3][493/750]	BT 0.185 (1.160)	DT 0.002 (1.027)	loss 10.268 (10.268)	gnorm 456102.625 (456102.625)	prob -0.155 (-0.1549)	GS 31.750 (31.750)	mem 43.658
Train: [3][494/750]	BT 0.189 (1.158)	DT 0.002 (1.025)	loss 10.298 (10.298)	gnorm 466461.156 (466461.156)	prob 0.421 (0.4209)	GS 36.812 (36.812)	mem 43.713
Train: [3][495/750]	BT 0.123 (1.156)	DT 0.002 (1.023)	loss 9.810 (9.810)	gnorm 513623.375 (513623.375)	prob 0.332 (0.3323)	GS 35.125 (35.125)	mem 43.657
Train: [3][496/750]	BT 3.519 (1.160)	DT 3.393 (1.028)	loss 9.705 (9.705)	gnorm 469036.875 (469036.875)	prob 0.908 (0.9082)	GS 35.469 (35.469)	mem 43.758
Train: [3][497/750]	BT 0.095 (1.158)	DT 0.003 (1.026)	loss 9.715 (9.715)	gnorm 467473.469 (467473.469)	prob 0.888 (0.8883)	GS 30.594 (30.594)	mem 43.684
Train: [3][498/750]	BT 0.131 (1.156)	DT 0.003 (1.023)	loss 10.354 (10.354)	gnorm 446046.250 (446046.250)	prob -0.100 (-0.1001)	GS 32.000 (32.000)	mem 43.683
Train: [3][499/750]	BT 0.105 (1.154)	DT 0.014 (1.021)	loss 10.330 (10.330)	gnorm 538948.500 (538948.500)	prob -0.204 (-0.2040)	GS 39.484 (39.484)	mem 43.717
Train: [3][500/750]	BT 0.128 (1.152)	DT 0.012 (1.019)	loss 9.840 (9.840)	gnorm 478984.312 (478984.312)	prob 0.719 (0.7186)	GS 31.016 (31.016)	mem 43.723
Train: [3][501/750]	BT 0.211 (1.150)	DT 0.018 (1.017)	loss 10.157 (10.157)	gnorm 505724.531 (505724.531)	prob 0.445 (0.4445)	GS 31.469 (31.469)	mem 43.685
Train: [3][502/750]	BT 9.231 (1.166)	DT 9.126 (1.034)	loss 10.012 (10.012)	gnorm 485303.625 (485303.625)	prob 0.361 (0.3613)	GS 31.375 (31.375)	mem 43.714
Train: [3][503/750]	BT 0.147 (1.164)	DT 0.002 (1.032)	loss 10.607 (10.607)	gnorm 465389.938 (465389.938)	prob -0.169 (-0.1693)	GS 32.500 (32.500)	mem 43.686
Train: [3][504/750]	BT 0.151 (1.162)	DT 0.002 (1.030)	loss 9.855 (9.855)	gnorm 451121.406 (451121.406)	prob 0.853 (0.8527)	GS 34.922 (34.922)	mem 43.646
Train: [3][505/750]	BT 0.123 (1.160)	DT 0.002 (1.027)	loss 10.129 (10.129)	gnorm 500147.781 (500147.781)	prob 0.853 (0.8535)	GS 32.594 (32.594)	mem 43.685
Train: [3][506/750]	BT 0.117 (1.158)	DT 0.002 (1.025)	loss 9.387 (9.387)	gnorm 441442.875 (441442.875)	prob 1.318 (1.3175)	GS 31.500 (31.500)	mem 43.647
Train: [3][507/750]	BT 0.079 (1.156)	DT 0.012 (1.023)	loss 9.570 (9.570)	gnorm 449103.406 (449103.406)	prob 1.724 (1.7245)	GS 28.391 (28.391)	mem 43.647
Train: [3][508/750]	BT 2.810 (1.159)	DT 2.681 (1.027)	loss 9.822 (9.822)	gnorm 460122.062 (460122.062)	prob 1.008 (1.0080)	GS 36.234 (36.234)	mem 43.592
Train: [3][509/750]	BT 0.091 (1.157)	DT 0.002 (1.025)	loss 9.829 (9.829)	gnorm 421143.281 (421143.281)	prob 1.423 (1.4227)	GS 30.953 (30.953)	mem 43.648
Train: [3][510/750]	BT 0.323 (1.155)	DT 0.075 (1.023)	loss 9.661 (9.661)	gnorm 470136.625 (470136.625)	prob 1.745 (1.7445)	GS 36.281 (36.281)	mem 43.754
Train: [3][511/750]	BT 0.108 (1.153)	DT 0.007 (1.021)	loss 10.115 (10.115)	gnorm 433470.688 (433470.688)	prob 1.932 (1.9319)	GS 35.656 (35.656)	mem 43.591
Train: [3][512/750]	BT 0.153 (1.151)	DT 0.005 (1.019)	loss 10.229 (10.229)	gnorm 425543.656 (425543.656)	prob 1.309 (1.3086)	GS 33.281 (33.281)	mem 43.591
Train: [3][513/750]	BT 0.213 (1.150)	DT 0.003 (1.017)	loss 10.332 (10.332)	gnorm 451598.750 (451598.750)	prob 0.818 (0.8181)	GS 28.891 (28.891)	mem 43.591
Train: [3][514/750]	BT 9.884 (1.167)	DT 9.793 (1.034)	loss 9.769 (9.769)	gnorm 465429.000 (465429.000)	prob 1.187 (1.1870)	GS 34.062 (34.062)	mem 43.593
Train: [3][515/750]	BT 0.101 (1.165)	DT 0.002 (1.032)	loss 9.695 (9.695)	gnorm 464673.688 (464673.688)	prob 1.864 (1.8636)	GS 29.312 (29.312)	mem 43.593
Train: [3][516/750]	BT 0.103 (1.162)	DT 0.013 (1.030)	loss 9.822 (9.822)	gnorm 412243.281 (412243.281)	prob 0.590 (0.5904)	GS 32.594 (32.594)	mem 43.593
Train: [3][517/750]	BT 0.087 (1.160)	DT 0.001 (1.028)	loss 10.412 (10.412)	gnorm 462599.094 (462599.094)	prob 0.334 (0.3340)	GS 32.219 (32.219)	mem 43.592
Train: [3][518/750]	BT 0.115 (1.158)	DT 0.002 (1.026)	loss 9.467 (9.467)	gnorm 488012.500 (488012.500)	prob 0.963 (0.9633)	GS 31.562 (31.562)	mem 43.593
Train: [3][519/750]	BT 0.100 (1.156)	DT 0.002 (1.024)	loss 9.954 (9.954)	gnorm 417227.719 (417227.719)	prob 0.921 (0.9210)	GS 32.016 (32.016)	mem 43.632
Train: [3][520/750]	BT 0.280 (1.155)	DT 0.050 (1.022)	loss 9.729 (9.729)	gnorm 453109.312 (453109.312)	prob 1.029 (1.0290)	GS 29.297 (29.297)	mem 43.594
Train: [3][521/750]	BT 0.222 (1.153)	DT 0.011 (1.020)	loss 10.097 (10.097)	gnorm 456131.000 (456131.000)	prob 0.957 (0.9570)	GS 32.047 (32.047)	mem 43.595
Train: [3][522/750]	BT 0.110 (1.151)	DT 0.002 (1.018)	loss 10.092 (10.092)	gnorm 437624.344 (437624.344)	prob 1.038 (1.0378)	GS 33.406 (33.406)	mem 43.595
Train: [3][523/750]	BT 0.120 (1.149)	DT 0.002 (1.016)	loss 10.580 (10.580)	gnorm 483056.875 (483056.875)	prob 0.674 (0.6741)	GS 29.156 (29.156)	mem 43.595
Train: [3][524/750]	BT 0.116 (1.147)	DT 0.002 (1.014)	loss 9.332 (9.332)	gnorm 458160.594 (458160.594)	prob 1.653 (1.6530)	GS 35.125 (35.125)	mem 43.595
Train: [3][525/750]	BT 0.203 (1.145)	DT 0.005 (1.012)	loss 9.882 (9.882)	gnorm 425511.812 (425511.812)	prob 0.975 (0.9752)	GS 32.641 (32.641)	mem 43.595
Train: [3][526/750]	BT 11.722 (1.165)	DT 11.624 (1.033)	loss 10.208 (10.208)	gnorm 430743.750 (430743.750)	prob 0.807 (0.8069)	GS 31.172 (31.172)	mem 43.598
Train: [3][527/750]	BT 0.086 (1.163)	DT 0.002 (1.031)	loss 10.232 (10.232)	gnorm 478290.812 (478290.812)	prob 0.346 (0.3463)	GS 29.250 (29.250)	mem 43.598
Train: [3][528/750]	BT 0.093 (1.161)	DT 0.001 (1.029)	loss 9.689 (9.689)	gnorm 474978.969 (474978.969)	prob 1.137 (1.1367)	GS 35.641 (35.641)	mem 43.598
Train: [3][529/750]	BT 0.074 (1.159)	DT 0.002 (1.027)	loss 10.052 (10.052)	gnorm 467997.875 (467997.875)	prob 1.704 (1.7042)	GS 33.031 (33.031)	mem 43.598
Train: [3][530/750]	BT 0.108 (1.157)	DT 0.002 (1.025)	loss 10.379 (10.379)	gnorm 471679.938 (471679.938)	prob 1.008 (1.0079)	GS 38.781 (38.781)	mem 43.598
Train: [3][531/750]	BT 0.230 (1.155)	DT 0.003 (1.023)	loss 10.411 (10.411)	gnorm 448004.125 (448004.125)	prob 1.402 (1.4019)	GS 39.922 (39.922)	mem 43.649
Train: [3][532/750]	BT 1.173 (1.155)	DT 1.058 (1.023)	loss 9.679 (9.679)	gnorm 442919.812 (442919.812)	prob 1.852 (1.8525)	GS 34.281 (34.281)	mem 43.538
Train: [3][533/750]	BT 0.079 (1.153)	DT 0.001 (1.021)	loss 10.436 (10.436)	gnorm 505048.188 (505048.188)	prob 0.810 (0.8097)	GS 30.000 (30.000)	mem 43.603
Train: [3][534/750]	BT 0.087 (1.151)	DT 0.002 (1.019)	loss 9.687 (9.687)	gnorm 441364.312 (441364.312)	prob 1.802 (1.8024)	GS 32.344 (32.344)	mem 43.568
Train: [3][535/750]	BT 0.160 (1.150)	DT 0.001 (1.017)	loss 10.017 (10.017)	gnorm 475920.719 (475920.719)	prob 1.383 (1.3826)	GS 28.234 (28.234)	mem 43.610
Train: [3][536/750]	BT 0.137 (1.148)	DT 0.004 (1.015)	loss 9.834 (9.834)	gnorm 471166.594 (471166.594)	prob 0.979 (0.9792)	GS 32.922 (32.922)	mem 43.537
Train: [3][537/750]	BT 0.194 (1.146)	DT 0.003 (1.014)	loss 10.411 (10.411)	gnorm 430941.344 (430941.344)	prob 0.829 (0.8287)	GS 29.094 (29.094)	mem 43.539
Train: [3][538/750]	BT 11.389 (1.165)	DT 11.236 (1.033)	loss 10.054 (10.054)	gnorm 451570.094 (451570.094)	prob 0.694 (0.6939)	GS 30.312 (30.312)	mem 43.671
Train: [3][539/750]	BT 0.096 (1.163)	DT 0.002 (1.031)	loss 10.257 (10.257)	gnorm 452670.469 (452670.469)	prob 0.870 (0.8697)	GS 32.047 (32.047)	mem 43.782
Train: [3][540/750]	BT 0.097 (1.161)	DT 0.003 (1.029)	loss 9.683 (9.683)	gnorm 413758.125 (413758.125)	prob 1.037 (1.0372)	GS 29.531 (29.531)	mem 43.765
Train: [3][541/750]	BT 0.086 (1.159)	DT 0.003 (1.027)	loss 9.782 (9.782)	gnorm 437892.875 (437892.875)	prob 0.515 (0.5149)	GS 29.656 (29.656)	mem 43.765
Train: [3][542/750]	BT 0.111 (1.157)	DT 0.003 (1.025)	loss 9.387 (9.387)	gnorm 430986.375 (430986.375)	prob 1.294 (1.2940)	GS 31.172 (31.172)	mem 43.660
Train: [3][543/750]	BT 0.096 (1.155)	DT 0.001 (1.023)	loss 10.413 (10.413)	gnorm 486533.844 (486533.844)	prob 0.914 (0.9142)	GS 32.922 (32.922)	mem 43.660
Train: [3][544/750]	BT 1.574 (1.156)	DT 1.493 (1.024)	loss 10.283 (10.283)	gnorm 457054.125 (457054.125)	prob 1.061 (1.0611)	GS 37.891 (37.891)	mem 43.619
Train: [3][545/750]	BT 0.083 (1.154)	DT 0.001 (1.022)	loss 10.288 (10.288)	gnorm 494407.531 (494407.531)	prob 1.081 (1.0806)	GS 34.031 (34.031)	mem 43.640
Train: [3][546/750]	BT 0.201 (1.152)	DT 0.002 (1.020)	loss 9.940 (9.940)	gnorm 439565.406 (439565.406)	prob 1.157 (1.1569)	GS 34.406 (34.406)	mem 43.668
Train: [3][547/750]	BT 0.171 (1.150)	DT 0.012 (1.018)	loss 10.069 (10.069)	gnorm 473026.531 (473026.531)	prob 1.042 (1.0419)	GS 29.406 (29.406)	mem 43.730
Train: [3][548/750]	BT 0.158 (1.149)	DT 0.002 (1.016)	loss 9.301 (9.301)	gnorm 442472.031 (442472.031)	prob 1.816 (1.8157)	GS 33.891 (33.891)	mem 43.764
Train: [3][549/750]	BT 0.201 (1.147)	DT 0.005 (1.015)	loss 10.151 (10.151)	gnorm 449105.156 (449105.156)	prob 1.364 (1.3644)	GS 33.312 (33.312)	mem 43.699
Train: [3][550/750]	BT 9.692 (1.162)	DT 9.522 (1.030)	loss 9.757 (9.757)	gnorm 413499.750 (413499.750)	prob 1.210 (1.2102)	GS 33.859 (33.859)	mem 43.794
Train: [3][551/750]	BT 0.143 (1.161)	DT 0.002 (1.028)	loss 9.807 (9.807)	gnorm 479235.562 (479235.562)	prob 0.879 (0.8793)	GS 29.859 (29.859)	mem 43.721
Train: [3][552/750]	BT 0.101 (1.159)	DT 0.002 (1.026)	loss 9.551 (9.551)	gnorm 428097.719 (428097.719)	prob 1.048 (1.0479)	GS 34.422 (34.422)	mem 43.761
Train: [3][553/750]	BT 0.086 (1.157)	DT 0.002 (1.024)	loss 9.383 (9.383)	gnorm 457494.156 (457494.156)	prob 1.599 (1.5992)	GS 31.016 (31.016)	mem 43.722
Train: [3][554/750]	BT 0.191 (1.155)	DT 0.001 (1.023)	loss 9.750 (9.750)	gnorm 448218.625 (448218.625)	prob 0.930 (0.9297)	GS 32.094 (32.094)	mem 43.773
Train: [3][555/750]	BT 0.263 (1.153)	DT 0.012 (1.021)	loss 10.298 (10.298)	gnorm 445009.031 (445009.031)	prob 0.410 (0.4096)	GS 32.469 (32.469)	mem 43.723
Train: [3][556/750]	BT 7.274 (1.164)	DT 7.170 (1.032)	loss 9.900 (9.900)	gnorm 420309.938 (420309.938)	prob 1.208 (1.2083)	GS 37.812 (37.812)	mem 43.671
Train: [3][557/750]	BT 0.104 (1.162)	DT 0.001 (1.030)	loss 10.076 (10.076)	gnorm 455200.156 (455200.156)	prob 1.409 (1.4095)	GS 29.641 (29.641)	mem 43.673
Train: [3][558/750]	BT 0.100 (1.161)	DT 0.005 (1.028)	loss 10.196 (10.196)	gnorm 457086.219 (457086.219)	prob 0.644 (0.6445)	GS 35.672 (35.672)	mem 43.672
Train: [3][559/750]	BT 0.187 (1.159)	DT 0.002 (1.026)	loss 9.677 (9.677)	gnorm 420889.344 (420889.344)	prob 1.181 (1.1812)	GS 26.469 (26.469)	mem 43.891
Train: [3][560/750]	BT 0.232 (1.157)	DT 0.006 (1.025)	loss 10.065 (10.065)	gnorm 441677.625 (441677.625)	prob 0.644 (0.6444)	GS 32.500 (32.500)	mem 43.611
Train: [3][561/750]	BT 0.131 (1.155)	DT 0.003 (1.023)	loss 10.145 (10.145)	gnorm 465825.375 (465825.375)	prob 0.281 (0.2805)	GS 34.016 (34.016)	mem 43.667
Train: [3][562/750]	BT 3.559 (1.160)	DT 3.373 (1.027)	loss 9.941 (9.941)	gnorm 443390.938 (443390.938)	prob 0.541 (0.5413)	GS 32.016 (32.016)	mem 43.663
Train: [3][563/750]	BT 0.126 (1.158)	DT 0.002 (1.025)	loss 10.086 (10.086)	gnorm 437791.781 (437791.781)	prob 0.553 (0.5533)	GS 32.891 (32.891)	mem 43.721
Train: [3][564/750]	BT 0.097 (1.156)	DT 0.005 (1.023)	loss 10.550 (10.550)	gnorm 448788.562 (448788.562)	prob -0.016 (-0.0163)	GS 35.641 (35.641)	mem 43.663
Train: [3][565/750]	BT 0.104 (1.154)	DT 0.002 (1.021)	loss 10.280 (10.280)	gnorm 465369.438 (465369.438)	prob 1.262 (1.2625)	GS 33.688 (33.688)	mem 43.664
Train: [3][566/750]	BT 0.166 (1.152)	DT 0.002 (1.020)	loss 10.006 (10.006)	gnorm 486530.875 (486530.875)	prob 0.455 (0.4555)	GS 30.031 (30.031)	mem 43.664
Train: [3][567/750]	BT 0.086 (1.150)	DT 0.002 (1.018)	loss 10.188 (10.188)	gnorm 460020.969 (460020.969)	prob 0.233 (0.2333)	GS 34.484 (34.484)	mem 43.745
Train: [3][568/750]	BT 6.721 (1.160)	DT 6.599 (1.028)	loss 10.168 (10.168)	gnorm 431188.312 (431188.312)	prob 0.506 (0.5064)	GS 31.609 (31.609)	mem 43.699
Train: [3][569/750]	BT 0.109 (1.158)	DT 0.001 (1.026)	loss 9.849 (9.849)	gnorm 475745.406 (475745.406)	prob 0.666 (0.6656)	GS 32.047 (32.047)	mem 43.813
Train: [3][570/750]	BT 0.120 (1.156)	DT 0.011 (1.024)	loss 9.528 (9.528)	gnorm 429963.000 (429963.000)	prob 0.914 (0.9142)	GS 35.609 (35.609)	mem 43.766
Train: [3][571/750]	BT 0.168 (1.155)	DT 0.009 (1.022)	loss 9.908 (9.908)	gnorm 424978.375 (424978.375)	prob 1.041 (1.0412)	GS 34.375 (34.375)	mem 43.805
Train: [3][572/750]	BT 0.181 (1.153)	DT 0.002 (1.021)	loss 9.357 (9.357)	gnorm 450784.062 (450784.062)	prob 1.149 (1.1486)	GS 33.031 (33.031)	mem 43.660
Train: [3][573/750]	BT 0.199 (1.151)	DT 0.003 (1.019)	loss 9.710 (9.710)	gnorm 425756.469 (425756.469)	prob 1.646 (1.6458)	GS 31.562 (31.562)	mem 43.661
Train: [3][574/750]	BT 8.248 (1.164)	DT 8.116 (1.031)	loss 9.871 (9.871)	gnorm 411391.250 (411391.250)	prob 0.885 (0.8852)	GS 35.391 (35.391)	mem 43.642
Train: [3][575/750]	BT 0.101 (1.162)	DT 0.011 (1.029)	loss 9.610 (9.610)	gnorm 437435.062 (437435.062)	prob 1.325 (1.3248)	GS 32.375 (32.375)	mem 43.643
Train: [3][576/750]	BT 0.080 (1.160)	DT 0.002 (1.028)	loss 9.681 (9.681)	gnorm 405079.906 (405079.906)	prob 1.042 (1.0418)	GS 35.562 (35.562)	mem 43.643
Train: [3][577/750]	BT 0.078 (1.158)	DT 0.001 (1.026)	loss 9.867 (9.867)	gnorm 438677.344 (438677.344)	prob 1.267 (1.2671)	GS 31.547 (31.547)	mem 43.682
Train: [3][578/750]	BT 0.122 (1.156)	DT 0.002 (1.024)	loss 10.208 (10.208)	gnorm 439253.875 (439253.875)	prob 1.109 (1.1091)	GS 30.297 (30.297)	mem 43.682
Train: [3][579/750]	BT 0.286 (1.155)	DT 0.017 (1.022)	loss 9.512 (9.512)	gnorm 456469.125 (456469.125)	prob 1.015 (1.0153)	GS 32.359 (32.359)	mem 43.691
Train: [3][580/750]	BT 2.545 (1.157)	DT 2.430 (1.025)	loss 9.854 (9.854)	gnorm 465237.500 (465237.500)	prob 0.588 (0.5882)	GS 31.859 (31.859)	mem 43.686
Train: [3][581/750]	BT 0.175 (1.156)	DT 0.018 (1.023)	loss 10.224 (10.224)	gnorm 454616.156 (454616.156)	prob 0.475 (0.4751)	GS 34.047 (34.047)	mem 43.663
Train: [3][582/750]	BT 0.191 (1.154)	DT 0.007 (1.021)	loss 9.613 (9.613)	gnorm 417565.438 (417565.438)	prob 1.043 (1.0426)	GS 32.562 (32.562)	mem 43.670
Train: [3][583/750]	BT 0.156 (1.152)	DT 0.001 (1.019)	loss 9.811 (9.811)	gnorm 447400.406 (447400.406)	prob 0.563 (0.5633)	GS 30.828 (30.828)	mem 43.670
Train: [3][584/750]	BT 0.203 (1.151)	DT 0.002 (1.018)	loss 9.661 (9.661)	gnorm 412232.812 (412232.812)	prob 0.918 (0.9178)	GS 30.547 (30.547)	mem 43.741
Train: [3][585/750]	BT 0.261 (1.149)	DT 0.003 (1.016)	loss 9.655 (9.655)	gnorm 444613.156 (444613.156)	prob 1.198 (1.1976)	GS 31.359 (31.359)	mem 43.672
Train: [3][586/750]	BT 9.868 (1.164)	DT 9.628 (1.031)	loss 10.178 (10.178)	gnorm 454210.406 (454210.406)	prob 0.901 (0.9010)	GS 32.156 (32.156)	mem 43.727
Train: [3][587/750]	BT 0.191 (1.162)	DT 0.009 (1.029)	loss 10.228 (10.228)	gnorm 457077.938 (457077.938)	prob 0.710 (0.7099)	GS 32.828 (32.828)	mem 43.735
Train: [3][588/750]	BT 0.168 (1.161)	DT 0.037 (1.027)	loss 9.825 (9.825)	gnorm 447820.250 (447820.250)	prob 1.494 (1.4943)	GS 35.328 (35.328)	mem 43.734
Train: [3][589/750]	BT 0.159 (1.159)	DT 0.008 (1.026)	loss 9.819 (9.819)	gnorm 446030.344 (446030.344)	prob 1.291 (1.2913)	GS 31.422 (31.422)	mem 43.734
Train: [3][590/750]	BT 0.145 (1.157)	DT 0.008 (1.024)	loss 10.018 (10.018)	gnorm 417371.719 (417371.719)	prob 0.698 (0.6984)	GS 36.391 (36.391)	mem 43.734
Train: [3][591/750]	BT 0.107 (1.155)	DT 0.002 (1.022)	loss 10.251 (10.251)	gnorm 430313.844 (430313.844)	prob 0.648 (0.6476)	GS 33.719 (33.719)	mem 43.734
Train: [3][592/750]	BT 0.976 (1.155)	DT 0.828 (1.022)	loss 9.881 (9.881)	gnorm 424351.219 (424351.219)	prob 1.476 (1.4763)	GS 32.391 (32.391)	mem 43.724
Train: [3][593/750]	BT 0.093 (1.153)	DT 0.003 (1.020)	loss 9.094 (9.094)	gnorm 439835.406 (439835.406)	prob 2.252 (2.2521)	GS 35.078 (35.078)	mem 43.667
Train: [3][594/750]	BT 0.088 (1.151)	DT 0.002 (1.018)	loss 9.671 (9.671)	gnorm 462999.750 (462999.750)	prob 1.555 (1.5551)	GS 35.078 (35.078)	mem 43.667
Train: [3][595/750]	BT 0.086 (1.150)	DT 0.003 (1.017)	loss 9.996 (9.996)	gnorm 450012.062 (450012.062)	prob 1.388 (1.3882)	GS 31.594 (31.594)	mem 43.667
Train: [3][596/750]	BT 0.103 (1.148)	DT 0.003 (1.015)	loss 9.665 (9.665)	gnorm 445852.062 (445852.062)	prob 1.479 (1.4789)	GS 33.547 (33.547)	mem 43.669
Train: [3][597/750]	BT 0.087 (1.146)	DT 0.001 (1.013)	loss 9.846 (9.846)	gnorm 435699.656 (435699.656)	prob 1.414 (1.4140)	GS 30.234 (30.234)	mem 43.673
Train: [3][598/750]	BT 14.216 (1.168)	DT 14.130 (1.035)	loss 10.154 (10.154)	gnorm 435369.125 (435369.125)	prob 0.681 (0.6808)	GS 38.797 (38.797)	mem 43.646
Train: [3][599/750]	BT 0.067 (1.166)	DT 0.001 (1.033)	loss 9.698 (9.698)	gnorm 445325.062 (445325.062)	prob 1.007 (1.0067)	GS 30.328 (30.328)	mem 43.646
Train: [3][600/750]	BT 0.125 (1.164)	DT 0.002 (1.032)	loss 9.981 (9.981)	gnorm 423663.781 (423663.781)	prob 0.831 (0.8311)	GS 31.359 (31.359)	mem 43.667
Train: [3][601/750]	BT 0.073 (1.163)	DT 0.002 (1.030)	loss 10.808 (10.808)	gnorm 483618.219 (483618.219)	prob 0.597 (0.5972)	GS 38.500 (38.500)	mem 43.760
Train: [3][602/750]	BT 0.115 (1.161)	DT 0.001 (1.028)	loss 9.739 (9.739)	gnorm 424545.656 (424545.656)	prob 1.448 (1.4485)	GS 38.016 (38.016)	mem 43.796
Train: [3][603/750]	BT 0.108 (1.159)	DT 0.005 (1.027)	loss 10.024 (10.024)	gnorm 466045.969 (466045.969)	prob 0.679 (0.6791)	GS 35.344 (35.344)	mem 43.647
Train: [3][604/750]	BT 0.077 (1.157)	DT 0.001 (1.025)	loss 9.318 (9.318)	gnorm 412294.781 (412294.781)	prob 1.957 (1.9574)	GS 33.953 (33.953)	mem 43.582
Train: [3][605/750]	BT 0.177 (1.156)	DT 0.002 (1.023)	loss 10.026 (10.026)	gnorm 413368.500 (413368.500)	prob 1.700 (1.7002)	GS 33.375 (33.375)	mem 43.582
Train: [3][606/750]	BT 0.089 (1.154)	DT 0.003 (1.022)	loss 10.095 (10.095)	gnorm 444216.375 (444216.375)	prob 1.392 (1.3919)	GS 37.875 (37.875)	mem 43.590
Train: [3][607/750]	BT 0.188 (1.152)	DT 0.003 (1.020)	loss 9.994 (9.994)	gnorm 402373.344 (402373.344)	prob 1.367 (1.3671)	GS 31.156 (31.156)	mem 43.595
Train: [3][608/750]	BT 0.141 (1.151)	DT 0.015 (1.018)	loss 10.416 (10.416)	gnorm 426585.781 (426585.781)	prob 1.200 (1.2002)	GS 36.344 (36.344)	mem 43.595
Train: [3][609/750]	BT 0.108 (1.149)	DT 0.006 (1.017)	loss 10.266 (10.266)	gnorm 432739.938 (432739.938)	prob 0.855 (0.8552)	GS 31.906 (31.906)	mem 43.612
Train: [3][610/750]	BT 11.250 (1.166)	DT 11.121 (1.033)	loss 9.812 (9.812)	gnorm 447510.781 (447510.781)	prob 1.486 (1.4861)	GS 31.891 (31.891)	mem 43.737
Train: [3][611/750]	BT 0.144 (1.164)	DT 0.007 (1.031)	loss 9.901 (9.901)	gnorm 466868.000 (466868.000)	prob 0.685 (0.6854)	GS 27.844 (27.844)	mem 43.665
Train: [3][612/750]	BT 0.131 (1.162)	DT 0.002 (1.030)	loss 9.580 (9.580)	gnorm 445231.156 (445231.156)	prob 0.921 (0.9208)	GS 31.828 (31.828)	mem 43.667
Train: [3][613/750]	BT 0.107 (1.160)	DT 0.006 (1.028)	loss 9.732 (9.732)	gnorm 448898.156 (448898.156)	prob 0.824 (0.8239)	GS 26.750 (26.750)	mem 43.669
Train: [3][614/750]	BT 0.085 (1.159)	DT 0.002 (1.026)	loss 10.083 (10.083)	gnorm 468108.125 (468108.125)	prob 0.262 (0.2624)	GS 36.297 (36.297)	mem 43.669
Train: [3][615/750]	BT 0.077 (1.157)	DT 0.001 (1.025)	loss 9.628 (9.628)	gnorm 436810.094 (436810.094)	prob 0.995 (0.9950)	GS 26.875 (26.875)	mem 43.670
Train: [3][616/750]	BT 0.325 (1.156)	DT 0.005 (1.023)	loss 10.384 (10.384)	gnorm 440778.531 (440778.531)	prob -0.203 (-0.2034)	GS 35.516 (35.516)	mem 43.765
Train: [3][617/750]	BT 0.083 (1.154)	DT 0.002 (1.021)	loss 9.582 (9.582)	gnorm 425416.500 (425416.500)	prob 0.540 (0.5403)	GS 37.141 (37.141)	mem 43.729
Train: [3][618/750]	BT 0.172 (1.152)	DT 0.010 (1.020)	loss 10.196 (10.196)	gnorm 439168.438 (439168.438)	prob 0.068 (0.0680)	GS 37.250 (37.250)	mem 43.902
Train: [3][619/750]	BT 0.127 (1.151)	DT 0.006 (1.018)	loss 9.871 (9.871)	gnorm 431771.625 (431771.625)	prob 0.853 (0.8528)	GS 31.516 (31.516)	mem 43.671
Train: [3][620/750]	BT 0.100 (1.149)	DT 0.001 (1.017)	loss 9.808 (9.808)	gnorm 422450.312 (422450.312)	prob 0.356 (0.3556)	GS 30.281 (30.281)	mem 43.766
Train: [3][621/750]	BT 0.104 (1.147)	DT 0.021 (1.015)	loss 9.997 (9.997)	gnorm 465907.125 (465907.125)	prob 0.217 (0.2169)	GS 34.875 (34.875)	mem 43.879
Train: [3][622/750]	BT 9.921 (1.161)	DT 9.838 (1.029)	loss 9.288 (9.288)	gnorm 404396.594 (404396.594)	prob 0.991 (0.9913)	GS 31.953 (31.953)	mem 43.645
Train: [3][623/750]	BT 0.107 (1.160)	DT 0.002 (1.027)	loss 10.040 (10.040)	gnorm 431393.656 (431393.656)	prob 0.714 (0.7140)	GS 34.641 (34.641)	mem 43.667
Train: [3][624/750]	BT 0.130 (1.158)	DT 0.002 (1.026)	loss 9.981 (9.981)	gnorm 382854.969 (382854.969)	prob 0.084 (0.0840)	GS 34.953 (34.953)	mem 43.768
Train: [3][625/750]	BT 0.211 (1.157)	DT 0.037 (1.024)	loss 9.362 (9.362)	gnorm 401706.250 (401706.250)	prob 0.919 (0.9188)	GS 28.312 (28.312)	mem 43.645
Train: [3][626/750]	BT 0.136 (1.155)	DT 0.002 (1.023)	loss 9.793 (9.793)	gnorm 416906.812 (416906.812)	prob 0.639 (0.6389)	GS 35.406 (35.406)	mem 43.652
Train: [3][627/750]	BT 0.179 (1.153)	DT 0.005 (1.021)	loss 9.576 (9.576)	gnorm 425921.375 (425921.375)	prob 0.964 (0.9636)	GS 29.969 (29.969)	mem 43.653
Train: [3][628/750]	BT 0.130 (1.152)	DT 0.037 (1.019)	loss 9.553 (9.553)	gnorm 416389.406 (416389.406)	prob 1.105 (1.1049)	GS 31.406 (31.406)	mem 43.651
Train: [3][629/750]	BT 0.099 (1.150)	DT 0.018 (1.018)	loss 9.781 (9.781)	gnorm 421945.156 (421945.156)	prob 0.771 (0.7713)	GS 32.125 (32.125)	mem 43.651
Train: [3][630/750]	BT 0.097 (1.148)	DT 0.003 (1.016)	loss 10.170 (10.170)	gnorm 443171.000 (443171.000)	prob 0.584 (0.5845)	GS 34.109 (34.109)	mem 43.651
Train: [3][631/750]	BT 0.129 (1.147)	DT 0.011 (1.015)	loss 9.482 (9.482)	gnorm 399227.750 (399227.750)	prob 1.108 (1.1079)	GS 33.188 (33.188)	mem 43.651
Train: [3][632/750]	BT 0.172 (1.145)	DT 0.024 (1.013)	loss 9.888 (9.888)	gnorm 421536.656 (421536.656)	prob 0.584 (0.5843)	GS 33.859 (33.859)	mem 43.653
Train: [3][633/750]	BT 0.091 (1.144)	DT 0.002 (1.011)	loss 9.827 (9.827)	gnorm 466989.094 (466989.094)	prob 0.234 (0.2336)	GS 29.547 (29.547)	mem 43.653
Train: [3][634/750]	BT 11.362 (1.160)	DT 11.304 (1.028)	loss 9.663 (9.663)	gnorm 403412.844 (403412.844)	prob 0.510 (0.5098)	GS 33.750 (33.750)	mem 43.710
Train: [3][635/750]	BT 0.138 (1.158)	DT 0.001 (1.026)	loss 10.361 (10.361)	gnorm 471635.875 (471635.875)	prob -0.287 (-0.2874)	GS 29.516 (29.516)	mem 43.710
Train: [3][636/750]	BT 0.089 (1.156)	DT 0.005 (1.024)	loss 9.813 (9.813)	gnorm 436133.125 (436133.125)	prob 0.497 (0.4969)	GS 32.328 (32.328)	mem 43.710
Train: [3][637/750]	BT 0.139 (1.155)	DT 0.002 (1.023)	loss 9.315 (9.315)	gnorm 441895.156 (441895.156)	prob 0.946 (0.9459)	GS 29.281 (29.281)	mem 43.787
Train: [3][638/750]	BT 0.090 (1.153)	DT 0.003 (1.021)	loss 9.910 (9.910)	gnorm 415404.250 (415404.250)	prob 1.072 (1.0722)	GS 38.125 (38.125)	mem 43.713
Train: [3][639/750]	BT 0.073 (1.151)	DT 0.001 (1.020)	loss 10.267 (10.267)	gnorm 435977.250 (435977.250)	prob 0.629 (0.6288)	GS 33.078 (33.078)	mem 43.723
Train: [3][640/750]	BT 0.161 (1.150)	DT 0.004 (1.018)	loss 9.499 (9.499)	gnorm 463458.281 (463458.281)	prob 1.071 (1.0715)	GS 32.641 (32.641)	mem 43.796
Train: [3][641/750]	BT 0.161 (1.148)	DT 0.008 (1.016)	loss 10.180 (10.180)	gnorm 430407.312 (430407.312)	prob 0.156 (0.1557)	GS 30.141 (30.141)	mem 43.796
Train: [3][642/750]	BT 0.088 (1.147)	DT 0.002 (1.015)	loss 9.909 (9.909)	gnorm 388300.219 (388300.219)	prob 0.504 (0.5039)	GS 36.047 (36.047)	mem 43.761
Train: [3][643/750]	BT 0.095 (1.145)	DT 0.002 (1.013)	loss 9.796 (9.796)	gnorm 421134.500 (421134.500)	prob 0.391 (0.3914)	GS 30.188 (30.188)	mem 43.864
Train: [3][644/750]	BT 0.148 (1.143)	DT 0.002 (1.012)	loss 9.693 (9.693)	gnorm 430580.844 (430580.844)	prob 0.583 (0.5828)	GS 32.766 (32.766)	mem 43.732
Train: [3][645/750]	BT 0.090 (1.142)	DT 0.003 (1.010)	loss 9.578 (9.578)	gnorm 410290.344 (410290.344)	prob 0.353 (0.3533)	GS 24.453 (24.453)	mem 43.733
Train: [3][646/750]	BT 13.838 (1.161)	DT 13.694 (1.030)	loss 10.218 (10.218)	gnorm 400361.406 (400361.406)	prob -0.333 (-0.3330)	GS 31.406 (31.406)	mem 43.586
Train: [3][647/750]	BT 0.203 (1.160)	DT 0.002 (1.028)	loss 10.237 (10.237)	gnorm 422273.969 (422273.969)	prob 0.376 (0.3757)	GS 26.844 (26.844)	mem 43.601
Train: [3][648/750]	BT 0.122 (1.158)	DT 0.007 (1.027)	loss 9.730 (9.730)	gnorm 436883.688 (436883.688)	prob 0.462 (0.4621)	GS 34.984 (34.984)	mem 43.603
Train: [3][649/750]	BT 0.063 (1.157)	DT 0.001 (1.025)	loss 9.989 (9.989)	gnorm 423563.531 (423563.531)	prob 0.654 (0.6545)	GS 30.719 (30.719)	mem 43.603
Train: [3][650/750]	BT 0.100 (1.155)	DT 0.001 (1.023)	loss 10.114 (10.114)	gnorm 422753.625 (422753.625)	prob 0.295 (0.2949)	GS 32.547 (32.547)	mem 43.603
Train: [3][651/750]	BT 0.134 (1.154)	DT 0.002 (1.022)	loss 10.113 (10.113)	gnorm 441138.188 (441138.188)	prob 1.082 (1.0823)	GS 38.281 (38.281)	mem 43.603
Train: [3][652/750]	BT 0.148 (1.152)	DT 0.007 (1.020)	loss 9.717 (9.717)	gnorm 445781.938 (445781.938)	prob 1.384 (1.3841)	GS 33.828 (33.828)	mem 43.603
Train: [3][653/750]	BT 0.135 (1.150)	DT 0.005 (1.019)	loss 9.605 (9.605)	gnorm 419264.562 (419264.562)	prob 0.918 (0.9183)	GS 28.641 (28.641)	mem 43.627
Train: [3][654/750]	BT 0.108 (1.149)	DT 0.004 (1.017)	loss 9.899 (9.899)	gnorm 401094.406 (401094.406)	prob 1.079 (1.0795)	GS 34.234 (34.234)	mem 43.584
Train: [3][655/750]	BT 0.176 (1.147)	DT 0.001 (1.016)	loss 9.635 (9.635)	gnorm 400232.125 (400232.125)	prob 1.292 (1.2915)	GS 33.750 (33.750)	mem 43.662
Train: [3][656/750]	BT 0.232 (1.146)	DT 0.008 (1.014)	loss 9.434 (9.434)	gnorm 404917.656 (404917.656)	prob 1.650 (1.6498)	GS 29.109 (29.109)	mem 43.532
Train: [3][657/750]	BT 0.188 (1.145)	DT 0.005 (1.013)	loss 10.162 (10.162)	gnorm 446104.281 (446104.281)	prob 1.229 (1.2288)	GS 29.781 (29.781)	mem 43.535
Train: [3][658/750]	BT 10.796 (1.159)	DT 10.724 (1.027)	loss 10.063 (10.063)	gnorm 434232.625 (434232.625)	prob 0.866 (0.8664)	GS 33.859 (33.859)	mem 43.629
Train: [3][659/750]	BT 0.142 (1.158)	DT 0.001 (1.026)	loss 10.216 (10.216)	gnorm 517942.281 (517942.281)	prob 1.074 (1.0736)	GS 32.047 (32.047)	mem 43.701
Train: [3][660/750]	BT 0.078 (1.156)	DT 0.002 (1.024)	loss 9.960 (9.960)	gnorm 445875.875 (445875.875)	prob 0.549 (0.5491)	GS 30.469 (30.469)	mem 43.617
Train: [3][661/750]	BT 0.110 (1.154)	DT 0.001 (1.023)	loss 9.888 (9.888)	gnorm 445937.375 (445937.375)	prob 0.507 (0.5067)	GS 28.875 (28.875)	mem 43.730
Train: [3][662/750]	BT 0.175 (1.153)	DT 0.006 (1.021)	loss 9.722 (9.722)	gnorm 398048.312 (398048.312)	prob 0.436 (0.4357)	GS 33.000 (33.000)	mem 43.624
Train: [3][663/750]	BT 0.080 (1.151)	DT 0.002 (1.020)	loss 9.819 (9.819)	gnorm 388601.469 (388601.469)	prob 0.117 (0.1167)	GS 35.766 (35.766)	mem 43.584
Train: [3][664/750]	BT 0.155 (1.150)	DT 0.002 (1.018)	loss 9.984 (9.984)	gnorm 395420.250 (395420.250)	prob 0.243 (0.2428)	GS 34.594 (34.594)	mem 43.591
Train: [3][665/750]	BT 0.168 (1.148)	DT 0.005 (1.017)	loss 9.863 (9.863)	gnorm 430332.344 (430332.344)	prob -0.035 (-0.0346)	GS 34.422 (34.422)	mem 43.520
Train: [3][666/750]	BT 0.126 (1.147)	DT 0.002 (1.015)	loss 9.555 (9.555)	gnorm 410365.812 (410365.812)	prob 0.658 (0.6575)	GS 33.547 (33.547)	mem 43.559
Train: [3][667/750]	BT 0.137 (1.145)	DT 0.006 (1.014)	loss 9.923 (9.923)	gnorm 404614.750 (404614.750)	prob 0.227 (0.2269)	GS 32.672 (32.672)	mem 43.569
Train: [3][668/750]	BT 0.186 (1.144)	DT 0.004 (1.012)	loss 10.151 (10.151)	gnorm 430754.094 (430754.094)	prob 0.068 (0.0685)	GS 32.922 (32.922)	mem 43.608
Train: [3][669/750]	BT 0.101 (1.142)	DT 0.002 (1.011)	loss 9.941 (9.941)	gnorm 436229.469 (436229.469)	prob 0.344 (0.3444)	GS 27.203 (27.203)	mem 43.639
Train: [3][670/750]	BT 12.493 (1.159)	DT 12.369 (1.028)	loss 9.888 (9.888)	gnorm 410273.844 (410273.844)	prob 0.265 (0.2645)	GS 40.391 (40.391)	mem 43.618
Train: [3][671/750]	BT 0.113 (1.158)	DT 0.004 (1.026)	loss 9.671 (9.671)	gnorm 409421.469 (409421.469)	prob 0.783 (0.7828)	GS 29.312 (29.312)	mem 43.488
Train: [3][672/750]	BT 0.089 (1.156)	DT 0.007 (1.024)	loss 9.794 (9.794)	gnorm 432334.562 (432334.562)	prob 0.313 (0.3129)	GS 34.203 (34.203)	mem 43.488
Train: [3][673/750]	BT 0.112 (1.155)	DT 0.002 (1.023)	loss 10.181 (10.181)	gnorm 422468.719 (422468.719)	prob 0.334 (0.3339)	GS 32.391 (32.391)	mem 43.488
Train: [3][674/750]	BT 0.168 (1.153)	DT 0.002 (1.021)	loss 9.914 (9.914)	gnorm 446620.531 (446620.531)	prob 1.079 (1.0786)	GS 35.859 (35.859)	mem 43.487
Train: [3][675/750]	BT 0.121 (1.152)	DT 0.002 (1.020)	loss 10.344 (10.344)	gnorm 418776.562 (418776.562)	prob 0.291 (0.2908)	GS 30.422 (30.422)	mem 43.488
Train: [3][676/750]	BT 0.102 (1.150)	DT 0.002 (1.018)	loss 9.623 (9.623)	gnorm 450717.281 (450717.281)	prob 0.744 (0.7445)	GS 28.703 (28.703)	mem 43.488
Train: [3][677/750]	BT 0.125 (1.148)	DT 0.001 (1.017)	loss 10.087 (10.087)	gnorm 398276.531 (398276.531)	prob -0.229 (-0.2294)	GS 28.953 (28.953)	mem 43.488
Train: [3][678/750]	BT 0.108 (1.147)	DT 0.010 (1.015)	loss 9.685 (9.685)	gnorm 430625.344 (430625.344)	prob 0.444 (0.4442)	GS 33.594 (33.594)	mem 43.488
Train: [3][679/750]	BT 0.146 (1.145)	DT 0.002 (1.014)	loss 10.386 (10.386)	gnorm 474347.625 (474347.625)	prob 0.162 (0.1623)	GS 31.953 (31.953)	mem 43.540
Train: [3][680/750]	BT 0.106 (1.144)	DT 0.003 (1.012)	loss 10.011 (10.011)	gnorm 451729.344 (451729.344)	prob 0.779 (0.7786)	GS 33.797 (33.797)	mem 43.542
Train: [3][681/750]	BT 0.085 (1.142)	DT 0.001 (1.011)	loss 9.808 (9.808)	gnorm 442888.781 (442888.781)	prob 0.709 (0.7095)	GS 31.234 (31.234)	mem 43.606
Train: [3][682/750]	BT 12.952 (1.160)	DT 12.857 (1.028)	loss 10.250 (10.250)	gnorm 457795.250 (457795.250)	prob 0.066 (0.0657)	GS 37.078 (37.078)	mem 43.613
Train: [3][683/750]	BT 0.145 (1.158)	DT 0.002 (1.027)	loss 9.767 (9.767)	gnorm 401271.906 (401271.906)	prob 1.000 (0.9999)	GS 29.703 (29.703)	mem 43.527
Train: [3][684/750]	BT 0.119 (1.157)	DT 0.002 (1.025)	loss 9.858 (9.858)	gnorm 458297.719 (458297.719)	prob 0.615 (0.6149)	GS 34.766 (34.766)	mem 43.489
Train: [3][685/750]	BT 0.061 (1.155)	DT 0.001 (1.024)	loss 10.138 (10.138)	gnorm 470263.906 (470263.906)	prob 0.482 (0.4820)	GS 30.094 (30.094)	mem 43.511
Train: [3][686/750]	BT 0.075 (1.154)	DT 0.002 (1.022)	loss 10.043 (10.043)	gnorm 442732.250 (442732.250)	prob 0.060 (0.0605)	GS 30.266 (30.266)	mem 43.587
Train: [3][687/750]	BT 0.084 (1.152)	DT 0.002 (1.021)	loss 10.188 (10.188)	gnorm 419891.750 (419891.750)	prob 1.011 (1.0106)	GS 34.344 (34.344)	mem 43.490
Train: [3][688/750]	BT 0.071 (1.150)	DT 0.002 (1.019)	loss 10.318 (10.318)	gnorm 456964.719 (456964.719)	prob 0.901 (0.9007)	GS 35.609 (35.609)	mem 43.525
Train: [3][689/750]	BT 0.081 (1.149)	DT 0.003 (1.018)	loss 9.303 (9.303)	gnorm 413184.594 (413184.594)	prob 1.693 (1.6926)	GS 33.047 (33.047)	mem 43.490
Train: [3][690/750]	BT 0.079 (1.147)	DT 0.001 (1.016)	loss 9.852 (9.852)	gnorm 387195.188 (387195.188)	prob 1.067 (1.0667)	GS 33.828 (33.828)	mem 43.490
Train: [3][691/750]	BT 0.070 (1.146)	DT 0.002 (1.015)	loss 9.697 (9.697)	gnorm 407487.094 (407487.094)	prob 0.793 (0.7931)	GS 26.875 (26.875)	mem 43.517
Train: [3][692/750]	BT 0.200 (1.144)	DT 0.002 (1.013)	loss 9.657 (9.657)	gnorm 407710.406 (407710.406)	prob 0.712 (0.7125)	GS 31.969 (31.969)	mem 43.681
Train: [3][693/750]	BT 0.114 (1.143)	DT 0.002 (1.012)	loss 9.866 (9.866)	gnorm 481806.906 (481806.906)	prob 1.166 (1.1664)	GS 32.000 (32.000)	mem 43.666
Train: [3][694/750]	BT 12.362 (1.159)	DT 12.269 (1.028)	loss 10.090 (10.090)	gnorm 424862.938 (424862.938)	prob 0.245 (0.2448)	GS 32.422 (32.422)	mem 43.546
Train: [3][695/750]	BT 0.127 (1.158)	DT 0.002 (1.027)	loss 9.817 (9.817)	gnorm 432897.188 (432897.188)	prob 0.983 (0.9827)	GS 30.312 (30.312)	mem 43.545
Train: [3][696/750]	BT 0.098 (1.156)	DT 0.010 (1.025)	loss 9.757 (9.757)	gnorm 415082.812 (415082.812)	prob 0.828 (0.8284)	GS 31.500 (31.500)	mem 43.547
Train: [3][697/750]	BT 0.121 (1.155)	DT 0.001 (1.024)	loss 9.978 (9.978)	gnorm 465839.125 (465839.125)	prob 0.312 (0.3125)	GS 30.359 (30.359)	mem 43.547
Train: [3][698/750]	BT 0.110 (1.153)	DT 0.010 (1.022)	loss 9.461 (9.461)	gnorm 461332.719 (461332.719)	prob 0.483 (0.4827)	GS 35.609 (35.609)	mem 43.548
Train: [3][699/750]	BT 0.119 (1.152)	DT 0.002 (1.021)	loss 9.915 (9.915)	gnorm 466024.688 (466024.688)	prob 0.852 (0.8521)	GS 33.828 (33.828)	mem 43.548
Train: [3][700/750]	BT 0.120 (1.150)	DT 0.001 (1.019)	loss 10.052 (10.052)	gnorm 422620.812 (422620.812)	prob 0.462 (0.4619)	GS 30.016 (30.016)	mem 43.548
Train: [3][701/750]	BT 0.157 (1.149)	DT 0.009 (1.018)	loss 9.415 (9.415)	gnorm 409358.500 (409358.500)	prob 0.957 (0.9568)	GS 32.812 (32.812)	mem 43.564
Train: [3][702/750]	BT 0.134 (1.147)	DT 0.017 (1.017)	loss 9.766 (9.766)	gnorm 402793.625 (402793.625)	prob 0.904 (0.9044)	GS 34.578 (34.578)	mem 43.677
Train: [3][703/750]	BT 0.147 (1.146)	DT 0.002 (1.015)	loss 9.583 (9.583)	gnorm 409581.281 (409581.281)	prob 0.300 (0.3001)	GS 28.922 (28.922)	mem 43.559
Train: [3][704/750]	BT 0.091 (1.144)	DT 0.002 (1.014)	loss 10.119 (10.119)	gnorm 400971.188 (400971.188)	prob 0.205 (0.2049)	GS 32.984 (32.984)	mem 43.559
Train: [3][705/750]	BT 0.144 (1.143)	DT 0.002 (1.012)	loss 9.737 (9.737)	gnorm 412068.312 (412068.312)	prob 0.375 (0.3746)	GS 34.203 (34.203)	mem 43.560
Train: [3][706/750]	BT 13.128 (1.160)	DT 13.055 (1.029)	loss 9.862 (9.862)	gnorm 408169.250 (408169.250)	prob 0.181 (0.1811)	GS 36.469 (36.469)	mem 43.515
Train: [3][707/750]	BT 0.078 (1.158)	DT 0.001 (1.028)	loss 10.059 (10.059)	gnorm 447218.375 (447218.375)	prob -0.074 (-0.0735)	GS 31.188 (31.188)	mem 43.531
Train: [3][708/750]	BT 0.107 (1.157)	DT 0.002 (1.026)	loss 9.697 (9.697)	gnorm 423615.750 (423615.750)	prob 0.735 (0.7347)	GS 34.578 (34.578)	mem 43.603
Train: [3][709/750]	BT 0.110 (1.155)	DT 0.003 (1.025)	loss 9.812 (9.812)	gnorm 446266.906 (446266.906)	prob 0.205 (0.2052)	GS 31.172 (31.172)	mem 43.707
Train: [3][710/750]	BT 0.130 (1.154)	DT 0.012 (1.024)	loss 10.401 (10.401)	gnorm 400181.500 (400181.500)	prob -0.402 (-0.4020)	GS 34.156 (34.156)	mem 44.144
Train: [3][711/750]	BT 0.217 (1.153)	DT 0.006 (1.022)	loss 10.064 (10.064)	gnorm 438516.125 (438516.125)	prob 0.095 (0.0954)	GS 29.812 (29.812)	mem 44.198
Train: [3][712/750]	BT 0.174 (1.151)	DT 0.022 (1.021)	loss 10.119 (10.119)	gnorm 418404.062 (418404.062)	prob -0.318 (-0.3182)	GS 33.812 (33.812)	mem 44.159
Train: [3][713/750]	BT 0.131 (1.150)	DT 0.010 (1.019)	loss 10.252 (10.252)	gnorm 422518.406 (422518.406)	prob 0.194 (0.1940)	GS 29.062 (29.062)	mem 44.087
Train: [3][714/750]	BT 0.087 (1.148)	DT 0.002 (1.018)	loss 10.122 (10.122)	gnorm 378240.062 (378240.062)	prob -0.084 (-0.0842)	GS 32.078 (32.078)	mem 43.527
Train: [3][715/750]	BT 0.105 (1.147)	DT 0.002 (1.016)	loss 9.764 (9.764)	gnorm 429754.844 (429754.844)	prob 0.490 (0.4903)	GS 27.594 (27.594)	mem 43.638
Train: [3][716/750]	BT 0.153 (1.145)	DT 0.007 (1.015)	loss 9.753 (9.753)	gnorm 430777.188 (430777.188)	prob 0.305 (0.3053)	GS 33.234 (33.234)	mem 43.852
Train: [3][717/750]	BT 0.158 (1.144)	DT 0.050 (1.014)	loss 9.284 (9.284)	gnorm 427397.844 (427397.844)	prob 0.999 (0.9985)	GS 38.812 (38.812)	mem 43.843
Train: [3][718/750]	BT 7.285 (1.153)	DT 7.111 (1.022)	loss 10.123 (10.123)	gnorm 407022.531 (407022.531)	prob -0.080 (-0.0800)	GS 31.812 (31.812)	mem 43.587
Train: [3][719/750]	BT 0.129 (1.151)	DT 0.006 (1.021)	loss 9.646 (9.646)	gnorm 422350.219 (422350.219)	prob 0.850 (0.8496)	GS 27.594 (27.594)	mem 43.587
Train: [3][720/750]	BT 0.104 (1.150)	DT 0.001 (1.019)	loss 9.915 (9.915)	gnorm 394535.938 (394535.938)	prob 0.673 (0.6729)	GS 32.922 (32.922)	mem 43.588
Train: [3][721/750]	BT 0.142 (1.148)	DT 0.003 (1.018)	loss 9.565 (9.565)	gnorm 392630.875 (392630.875)	prob 0.808 (0.8080)	GS 30.578 (30.578)	mem 43.600
Train: [3][722/750]	BT 0.163 (1.147)	DT 0.002 (1.017)	loss 9.468 (9.468)	gnorm 409042.750 (409042.750)	prob 0.904 (0.9039)	GS 36.359 (36.359)	mem 43.779
Train: [3][723/750]	BT 0.200 (1.146)	DT 0.018 (1.015)	loss 10.711 (10.711)	gnorm 424101.625 (424101.625)	prob -0.660 (-0.6596)	GS 32.844 (32.844)	mem 43.773
Train: [3][724/750]	BT 0.153 (1.144)	DT 0.011 (1.014)	loss 9.902 (9.902)	gnorm 434715.938 (434715.938)	prob 0.223 (0.2230)	GS 35.000 (35.000)	mem 43.588
Train: [3][725/750]	BT 0.074 (1.143)	DT 0.002 (1.012)	loss 10.113 (10.113)	gnorm 414255.281 (414255.281)	prob -0.285 (-0.2853)	GS 31.062 (31.062)	mem 43.646
Train: [3][726/750]	BT 0.087 (1.141)	DT 0.001 (1.011)	loss 10.417 (10.417)	gnorm 453427.062 (453427.062)	prob -0.477 (-0.4768)	GS 30.469 (30.469)	mem 43.590
Train: [3][727/750]	BT 0.088 (1.140)	DT 0.002 (1.010)	loss 9.753 (9.753)	gnorm 434836.594 (434836.594)	prob 0.281 (0.2809)	GS 30.688 (30.688)	mem 43.597
Train: [3][728/750]	BT 0.088 (1.139)	DT 0.002 (1.008)	loss 9.997 (9.997)	gnorm 415259.812 (415259.812)	prob 0.065 (0.0653)	GS 33.172 (33.172)	mem 43.646
Train: [3][729/750]	BT 0.183 (1.137)	DT 0.002 (1.007)	loss 9.596 (9.596)	gnorm 423989.938 (423989.938)	prob 0.997 (0.9970)	GS 27.891 (27.891)	mem 43.819
Train: [3][730/750]	BT 12.880 (1.153)	DT 12.789 (1.023)	loss 9.714 (9.714)	gnorm 409711.000 (409711.000)	prob 0.551 (0.5510)	GS 34.766 (34.766)	mem 43.059
Train: [3][731/750]	BT 0.059 (1.152)	DT 0.001 (1.022)	loss 9.360 (9.360)	gnorm 416214.781 (416214.781)	prob 0.973 (0.9727)	GS 33.922 (33.922)	mem 43.059
Train: [3][732/750]	BT 0.072 (1.150)	DT 0.001 (1.020)	loss 10.031 (10.031)	gnorm 407229.594 (407229.594)	prob 0.394 (0.3935)	GS 32.062 (32.062)	mem 43.069
Train: [3][733/750]	BT 0.079 (1.149)	DT 0.002 (1.019)	loss 10.137 (10.137)	gnorm 435628.219 (435628.219)	prob 0.052 (0.0521)	GS 33.297 (33.297)	mem 43.164
Train: [3][734/750]	BT 0.084 (1.147)	DT 0.001 (1.017)	loss 9.341 (9.341)	gnorm 402735.000 (402735.000)	prob 0.702 (0.7016)	GS 30.969 (30.969)	mem 43.093
Train: [3][735/750]	BT 0.110 (1.146)	DT 0.027 (1.016)	loss 9.441 (9.441)	gnorm 396937.000 (396937.000)	prob 0.554 (0.5535)	GS 28.281 (28.281)	mem 43.061
Train: [3][736/750]	BT 0.080 (1.145)	DT 0.002 (1.015)	loss 9.927 (9.927)	gnorm 435090.500 (435090.500)	prob -0.416 (-0.4163)	GS 34.953 (34.953)	mem 43.061
Train: [3][737/750]	BT 0.086 (1.143)	DT 0.002 (1.013)	loss 9.583 (9.583)	gnorm 394150.875 (394150.875)	prob 0.181 (0.1808)	GS 30.484 (30.484)	mem 43.024
Train: [3][738/750]	BT 0.073 (1.142)	DT 0.001 (1.012)	loss 9.473 (9.473)	gnorm 404597.688 (404597.688)	prob 0.068 (0.0678)	GS 28.891 (28.891)	mem 43.063
Train: [3][739/750]	BT 0.112 (1.140)	DT 0.002 (1.011)	loss 9.059 (9.059)	gnorm 406021.312 (406021.312)	prob 0.863 (0.8630)	GS 35.953 (35.953)	mem 43.119
Train: [3][740/750]	BT 0.077 (1.139)	DT 0.002 (1.009)	loss 9.838 (9.838)	gnorm 417068.094 (417068.094)	prob 0.434 (0.4341)	GS 30.609 (30.609)	mem 43.026
Train: [3][741/750]	BT 0.077 (1.137)	DT 0.001 (1.008)	loss 9.929 (9.929)	gnorm 423986.156 (423986.156)	prob 0.099 (0.0991)	GS 30.719 (30.719)	mem 43.026
Train: [3][742/750]	BT 5.984 (1.144)	DT 5.922 (1.014)	loss 9.580 (9.580)	gnorm 429537.406 (429537.406)	prob 0.854 (0.8537)	GS 32.453 (32.453)	mem 11.317
Train: [3][743/750]	BT 0.070 (1.142)	DT 0.001 (1.013)	loss 9.855 (9.855)	gnorm 488916.500 (488916.500)	prob 0.121 (0.1206)	GS 31.344 (31.344)	mem 11.317
Train: [3][744/750]	BT 0.073 (1.141)	DT 0.001 (1.012)	loss 9.366 (9.366)	gnorm 411600.125 (411600.125)	prob 0.954 (0.9536)	GS 32.438 (32.438)	mem 11.317
Train: [3][745/750]	BT 0.076 (1.140)	DT 0.001 (1.010)	loss 10.048 (10.048)	gnorm 628245.562 (628245.562)	prob 0.207 (0.2065)	GS 28.438 (28.438)	mem 11.317
Train: [3][746/750]	BT 0.087 (1.138)	DT 0.001 (1.009)	loss 11.236 (11.236)	gnorm 630146.375 (630146.375)	prob -1.385 (-1.3849)	GS 37.750 (37.750)	mem 11.317
Train: [3][747/750]	BT 0.054 (1.137)	DT 0.001 (1.008)	loss 9.405 (9.405)	gnorm 595733.125 (595733.125)	prob 0.866 (0.8661)	GS 28.188 (28.188)	mem 11.317
Train: [3][748/750]	BT 0.068 (1.135)	DT 0.001 (1.006)	loss 10.029 (10.029)	gnorm 649383.062 (649383.062)	prob 0.487 (0.4871)	GS 33.469 (33.469)	mem 11.317
Train: [3][749/750]	BT 0.073 (1.134)	DT 0.001 (1.005)	loss 9.981 (9.981)	gnorm 608624.938 (608624.938)	prob 0.480 (0.4797)	GS 39.406 (39.406)	mem 11.318
Train: [3][750/750]	BT 0.104 (1.133)	DT 0.002 (1.004)	loss 10.084 (10.084)	gnorm 619167.938 (619167.938)	prob 0.138 (0.1384)	GS 34.156 (34.156)	mem 11.318
Train: [3][751/750]	BT 0.057 (1.131)	DT 0.001 (1.002)	loss 10.214 (10.214)	gnorm 640422.938 (640422.938)	prob 0.193 (0.1935)	GS 28.031 (28.031)	mem 11.318
Train: [3][752/750]	BT 0.081 (1.130)	DT 0.002 (1.001)	loss 9.488 (9.488)	gnorm 579177.812 (579177.812)	prob 0.965 (0.9653)	GS 32.688 (32.688)	mem 11.318
Train: [3][753/750]	BT 0.068 (1.128)	DT 0.001 (1.000)	loss 10.350 (10.350)	gnorm 636284.875 (636284.875)	prob 0.678 (0.6779)	GS 30.062 (30.062)	mem 11.319
Train: [3][754/750]	BT 2.290 (1.130)	DT 2.223 (1.001)	loss 10.238 (10.238)	gnorm 618380.875 (618380.875)	prob 0.761 (0.7606)	GS 34.281 (34.281)	mem 11.288
Train: [3][755/750]	BT 0.077 (1.128)	DT 0.001 (1.000)	loss 9.871 (9.871)	gnorm 571573.188 (571573.188)	prob 1.004 (1.0038)	GS 33.750 (33.750)	mem 11.288
Train: [3][756/750]	BT 0.068 (1.127)	DT 0.001 (0.999)	loss 9.964 (9.964)	gnorm 541568.125 (541568.125)	prob 1.103 (1.1025)	GS 32.156 (32.156)	mem 11.288
epoch 3, total time 852.23
==> Saving...
==> Saving...
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [4][1/750]	BT 21.970 (21.970)	DT 21.860 (21.860)	loss 10.124 (10.124)	gnorm 473545.312 (473545.312)	prob 0.828 (0.8280)	GS 29.266 (29.266)	mem 42.513
Train: [4][2/750]	BT 0.845 (11.407)	DT 0.601 (11.230)	loss 9.762 (9.762)	gnorm 411182.750 (411182.750)	prob 0.232 (0.2319)	GS 34.781 (34.781)	mem 42.307
Train: [4][3/750]	BT 1.809 (8.208)	DT 1.623 (8.028)	loss 9.824 (9.824)	gnorm 434281.750 (434281.750)	prob 0.249 (0.2493)	GS 34.672 (34.672)	mem 42.330
Train: [4][4/750]	BT 0.182 (6.201)	DT 0.001 (6.021)	loss 9.759 (9.759)	gnorm 408291.781 (408291.781)	prob 0.152 (0.1525)	GS 36.062 (36.062)	mem 42.365
Train: [4][5/750]	BT 0.144 (4.990)	DT 0.047 (4.826)	loss 9.810 (9.810)	gnorm 457424.094 (457424.094)	prob 0.402 (0.4017)	GS 30.656 (30.656)	mem 42.359
Train: [4][6/750]	BT 0.154 (4.184)	DT 0.002 (4.022)	loss 10.499 (10.499)	gnorm 388621.375 (388621.375)	prob -0.974 (-0.9737)	GS 31.422 (31.422)	mem 42.309
Train: [4][7/750]	BT 0.208 (3.616)	DT 0.006 (3.448)	loss 10.478 (10.478)	gnorm 426424.375 (426424.375)	prob -0.404 (-0.4035)	GS 39.047 (39.047)	mem 42.376
Train: [4][8/750]	BT 0.134 (3.181)	DT 0.007 (3.018)	loss 9.762 (9.762)	gnorm 441150.781 (441150.781)	prob 0.737 (0.7368)	GS 36.297 (36.297)	mem 42.376
Train: [4][9/750]	BT 0.114 (2.840)	DT 0.004 (2.683)	loss 9.873 (9.873)	gnorm 428492.875 (428492.875)	prob -0.097 (-0.0966)	GS 32.266 (32.266)	mem 42.376
Train: [4][10/750]	BT 0.101 (2.566)	DT 0.006 (2.416)	loss 9.779 (9.779)	gnorm 428264.094 (428264.094)	prob -0.023 (-0.0230)	GS 33.391 (33.391)	mem 42.311
Train: [4][11/750]	BT 0.126 (2.344)	DT 0.003 (2.196)	loss 10.574 (10.574)	gnorm 498832.594 (498832.594)	prob -1.102 (-1.1016)	GS 30.344 (30.344)	mem 42.310
Train: [4][12/750]	BT 0.088 (2.156)	DT 0.003 (2.014)	loss 9.026 (9.026)	gnorm 438474.938 (438474.938)	prob 0.348 (0.3477)	GS 34.984 (34.984)	mem 42.327
Train: [4][13/750]	BT 7.877 (2.596)	DT 7.766 (2.456)	loss 9.915 (9.915)	gnorm 418536.656 (418536.656)	prob -0.669 (-0.6692)	GS 33.828 (33.828)	mem 42.475
Train: [4][14/750]	BT 2.511 (2.590)	DT 2.401 (2.452)	loss 9.399 (9.399)	gnorm 372338.000 (372338.000)	prob -0.112 (-0.1118)	GS 36.406 (36.406)	mem 42.594
Train: [4][15/750]	BT 0.089 (2.423)	DT 0.002 (2.289)	loss 9.485 (9.485)	gnorm 404250.594 (404250.594)	prob 0.073 (0.0735)	GS 25.922 (25.922)	mem 42.665
Train: [4][16/750]	BT 4.343 (2.543)	DT 4.239 (2.411)	loss 9.507 (9.507)	gnorm 371410.688 (371410.688)	prob -0.471 (-0.4713)	GS 32.672 (32.672)	mem 42.508
Train: [4][17/750]	BT 0.105 (2.400)	DT 0.005 (2.269)	loss 9.361 (9.361)	gnorm 393736.062 (393736.062)	prob 0.035 (0.0354)	GS 28.703 (28.703)	mem 42.692
Train: [4][18/750]	BT 0.113 (2.273)	DT 0.006 (2.143)	loss 9.279 (9.279)	gnorm 388726.375 (388726.375)	prob 0.279 (0.2794)	GS 35.453 (35.453)	mem 42.602
Train: [4][19/750]	BT 0.091 (2.158)	DT 0.003 (2.031)	loss 10.439 (10.439)	gnorm 433065.375 (433065.375)	prob -0.288 (-0.2885)	GS 33.016 (33.016)	mem 42.602
Train: [4][20/750]	BT 0.106 (2.055)	DT 0.003 (1.929)	loss 9.787 (9.787)	gnorm 407529.531 (407529.531)	prob -0.426 (-0.4257)	GS 35.891 (35.891)	mem 42.602
Train: [4][21/750]	BT 0.089 (1.962)	DT 0.003 (1.838)	loss 9.914 (9.914)	gnorm 395719.625 (395719.625)	prob -0.962 (-0.9619)	GS 27.594 (27.594)	mem 42.453
Train: [4][22/750]	BT 0.109 (1.878)	DT 0.003 (1.754)	loss 8.960 (8.960)	gnorm 368260.562 (368260.562)	prob 0.314 (0.3139)	GS 35.188 (35.188)	mem 42.464
Train: [4][23/750]	BT 0.197 (1.804)	DT 0.004 (1.678)	loss 10.051 (10.051)	gnorm 421641.250 (421641.250)	prob -0.290 (-0.2897)	GS 31.656 (31.656)	mem 42.454
Train: [4][24/750]	BT 0.093 (1.733)	DT 0.006 (1.608)	loss 9.952 (9.952)	gnorm 399377.938 (399377.938)	prob -0.421 (-0.4210)	GS 33.375 (33.375)	mem 42.455
Train: [4][25/750]	BT 2.299 (1.756)	DT 2.139 (1.630)	loss 10.077 (10.077)	gnorm 454291.719 (454291.719)	prob -0.156 (-0.1556)	GS 32.859 (32.859)	mem 42.519
Train: [4][26/750]	BT 6.773 (1.949)	DT 6.584 (1.820)	loss 9.345 (9.345)	gnorm 418080.406 (418080.406)	prob 0.521 (0.5208)	GS 33.875 (33.875)	mem 42.583
Train: [4][27/750]	BT 0.095 (1.880)	DT 0.002 (1.753)	loss 9.542 (9.542)	gnorm 435528.469 (435528.469)	prob 1.149 (1.1488)	GS 31.547 (31.547)	mem 42.548
Train: [4][28/750]	BT 6.139 (2.032)	DT 5.967 (1.903)	loss 9.317 (9.317)	gnorm 396457.844 (396457.844)	prob 0.952 (0.9517)	GS 34.969 (34.969)	mem 42.608
Train: [4][29/750]	BT 0.155 (1.967)	DT 0.002 (1.838)	loss 9.812 (9.812)	gnorm 383344.000 (383344.000)	prob 0.154 (0.1539)	GS 34.656 (34.656)	mem 42.576
Train: [4][30/750]	BT 0.235 (1.910)	DT 0.007 (1.777)	loss 10.008 (10.008)	gnorm 405570.656 (405570.656)	prob -0.588 (-0.5884)	GS 32.438 (32.438)	mem 42.680
Train: [4][31/750]	BT 0.191 (1.854)	DT 0.004 (1.720)	loss 9.735 (9.735)	gnorm 450759.625 (450759.625)	prob -0.081 (-0.0808)	GS 34.391 (34.391)	mem 42.678
Train: [4][32/750]	BT 0.189 (1.802)	DT 0.007 (1.666)	loss 9.222 (9.222)	gnorm 396362.125 (396362.125)	prob 0.558 (0.5584)	GS 34.641 (34.641)	mem 42.583
Train: [4][33/750]	BT 0.085 (1.750)	DT 0.002 (1.616)	loss 9.664 (9.664)	gnorm 392383.844 (392383.844)	prob 0.068 (0.0678)	GS 26.531 (26.531)	mem 42.583
Train: [4][34/750]	BT 0.086 (1.701)	DT 0.003 (1.568)	loss 9.704 (9.704)	gnorm 473026.719 (473026.719)	prob -0.046 (-0.0456)	GS 35.703 (35.703)	mem 42.583
Train: [4][35/750]	BT 0.086 (1.655)	DT 0.003 (1.523)	loss 10.155 (10.155)	gnorm 430965.688 (430965.688)	prob -0.352 (-0.3523)	GS 26.906 (26.906)	mem 42.610
Train: [4][36/750]	BT 0.123 (1.613)	DT 0.002 (1.481)	loss 9.717 (9.717)	gnorm 401982.094 (401982.094)	prob -0.437 (-0.4369)	GS 31.891 (31.891)	mem 42.658
Train: [4][37/750]	BT 0.237 (1.575)	DT 0.005 (1.441)	loss 9.834 (9.834)	gnorm 410275.844 (410275.844)	prob -0.008 (-0.0081)	GS 30.594 (30.594)	mem 42.585
Train: [4][38/750]	BT 5.975 (1.691)	DT 5.807 (1.556)	loss 9.762 (9.762)	gnorm 397158.938 (397158.938)	prob 0.113 (0.1130)	GS 33.312 (33.312)	mem 42.781
Train: [4][39/750]	BT 0.100 (1.650)	DT 0.001 (1.516)	loss 9.578 (9.578)	gnorm 379513.719 (379513.719)	prob -0.160 (-0.1604)	GS 30.375 (30.375)	mem 42.661
Train: [4][40/750]	BT 8.610 (1.824)	DT 8.402 (1.688)	loss 9.169 (9.169)	gnorm 389788.312 (389788.312)	prob 0.334 (0.3339)	GS 30.812 (30.812)	mem 42.676
Train: [4][41/750]	BT 0.088 (1.782)	DT 0.002 (1.647)	loss 9.765 (9.765)	gnorm 387356.969 (387356.969)	prob -0.413 (-0.4133)	GS 31.094 (31.094)	mem 42.677
Train: [4][42/750]	BT 0.092 (1.742)	DT 0.007 (1.608)	loss 9.579 (9.579)	gnorm 375894.938 (375894.938)	prob 0.151 (0.1510)	GS 35.672 (35.672)	mem 42.716
Train: [4][43/750]	BT 0.230 (1.707)	DT 0.035 (1.572)	loss 10.295 (10.295)	gnorm 452692.656 (452692.656)	prob -0.234 (-0.2337)	GS 49.516 (49.516)	mem 42.677
Train: [4][44/750]	BT 0.113 (1.670)	DT 0.001 (1.536)	loss 9.541 (9.541)	gnorm 430297.250 (430297.250)	prob 0.064 (0.0639)	GS 36.438 (36.438)	mem 42.678
Train: [4][45/750]	BT 0.092 (1.635)	DT 0.002 (1.502)	loss 9.465 (9.465)	gnorm 424217.250 (424217.250)	prob 0.104 (0.1036)	GS 37.891 (37.891)	mem 42.679
Train: [4][46/750]	BT 0.118 (1.602)	DT 0.002 (1.469)	loss 9.510 (9.510)	gnorm 428161.562 (428161.562)	prob 0.269 (0.2691)	GS 37.453 (37.453)	mem 42.744
Train: [4][47/750]	BT 0.125 (1.571)	DT 0.002 (1.438)	loss 9.451 (9.451)	gnorm 455817.656 (455817.656)	prob 0.032 (0.0322)	GS 29.672 (29.672)	mem 42.743
Train: [4][48/750]	BT 0.178 (1.542)	DT 0.002 (1.408)	loss 8.928 (8.928)	gnorm 390224.125 (390224.125)	prob 0.392 (0.3917)	GS 31.531 (31.531)	mem 42.825
Train: [4][49/750]	BT 0.167 (1.514)	DT 0.002 (1.379)	loss 9.812 (9.812)	gnorm 438973.031 (438973.031)	prob -0.223 (-0.2234)	GS 28.469 (28.469)	mem 42.738
Train: [4][50/750]	BT 2.658 (1.537)	DT 2.447 (1.401)	loss 9.525 (9.525)	gnorm 416154.844 (416154.844)	prob 0.189 (0.1888)	GS 30.766 (30.766)	mem 42.685
Train: [4][51/750]	BT 0.145 (1.509)	DT 0.010 (1.374)	loss 9.785 (9.785)	gnorm 458558.250 (458558.250)	prob 0.338 (0.3376)	GS 32.344 (32.344)	mem 42.684
Train: [4][52/750]	BT 9.677 (1.667)	DT 9.564 (1.531)	loss 9.779 (9.779)	gnorm 426028.875 (426028.875)	prob -0.090 (-0.0900)	GS 34.453 (34.453)	mem 42.643
Train: [4][53/750]	BT 0.097 (1.637)	DT 0.001 (1.502)	loss 10.579 (10.579)	gnorm 473961.938 (473961.938)	prob -0.416 (-0.4163)	GS 31.938 (31.938)	mem 42.643
Train: [4][54/750]	BT 0.089 (1.608)	DT 0.002 (1.474)	loss 9.513 (9.513)	gnorm 396870.219 (396870.219)	prob 0.311 (0.3107)	GS 31.391 (31.391)	mem 42.644
Train: [4][55/750]	BT 0.155 (1.582)	DT 0.010 (1.448)	loss 10.067 (10.067)	gnorm 411098.906 (411098.906)	prob -0.238 (-0.2384)	GS 30.328 (30.328)	mem 42.644
Train: [4][56/750]	BT 0.087 (1.555)	DT 0.003 (1.422)	loss 9.617 (9.617)	gnorm 424462.438 (424462.438)	prob 0.331 (0.3313)	GS 29.219 (29.219)	mem 42.644
Train: [4][57/750]	BT 0.067 (1.529)	DT 0.002 (1.397)	loss 10.418 (10.418)	gnorm 423612.969 (423612.969)	prob -0.146 (-0.1455)	GS 29.781 (29.781)	mem 42.684
Train: [4][58/750]	BT 0.171 (1.506)	DT 0.002 (1.373)	loss 9.788 (9.788)	gnorm 379485.562 (379485.562)	prob 0.018 (0.0182)	GS 31.312 (31.312)	mem 42.871
Train: [4][59/750]	BT 0.184 (1.483)	DT 0.007 (1.350)	loss 8.865 (8.865)	gnorm 367034.000 (367034.000)	prob 1.173 (1.1725)	GS 25.000 (25.000)	mem 42.908
Train: [4][60/750]	BT 0.182 (1.461)	DT 0.001 (1.327)	loss 10.005 (10.005)	gnorm 376940.844 (376940.844)	prob 0.102 (0.1018)	GS 35.141 (35.141)	mem 42.647
Train: [4][61/750]	BT 0.147 (1.440)	DT 0.021 (1.306)	loss 9.655 (9.655)	gnorm 461751.969 (461751.969)	prob 0.428 (0.4283)	GS 29.656 (29.656)	mem 42.689
Train: [4][62/750]	BT 2.043 (1.450)	DT 1.927 (1.316)	loss 10.127 (10.127)	gnorm 443414.031 (443414.031)	prob -0.327 (-0.3266)	GS 32.641 (32.641)	mem 42.794
Train: [4][63/750]	BT 0.097 (1.428)	DT 0.004 (1.295)	loss 9.952 (9.952)	gnorm 433512.938 (433512.938)	prob 0.054 (0.0543)	GS 27.969 (27.969)	mem 42.729
Train: [4][64/750]	BT 9.998 (1.562)	DT 9.918 (1.430)	loss 9.759 (9.759)	gnorm 429346.812 (429346.812)	prob -0.213 (-0.2126)	GS 29.688 (29.688)	mem 42.811
Train: [4][65/750]	BT 0.104 (1.540)	DT 0.001 (1.408)	loss 9.792 (9.792)	gnorm 444825.688 (444825.688)	prob -0.202 (-0.2018)	GS 31.312 (31.312)	mem 42.737
Train: [4][66/750]	BT 0.086 (1.518)	DT 0.004 (1.387)	loss 9.946 (9.946)	gnorm 462947.281 (462947.281)	prob -0.737 (-0.7372)	GS 35.719 (35.719)	mem 42.820
Train: [4][67/750]	BT 0.134 (1.497)	DT 0.001 (1.366)	loss 9.752 (9.752)	gnorm 453001.094 (453001.094)	prob -0.380 (-0.3797)	GS 31.484 (31.484)	mem 42.989
Train: [4][68/750]	BT 0.108 (1.477)	DT 0.011 (1.346)	loss 9.706 (9.706)	gnorm 401667.594 (401667.594)	prob -0.235 (-0.2354)	GS 35.469 (35.469)	mem 42.777
Train: [4][69/750]	BT 0.127 (1.457)	DT 0.002 (1.327)	loss 10.518 (10.518)	gnorm 448264.188 (448264.188)	prob -0.539 (-0.5389)	GS 31.984 (31.984)	mem 42.776
Train: [4][70/750]	BT 0.124 (1.438)	DT 0.012 (1.308)	loss 9.567 (9.567)	gnorm 411616.781 (411616.781)	prob 0.580 (0.5804)	GS 34.281 (34.281)	mem 42.827
Train: [4][71/750]	BT 0.183 (1.420)	DT 0.003 (1.289)	loss 9.777 (9.777)	gnorm 429294.625 (429294.625)	prob 0.288 (0.2879)	GS 31.875 (31.875)	mem 42.852
Train: [4][72/750]	BT 0.158 (1.403)	DT 0.004 (1.272)	loss 8.879 (8.879)	gnorm 366692.250 (366692.250)	prob 1.262 (1.2617)	GS 33.406 (33.406)	mem 42.821
Train: [4][73/750]	BT 0.094 (1.385)	DT 0.002 (1.254)	loss 9.830 (9.830)	gnorm 387168.094 (387168.094)	prob 0.374 (0.3739)	GS 30.672 (30.672)	mem 42.887
Train: [4][74/750]	BT 3.184 (1.409)	DT 3.091 (1.279)	loss 9.809 (9.809)	gnorm 342619.219 (342619.219)	prob 0.250 (0.2504)	GS 37.312 (37.312)	mem 42.654
Train: [4][75/750]	BT 0.077 (1.391)	DT 0.002 (1.262)	loss 9.788 (9.788)	gnorm 403342.000 (403342.000)	prob 0.503 (0.5034)	GS 29.516 (29.516)	mem 42.667
Train: [4][76/750]	BT 9.809 (1.502)	DT 9.725 (1.373)	loss 10.171 (10.171)	gnorm 442992.250 (442992.250)	prob -0.281 (-0.2806)	GS 33.828 (33.828)	mem 43.131
Train: [4][77/750]	BT 0.085 (1.484)	DT 0.002 (1.355)	loss 9.840 (9.840)	gnorm 480117.406 (480117.406)	prob 0.042 (0.0417)	GS 35.969 (35.969)	mem 43.131
Train: [4][78/750]	BT 0.082 (1.466)	DT 0.001 (1.338)	loss 9.451 (9.451)	gnorm 393781.875 (393781.875)	prob 0.565 (0.5646)	GS 31.172 (31.172)	mem 43.132
Train: [4][79/750]	BT 0.102 (1.448)	DT 0.001 (1.321)	loss 9.159 (9.159)	gnorm 460812.156 (460812.156)	prob 0.818 (0.8176)	GS 33.953 (33.953)	mem 43.132
Train: [4][80/750]	BT 0.116 (1.432)	DT 0.008 (1.305)	loss 9.210 (9.210)	gnorm 369007.125 (369007.125)	prob 0.727 (0.7266)	GS 36.094 (36.094)	mem 43.132
Train: [4][81/750]	BT 0.157 (1.416)	DT 0.001 (1.289)	loss 9.139 (9.139)	gnorm 384603.281 (384603.281)	prob 1.201 (1.2011)	GS 34.266 (34.266)	mem 43.132
Train: [4][82/750]	BT 0.135 (1.400)	DT 0.002 (1.273)	loss 9.692 (9.692)	gnorm 418392.938 (418392.938)	prob 0.513 (0.5132)	GS 32.891 (32.891)	mem 43.133
Train: [4][83/750]	BT 0.178 (1.386)	DT 0.002 (1.258)	loss 9.978 (9.978)	gnorm 419831.562 (419831.562)	prob -0.140 (-0.1398)	GS 37.016 (37.016)	mem 43.135
Train: [4][84/750]	BT 0.148 (1.371)	DT 0.004 (1.243)	loss 9.529 (9.529)	gnorm 404104.844 (404104.844)	prob 0.463 (0.4629)	GS 31.969 (31.969)	mem 43.133
Train: [4][85/750]	BT 0.151 (1.357)	DT 0.008 (1.228)	loss 9.884 (9.884)	gnorm 462836.781 (462836.781)	prob 0.480 (0.4802)	GS 28.891 (28.891)	mem 43.133
Train: [4][86/750]	BT 0.103 (1.342)	DT 0.014 (1.214)	loss 9.550 (9.550)	gnorm 384554.812 (384554.812)	prob 0.048 (0.0483)	GS 31.078 (31.078)	mem 43.134
Train: [4][87/750]	BT 0.211 (1.329)	DT 0.006 (1.200)	loss 10.097 (10.097)	gnorm 400897.281 (400897.281)	prob -0.322 (-0.3216)	GS 33.031 (33.031)	mem 43.134
Train: [4][88/750]	BT 14.081 (1.474)	DT 13.896 (1.344)	loss 10.016 (10.016)	gnorm 405112.875 (405112.875)	prob -0.165 (-0.1655)	GS 35.969 (35.969)	mem 43.131
Train: [4][89/750]	BT 0.186 (1.460)	DT 0.002 (1.329)	loss 9.023 (9.023)	gnorm 398376.938 (398376.938)	prob 0.760 (0.7602)	GS 32.359 (32.359)	mem 43.225
Train: [4][90/750]	BT 0.132 (1.445)	DT 0.015 (1.315)	loss 9.114 (9.114)	gnorm 398057.906 (398057.906)	prob 1.005 (1.0051)	GS 32.938 (32.938)	mem 43.066
Train: [4][91/750]	BT 0.089 (1.430)	DT 0.001 (1.300)	loss 9.294 (9.294)	gnorm 427683.906 (427683.906)	prob 0.622 (0.6222)	GS 32.812 (32.812)	mem 43.067
Train: [4][92/750]	BT 0.125 (1.416)	DT 0.005 (1.286)	loss 9.919 (9.919)	gnorm 393872.375 (393872.375)	prob -0.279 (-0.2785)	GS 34.516 (34.516)	mem 43.069
Train: [4][93/750]	BT 0.191 (1.403)	DT 0.007 (1.273)	loss 10.174 (10.174)	gnorm 422123.469 (422123.469)	prob -0.435 (-0.4350)	GS 35.000 (35.000)	mem 43.068
Train: [4][94/750]	BT 0.087 (1.389)	DT 0.003 (1.259)	loss 9.812 (9.812)	gnorm 398224.781 (398224.781)	prob -0.256 (-0.2559)	GS 32.875 (32.875)	mem 43.068
Train: [4][95/750]	BT 0.071 (1.375)	DT 0.001 (1.246)	loss 9.608 (9.608)	gnorm 399204.344 (399204.344)	prob -0.305 (-0.3047)	GS 28.016 (28.016)	mem 43.069
Train: [4][96/750]	BT 0.098 (1.361)	DT 0.001 (1.233)	loss 9.062 (9.062)	gnorm 407741.156 (407741.156)	prob 0.667 (0.6671)	GS 34.750 (34.750)	mem 43.069
Train: [4][97/750]	BT 0.200 (1.349)	DT 0.011 (1.220)	loss 9.997 (9.997)	gnorm 409727.094 (409727.094)	prob -0.293 (-0.2933)	GS 27.359 (27.359)	mem 43.069
Train: [4][98/750]	BT 0.089 (1.337)	DT 0.002 (1.208)	loss 9.230 (9.230)	gnorm 375884.688 (375884.688)	prob -0.130 (-0.1297)	GS 30.125 (30.125)	mem 43.074
Train: [4][99/750]	BT 0.093 (1.324)	DT 0.003 (1.196)	loss 9.808 (9.808)	gnorm 428400.125 (428400.125)	prob 0.295 (0.2954)	GS 34.781 (34.781)	mem 43.123
Train: [4][100/750]	BT 15.805 (1.469)	DT 15.682 (1.340)	loss 9.006 (9.006)	gnorm 401521.094 (401521.094)	prob 0.363 (0.3635)	GS 36.328 (36.328)	mem 43.265
Train: [4][101/750]	BT 0.091 (1.455)	DT 0.022 (1.327)	loss 9.760 (9.760)	gnorm 431615.125 (431615.125)	prob -0.086 (-0.0857)	GS 31.734 (31.734)	mem 43.264
Train: [4][102/750]	BT 0.067 (1.441)	DT 0.001 (1.314)	loss 10.127 (10.127)	gnorm 400966.719 (400966.719)	prob -1.002 (-1.0017)	GS 36.547 (36.547)	mem 43.264
Train: [4][103/750]	BT 0.094 (1.428)	DT 0.002 (1.302)	loss 9.167 (9.167)	gnorm 405383.250 (405383.250)	prob 0.059 (0.0585)	GS 29.703 (29.703)	mem 43.264
Train: [4][104/750]	BT 0.131 (1.416)	DT 0.003 (1.289)	loss 9.514 (9.514)	gnorm 411843.969 (411843.969)	prob -0.248 (-0.2478)	GS 36.469 (36.469)	mem 43.264
Train: [4][105/750]	BT 0.122 (1.404)	DT 0.002 (1.277)	loss 10.096 (10.096)	gnorm 455133.812 (455133.812)	prob -0.236 (-0.2358)	GS 32.547 (32.547)	mem 43.264
Train: [4][106/750]	BT 0.145 (1.392)	DT 0.002 (1.265)	loss 9.177 (9.177)	gnorm 396032.344 (396032.344)	prob -0.326 (-0.3257)	GS 32.984 (32.984)	mem 43.264
Train: [4][107/750]	BT 0.094 (1.380)	DT 0.002 (1.253)	loss 9.542 (9.542)	gnorm 437098.969 (437098.969)	prob -0.262 (-0.2616)	GS 33.484 (33.484)	mem 43.264
Train: [4][108/750]	BT 0.173 (1.368)	DT 0.024 (1.242)	loss 9.710 (9.710)	gnorm 375753.250 (375753.250)	prob -0.154 (-0.1539)	GS 32.859 (32.859)	mem 43.265
Train: [4][109/750]	BT 0.111 (1.357)	DT 0.002 (1.230)	loss 8.940 (8.940)	gnorm 402423.219 (402423.219)	prob 0.315 (0.3150)	GS 30.844 (30.844)	mem 43.289
Train: [4][110/750]	BT 0.140 (1.346)	DT 0.001 (1.219)	loss 9.632 (9.632)	gnorm 424148.844 (424148.844)	prob 0.161 (0.1607)	GS 38.781 (38.781)	mem 43.328
Train: [4][111/750]	BT 0.108 (1.335)	DT 0.027 (1.208)	loss 8.981 (8.981)	gnorm 393245.094 (393245.094)	prob 0.576 (0.5762)	GS 32.328 (32.328)	mem 43.265
Train: [4][112/750]	BT 14.248 (1.450)	DT 14.175 (1.324)	loss 10.089 (10.089)	gnorm 422202.875 (422202.875)	prob -0.855 (-0.8547)	GS 34.219 (34.219)	mem 43.391
Train: [4][113/750]	BT 0.076 (1.438)	DT 0.001 (1.312)	loss 9.230 (9.230)	gnorm 431056.344 (431056.344)	prob 0.200 (0.1998)	GS 30.859 (30.859)	mem 43.392
Train: [4][114/750]	BT 0.082 (1.426)	DT 0.002 (1.301)	loss 9.947 (9.947)	gnorm 422495.438 (422495.438)	prob -0.524 (-0.5243)	GS 36.156 (36.156)	mem 43.393
Train: [4][115/750]	BT 0.073 (1.414)	DT 0.002 (1.290)	loss 9.524 (9.524)	gnorm 392257.406 (392257.406)	prob 0.048 (0.0481)	GS 30.625 (30.625)	mem 43.393
Train: [4][116/750]	BT 0.076 (1.403)	DT 0.001 (1.279)	loss 9.875 (9.875)	gnorm 382853.594 (382853.594)	prob -0.740 (-0.7402)	GS 35.953 (35.953)	mem 43.394
Train: [4][117/750]	BT 0.073 (1.391)	DT 0.001 (1.268)	loss 9.251 (9.251)	gnorm 391505.062 (391505.062)	prob 0.185 (0.1852)	GS 34.547 (34.547)	mem 43.393
Train: [4][118/750]	BT 0.109 (1.380)	DT 0.001 (1.257)	loss 9.991 (9.991)	gnorm 412147.875 (412147.875)	prob -0.648 (-0.6484)	GS 34.094 (34.094)	mem 43.393
Train: [4][119/750]	BT 0.084 (1.370)	DT 0.013 (1.246)	loss 9.764 (9.764)	gnorm 430197.781 (430197.781)	prob -0.451 (-0.4512)	GS 32.984 (32.984)	mem 43.394
Train: [4][120/750]	BT 0.159 (1.359)	DT 0.001 (1.236)	loss 9.629 (9.629)	gnorm 403175.688 (403175.688)	prob -0.533 (-0.5329)	GS 33.031 (33.031)	mem 43.403
Train: [4][121/750]	BT 0.151 (1.349)	DT 0.002 (1.226)	loss 9.563 (9.563)	gnorm 418109.750 (418109.750)	prob -0.736 (-0.7363)	GS 28.766 (28.766)	mem 43.412
Train: [4][122/750]	BT 0.180 (1.340)	DT 0.020 (1.216)	loss 8.894 (8.894)	gnorm 374579.969 (374579.969)	prob 0.368 (0.3683)	GS 32.641 (32.641)	mem 43.413
Train: [4][123/750]	BT 0.177 (1.330)	DT 0.026 (1.206)	loss 9.893 (9.893)	gnorm 399674.281 (399674.281)	prob -0.442 (-0.4417)	GS 32.781 (32.781)	mem 43.414
Train: [4][124/750]	BT 9.119 (1.393)	DT 8.972 (1.269)	loss 9.720 (9.720)	gnorm 402225.844 (402225.844)	prob -0.373 (-0.3733)	GS 33.484 (33.484)	mem 43.469
Train: [4][125/750]	BT 0.087 (1.383)	DT 0.002 (1.259)	loss 9.485 (9.485)	gnorm 399635.438 (399635.438)	prob -0.032 (-0.0319)	GS 30.438 (30.438)	mem 43.469
Train: [4][126/750]	BT 0.094 (1.373)	DT 0.001 (1.249)	loss 9.088 (9.088)	gnorm 433815.688 (433815.688)	prob 0.509 (0.5087)	GS 30.703 (30.703)	mem 43.529
Train: [4][127/750]	BT 0.092 (1.362)	DT 0.002 (1.239)	loss 9.276 (9.276)	gnorm 395963.688 (395963.688)	prob 0.064 (0.0643)	GS 33.484 (33.484)	mem 43.729
Train: [4][128/750]	BT 0.217 (1.354)	DT 0.015 (1.229)	loss 9.943 (9.943)	gnorm 430756.094 (430756.094)	prob -0.467 (-0.4670)	GS 34.250 (34.250)	mem 43.633
Train: [4][129/750]	BT 0.120 (1.344)	DT 0.002 (1.220)	loss 9.057 (9.057)	gnorm 409205.812 (409205.812)	prob 0.256 (0.2559)	GS 29.406 (29.406)	mem 43.482
Train: [4][130/750]	BT 0.095 (1.334)	DT 0.002 (1.211)	loss 9.870 (9.870)	gnorm 428663.344 (428663.344)	prob -0.673 (-0.6732)	GS 31.828 (31.828)	mem 43.482
Train: [4][131/750]	BT 0.156 (1.325)	DT 0.001 (1.201)	loss 9.554 (9.554)	gnorm 393526.531 (393526.531)	prob 0.038 (0.0377)	GS 36.344 (36.344)	mem 43.565
Train: [4][132/750]	BT 0.152 (1.316)	DT 0.021 (1.192)	loss 9.093 (9.093)	gnorm 369678.469 (369678.469)	prob 0.991 (0.9915)	GS 31.469 (31.469)	mem 43.522
Train: [4][133/750]	BT 0.093 (1.307)	DT 0.002 (1.183)	loss 9.782 (9.782)	gnorm 396442.188 (396442.188)	prob 0.384 (0.3842)	GS 31.172 (31.172)	mem 43.484
Train: [4][134/750]	BT 0.089 (1.298)	DT 0.001 (1.175)	loss 9.686 (9.686)	gnorm 382107.375 (382107.375)	prob -0.204 (-0.2039)	GS 31.125 (31.125)	mem 43.484
Train: [4][135/750]	BT 0.181 (1.290)	DT 0.005 (1.166)	loss 9.354 (9.354)	gnorm 392432.438 (392432.438)	prob 0.425 (0.4254)	GS 28.234 (28.234)	mem 43.557
Train: [4][136/750]	BT 14.756 (1.389)	DT 14.690 (1.265)	loss 9.563 (9.563)	gnorm 368627.719 (368627.719)	prob -0.161 (-0.1610)	GS 31.719 (31.719)	mem 43.347
Train: [4][137/750]	BT 0.084 (1.379)	DT 0.001 (1.256)	loss 9.366 (9.366)	gnorm 387988.812 (387988.812)	prob 0.415 (0.4145)	GS 31.109 (31.109)	mem 43.347
Train: [4][138/750]	BT 0.078 (1.370)	DT 0.001 (1.247)	loss 9.871 (9.871)	gnorm 403685.344 (403685.344)	prob 0.152 (0.1522)	GS 30.953 (30.953)	mem 43.348
Train: [4][139/750]	BT 0.075 (1.361)	DT 0.002 (1.238)	loss 9.651 (9.651)	gnorm 405937.844 (405937.844)	prob -0.034 (-0.0344)	GS 30.625 (30.625)	mem 43.347
Train: [4][140/750]	BT 0.108 (1.352)	DT 0.002 (1.229)	loss 9.008 (9.008)	gnorm 422096.344 (422096.344)	prob 0.643 (0.6435)	GS 34.734 (34.734)	mem 43.347
Train: [4][141/750]	BT 0.095 (1.343)	DT 0.002 (1.221)	loss 9.917 (9.917)	gnorm 413239.844 (413239.844)	prob 0.063 (0.0626)	GS 28.125 (28.125)	mem 43.348
Train: [4][142/750]	BT 0.084 (1.334)	DT 0.002 (1.212)	loss 9.605 (9.605)	gnorm 403178.375 (403178.375)	prob -0.295 (-0.2946)	GS 36.922 (36.922)	mem 43.347
Train: [4][143/750]	BT 0.084 (1.325)	DT 0.001 (1.204)	loss 8.758 (8.758)	gnorm 371398.406 (371398.406)	prob 0.897 (0.8969)	GS 32.422 (32.422)	mem 43.348
Train: [4][144/750]	BT 0.104 (1.317)	DT 0.002 (1.195)	loss 8.951 (8.951)	gnorm 381078.438 (381078.438)	prob 1.168 (1.1681)	GS 33.688 (33.688)	mem 43.348
Train: [4][145/750]	BT 0.127 (1.308)	DT 0.006 (1.187)	loss 10.039 (10.039)	gnorm 403155.719 (403155.719)	prob 0.133 (0.1332)	GS 29.234 (29.234)	mem 43.348
Train: [4][146/750]	BT 0.111 (1.300)	DT 0.004 (1.179)	loss 9.294 (9.294)	gnorm 385245.812 (385245.812)	prob 0.818 (0.8176)	GS 34.828 (34.828)	mem 43.401
Train: [4][147/750]	BT 0.103 (1.292)	DT 0.002 (1.171)	loss 9.332 (9.332)	gnorm 386337.094 (386337.094)	prob 0.340 (0.3399)	GS 34.672 (34.672)	mem 43.349
Train: [4][148/750]	BT 10.073 (1.351)	DT 9.958 (1.230)	loss 9.331 (9.331)	gnorm 372022.469 (372022.469)	prob 0.373 (0.3729)	GS 31.891 (31.891)	mem 43.471
Train: [4][149/750]	BT 0.099 (1.343)	DT 0.002 (1.222)	loss 8.707 (8.707)	gnorm 406381.469 (406381.469)	prob 1.225 (1.2251)	GS 31.281 (31.281)	mem 43.472
Train: [4][150/750]	BT 0.094 (1.335)	DT 0.002 (1.214)	loss 9.850 (9.850)	gnorm 386572.469 (386572.469)	prob -0.167 (-0.1674)	GS 33.828 (33.828)	mem 43.472
Train: [4][151/750]	BT 0.090 (1.326)	DT 0.009 (1.206)	loss 9.112 (9.112)	gnorm 374562.281 (374562.281)	prob 0.378 (0.3783)	GS 31.391 (31.391)	mem 43.473
Train: [4][152/750]	BT 0.078 (1.318)	DT 0.002 (1.198)	loss 8.814 (8.814)	gnorm 396463.906 (396463.906)	prob 0.850 (0.8502)	GS 35.500 (35.500)	mem 43.473
Train: [4][153/750]	BT 0.167 (1.311)	DT 0.001 (1.190)	loss 9.452 (9.452)	gnorm 377700.312 (377700.312)	prob 0.220 (0.2199)	GS 36.391 (36.391)	mem 43.497
Train: [4][154/750]	BT 0.146 (1.303)	DT 0.001 (1.182)	loss 9.340 (9.340)	gnorm 377966.844 (377966.844)	prob 0.336 (0.3357)	GS 31.297 (31.297)	mem 43.499
Train: [4][155/750]	BT 0.099 (1.295)	DT 0.015 (1.175)	loss 9.569 (9.569)	gnorm 414074.531 (414074.531)	prob 0.439 (0.4392)	GS 28.047 (28.047)	mem 43.504
Train: [4][156/750]	BT 0.107 (1.288)	DT 0.002 (1.167)	loss 9.855 (9.855)	gnorm 377251.125 (377251.125)	prob -0.237 (-0.2374)	GS 33.188 (33.188)	mem 43.599
Train: [4][157/750]	BT 0.155 (1.281)	DT 0.009 (1.160)	loss 9.269 (9.269)	gnorm 426592.875 (426592.875)	prob 0.317 (0.3173)	GS 32.438 (32.438)	mem 43.507
Train: [4][158/750]	BT 0.201 (1.274)	DT 0.002 (1.153)	loss 9.326 (9.326)	gnorm 387070.312 (387070.312)	prob 0.065 (0.0653)	GS 31.891 (31.891)	mem 43.688
Train: [4][159/750]	BT 0.161 (1.267)	DT 0.018 (1.146)	loss 9.419 (9.419)	gnorm 371696.938 (371696.938)	prob -0.306 (-0.3062)	GS 30.750 (30.750)	mem 43.504
Train: [4][160/750]	BT 12.104 (1.334)	DT 12.014 (1.213)	loss 9.401 (9.401)	gnorm 376727.625 (376727.625)	prob -0.306 (-0.3057)	GS 29.906 (29.906)	mem 43.412
Train: [4][161/750]	BT 0.098 (1.327)	DT 0.003 (1.206)	loss 9.131 (9.131)	gnorm 386587.500 (386587.500)	prob -0.106 (-0.1058)	GS 30.281 (30.281)	mem 43.413
Train: [4][162/750]	BT 0.085 (1.319)	DT 0.003 (1.199)	loss 9.539 (9.539)	gnorm 390798.312 (390798.312)	prob 0.068 (0.0682)	GS 33.188 (33.188)	mem 43.452
Train: [4][163/750]	BT 0.131 (1.312)	DT 0.002 (1.191)	loss 9.664 (9.664)	gnorm 386006.188 (386006.188)	prob -0.245 (-0.2451)	GS 30.281 (30.281)	mem 43.580
Train: [4][164/750]	BT 0.091 (1.304)	DT 0.002 (1.184)	loss 9.906 (9.906)	gnorm 400549.844 (400549.844)	prob -1.108 (-1.1081)	GS 33.031 (33.031)	mem 43.580
Train: [4][165/750]	BT 0.078 (1.297)	DT 0.002 (1.177)	loss 10.004 (10.004)	gnorm 377586.969 (377586.969)	prob -1.150 (-1.1500)	GS 32.484 (32.484)	mem 43.449
Train: [4][166/750]	BT 0.148 (1.290)	DT 0.001 (1.170)	loss 10.087 (10.087)	gnorm 403574.219 (403574.219)	prob -1.031 (-1.0306)	GS 35.781 (35.781)	mem 43.422
Train: [4][167/750]	BT 0.074 (1.283)	DT 0.002 (1.163)	loss 9.783 (9.783)	gnorm 433138.156 (433138.156)	prob -0.624 (-0.6243)	GS 33.359 (33.359)	mem 43.422
Train: [4][168/750]	BT 0.089 (1.276)	DT 0.001 (1.156)	loss 9.900 (9.900)	gnorm 433728.938 (433728.938)	prob -0.889 (-0.8891)	GS 31.953 (31.953)	mem 43.424
Train: [4][169/750]	BT 0.096 (1.269)	DT 0.002 (1.149)	loss 9.945 (9.945)	gnorm 404429.875 (404429.875)	prob -0.927 (-0.9270)	GS 33.969 (33.969)	mem 43.479
Train: [4][170/750]	BT 0.132 (1.262)	DT 0.001 (1.142)	loss 9.351 (9.351)	gnorm 390227.938 (390227.938)	prob -0.455 (-0.4552)	GS 34.953 (34.953)	mem 43.639
Train: [4][171/750]	BT 0.204 (1.256)	DT 0.021 (1.136)	loss 9.429 (9.429)	gnorm 402523.281 (402523.281)	prob -0.780 (-0.7803)	GS 29.422 (29.422)	mem 43.708
Train: [4][172/750]	BT 11.745 (1.317)	DT 11.634 (1.197)	loss 9.418 (9.418)	gnorm 388991.938 (388991.938)	prob -0.467 (-0.4666)	GS 33.828 (33.828)	mem 43.527
Train: [4][173/750]	BT 0.091 (1.310)	DT 0.002 (1.190)	loss 9.100 (9.100)	gnorm 406285.031 (406285.031)	prob -0.497 (-0.4971)	GS 31.703 (31.703)	mem 43.528
Train: [4][174/750]	BT 0.199 (1.303)	DT 0.002 (1.183)	loss 9.521 (9.521)	gnorm 405723.906 (405723.906)	prob -0.571 (-0.5707)	GS 36.031 (36.031)	mem 43.529
Train: [4][175/750]	BT 0.088 (1.296)	DT 0.002 (1.176)	loss 9.442 (9.442)	gnorm 398206.531 (398206.531)	prob -0.578 (-0.5778)	GS 32.328 (32.328)	mem 43.529
Train: [4][176/750]	BT 0.074 (1.289)	DT 0.002 (1.170)	loss 9.267 (9.267)	gnorm 393787.625 (393787.625)	prob 0.165 (0.1651)	GS 34.938 (34.938)	mem 43.529
Train: [4][177/750]	BT 0.115 (1.283)	DT 0.001 (1.163)	loss 9.335 (9.335)	gnorm 378668.875 (378668.875)	prob 0.078 (0.0782)	GS 32.578 (32.578)	mem 43.530
Train: [4][178/750]	BT 0.130 (1.276)	DT 0.010 (1.156)	loss 9.010 (9.010)	gnorm 401280.469 (401280.469)	prob 0.084 (0.0837)	GS 33.906 (33.906)	mem 43.530
Train: [4][179/750]	BT 0.107 (1.270)	DT 0.009 (1.150)	loss 9.396 (9.396)	gnorm 391396.594 (391396.594)	prob 0.396 (0.3960)	GS 34.219 (34.219)	mem 43.530
Train: [4][180/750]	BT 0.081 (1.263)	DT 0.001 (1.144)	loss 9.418 (9.418)	gnorm 371026.969 (371026.969)	prob 0.527 (0.5275)	GS 33.922 (33.922)	mem 43.531
Train: [4][181/750]	BT 0.086 (1.257)	DT 0.002 (1.137)	loss 9.200 (9.200)	gnorm 375543.625 (375543.625)	prob 0.203 (0.2035)	GS 30.266 (30.266)	mem 43.531
Train: [4][182/750]	BT 0.081 (1.250)	DT 0.002 (1.131)	loss 9.227 (9.227)	gnorm 385862.375 (385862.375)	prob -0.168 (-0.1682)	GS 29.062 (29.062)	mem 43.531
Train: [4][183/750]	BT 0.088 (1.244)	DT 0.003 (1.125)	loss 8.989 (8.989)	gnorm 388316.250 (388316.250)	prob 0.599 (0.5988)	GS 34.297 (34.297)	mem 43.531
Train: [4][184/750]	BT 11.001 (1.297)	DT 10.910 (1.178)	loss 9.055 (9.055)	gnorm 401116.562 (401116.562)	prob 0.051 (0.0511)	GS 30.203 (30.203)	mem 43.527
Train: [4][185/750]	BT 0.155 (1.291)	DT 0.016 (1.172)	loss 9.578 (9.578)	gnorm 408756.344 (408756.344)	prob -0.358 (-0.3582)	GS 28.938 (28.938)	mem 43.527
Train: [4][186/750]	BT 0.130 (1.284)	DT 0.002 (1.166)	loss 9.718 (9.718)	gnorm 386404.625 (386404.625)	prob -1.020 (-1.0197)	GS 31.625 (31.625)	mem 43.560
Train: [4][187/750]	BT 0.135 (1.278)	DT 0.010 (1.159)	loss 9.031 (9.031)	gnorm 362552.875 (362552.875)	prob 0.868 (0.8676)	GS 34.344 (34.344)	mem 43.529
Train: [4][188/750]	BT 0.172 (1.272)	DT 0.002 (1.153)	loss 9.749 (9.749)	gnorm 389993.250 (389993.250)	prob -0.276 (-0.2764)	GS 33.828 (33.828)	mem 43.530
Train: [4][189/750]	BT 0.163 (1.267)	DT 0.002 (1.147)	loss 9.980 (9.980)	gnorm 398583.281 (398583.281)	prob -0.755 (-0.7546)	GS 29.016 (29.016)	mem 43.530
Train: [4][190/750]	BT 0.144 (1.261)	DT 0.011 (1.141)	loss 9.933 (9.933)	gnorm 375042.406 (375042.406)	prob -0.719 (-0.7186)	GS 30.688 (30.688)	mem 43.531
Train: [4][191/750]	BT 0.105 (1.255)	DT 0.010 (1.135)	loss 9.318 (9.318)	gnorm 372875.844 (372875.844)	prob -0.123 (-0.1226)	GS 33.875 (33.875)	mem 43.530
Train: [4][192/750]	BT 0.088 (1.249)	DT 0.002 (1.129)	loss 9.214 (9.214)	gnorm 364477.719 (364477.719)	prob -0.015 (-0.0153)	GS 31.562 (31.562)	mem 43.589
Train: [4][193/750]	BT 0.252 (1.243)	DT 0.016 (1.124)	loss 9.234 (9.234)	gnorm 409270.156 (409270.156)	prob 0.960 (0.9601)	GS 35.078 (35.078)	mem 43.532
Train: [4][194/750]	BT 0.136 (1.238)	DT 0.002 (1.118)	loss 8.555 (8.555)	gnorm 369670.625 (369670.625)	prob 0.999 (0.9989)	GS 31.750 (31.750)	mem 43.533
Train: [4][195/750]	BT 0.199 (1.232)	DT 0.013 (1.112)	loss 9.719 (9.719)	gnorm 369963.062 (369963.062)	prob 0.246 (0.2456)	GS 28.953 (28.953)	mem 43.573
Train: [4][196/750]	BT 14.442 (1.300)	DT 14.357 (1.180)	loss 9.531 (9.531)	gnorm 425298.250 (425298.250)	prob 0.121 (0.1214)	GS 35.812 (35.812)	mem 44.708
Train: [4][197/750]	BT 0.075 (1.294)	DT 0.001 (1.174)	loss 9.638 (9.638)	gnorm 400856.281 (400856.281)	prob 0.262 (0.2618)	GS 30.266 (30.266)	mem 44.729
Train: [4][198/750]	BT 0.075 (1.287)	DT 0.001 (1.168)	loss 9.246 (9.246)	gnorm 405927.250 (405927.250)	prob 0.341 (0.3407)	GS 36.469 (36.469)	mem 44.780
Train: [4][199/750]	BT 0.081 (1.281)	DT 0.002 (1.162)	loss 9.035 (9.035)	gnorm 391266.750 (391266.750)	prob 0.758 (0.7580)	GS 36.094 (36.094)	mem 44.833
Train: [4][200/750]	BT 0.105 (1.275)	DT 0.002 (1.156)	loss 9.518 (9.518)	gnorm 388348.344 (388348.344)	prob -0.027 (-0.0272)	GS 35.750 (35.750)	mem 44.869
Train: [4][201/750]	BT 0.116 (1.270)	DT 0.006 (1.150)	loss 10.232 (10.232)	gnorm 406742.000 (406742.000)	prob -0.754 (-0.7538)	GS 33.078 (33.078)	mem 44.888
Train: [4][202/750]	BT 0.116 (1.264)	DT 0.009 (1.145)	loss 9.974 (9.974)	gnorm 473817.688 (473817.688)	prob -0.305 (-0.3055)	GS 36.578 (36.578)	mem 44.945
Train: [4][203/750]	BT 0.154 (1.258)	DT 0.002 (1.139)	loss 9.496 (9.496)	gnorm 422567.125 (422567.125)	prob 0.369 (0.3695)	GS 31.531 (31.531)	mem 44.935
Train: [4][204/750]	BT 0.122 (1.253)	DT 0.002 (1.134)	loss 9.433 (9.433)	gnorm 371486.656 (371486.656)	prob 0.361 (0.3610)	GS 33.203 (33.203)	mem 44.962
Train: [4][205/750]	BT 0.187 (1.248)	DT 0.003 (1.128)	loss 9.045 (9.045)	gnorm 421930.906 (421930.906)	prob 0.707 (0.7067)	GS 26.844 (26.844)	mem 44.987
Train: [4][206/750]	BT 0.236 (1.243)	DT 0.011 (1.123)	loss 9.852 (9.852)	gnorm 394594.625 (394594.625)	prob -0.272 (-0.2722)	GS 32.344 (32.344)	mem 45.086
Train: [4][207/750]	BT 0.110 (1.237)	DT 0.016 (1.117)	loss 9.254 (9.254)	gnorm 378312.625 (378312.625)	prob 0.360 (0.3601)	GS 34.188 (34.188)	mem 45.041
Train: [4][208/750]	BT 15.115 (1.304)	DT 14.971 (1.184)	loss 10.070 (10.070)	gnorm 371607.719 (371607.719)	prob -0.676 (-0.6761)	GS 32.625 (32.625)	mem 52.172
Train: [4][209/750]	BT 0.096 (1.298)	DT 0.003 (1.178)	loss 9.378 (9.378)	gnorm 396529.594 (396529.594)	prob -0.296 (-0.2963)	GS 29.062 (29.062)	mem 52.142
Train: [4][210/750]	BT 0.140 (1.293)	DT 0.010 (1.173)	loss 9.001 (9.001)	gnorm 376876.000 (376876.000)	prob 0.273 (0.2728)	GS 30.859 (30.859)	mem 52.176
Train: [4][211/750]	BT 0.102 (1.287)	DT 0.003 (1.167)	loss 9.587 (9.587)	gnorm 391977.094 (391977.094)	prob 0.259 (0.2593)	GS 29.172 (29.172)	mem 52.199
Train: [4][212/750]	BT 0.094 (1.281)	DT 0.007 (1.162)	loss 9.213 (9.213)	gnorm 388624.062 (388624.062)	prob 0.220 (0.2197)	GS 30.250 (30.250)	mem 52.225
Train: [4][213/750]	BT 0.197 (1.276)	DT 0.003 (1.156)	loss 9.856 (9.856)	gnorm 468687.938 (468687.938)	prob 0.317 (0.3170)	GS 30.766 (30.766)	mem 52.267
Train: [4][214/750]	BT 0.193 (1.271)	DT 0.014 (1.151)	loss 9.382 (9.382)	gnorm 383362.188 (383362.188)	prob -0.059 (-0.0592)	GS 35.188 (35.188)	mem 52.335
Train: [4][215/750]	BT 0.180 (1.266)	DT 0.003 (1.146)	loss 9.159 (9.159)	gnorm 399493.781 (399493.781)	prob 0.155 (0.1549)	GS 28.938 (28.938)	mem 52.417
Train: [4][216/750]	BT 0.253 (1.262)	DT 0.002 (1.140)	loss 9.179 (9.179)	gnorm 415303.250 (415303.250)	prob 0.425 (0.4252)	GS 33.203 (33.203)	mem 52.504
Train: [4][217/750]	BT 0.105 (1.256)	DT 0.009 (1.135)	loss 9.337 (9.337)	gnorm 429299.656 (429299.656)	prob 0.355 (0.3549)	GS 32.141 (32.141)	mem 52.459
Train: [4][218/750]	BT 0.148 (1.251)	DT 0.001 (1.130)	loss 9.771 (9.771)	gnorm 415554.281 (415554.281)	prob -0.114 (-0.1140)	GS 33.672 (33.672)	mem 52.517
Train: [4][219/750]	BT 0.136 (1.246)	DT 0.003 (1.125)	loss 9.427 (9.427)	gnorm 475047.188 (475047.188)	prob 0.089 (0.0892)	GS 30.047 (30.047)	mem 52.505
Train: [4][220/750]	BT 10.787 (1.289)	DT 10.691 (1.168)	loss 9.698 (9.698)	gnorm 408785.031 (408785.031)	prob 0.050 (0.0502)	GS 34.266 (34.266)	mem 54.473
Train: [4][221/750]	BT 0.101 (1.284)	DT 0.002 (1.163)	loss 9.009 (9.009)	gnorm 362670.906 (362670.906)	prob 0.629 (0.6287)	GS 36.641 (36.641)	mem 54.487
Train: [4][222/750]	BT 0.301 (1.280)	DT 0.044 (1.158)	loss 9.800 (9.800)	gnorm 387319.344 (387319.344)	prob -0.626 (-0.6259)	GS 31.234 (31.234)	mem 54.532
Train: [4][223/750]	BT 0.212 (1.275)	DT 0.006 (1.153)	loss 9.365 (9.365)	gnorm 410326.750 (410326.750)	prob -0.216 (-0.2156)	GS 31.328 (31.328)	mem 54.613
Train: [4][224/750]	BT 0.250 (1.270)	DT 0.037 (1.148)	loss 9.330 (9.330)	gnorm 367601.344 (367601.344)	prob -0.385 (-0.3849)	GS 33.547 (33.547)	mem 54.545
Train: [4][225/750]	BT 0.166 (1.265)	DT 0.006 (1.143)	loss 8.929 (8.929)	gnorm 383721.594 (383721.594)	prob 0.514 (0.5139)	GS 36.688 (36.688)	mem 54.569
Train: [4][226/750]	BT 0.121 (1.260)	DT 0.002 (1.138)	loss 9.719 (9.719)	gnorm 377796.969 (377796.969)	prob -0.567 (-0.5668)	GS 35.688 (35.688)	mem 54.619
Train: [4][227/750]	BT 0.178 (1.256)	DT 0.002 (1.133)	loss 9.373 (9.373)	gnorm 437805.250 (437805.250)	prob -0.029 (-0.0288)	GS 30.281 (30.281)	mem 54.611
Train: [4][228/750]	BT 0.090 (1.250)	DT 0.002 (1.128)	loss 9.713 (9.713)	gnorm 409688.844 (409688.844)	prob -0.782 (-0.7820)	GS 34.094 (34.094)	mem 54.649
Train: [4][229/750]	BT 0.116 (1.245)	DT 0.003 (1.123)	loss 9.000 (9.000)	gnorm 371685.812 (371685.812)	prob 0.580 (0.5795)	GS 34.219 (34.219)	mem 54.686
Train: [4][230/750]	BT 0.158 (1.241)	DT 0.002 (1.118)	loss 9.709 (9.709)	gnorm 379474.875 (379474.875)	prob -0.570 (-0.5698)	GS 34.812 (34.812)	mem 54.711
Train: [4][231/750]	BT 0.246 (1.236)	DT 0.009 (1.113)	loss 9.386 (9.386)	gnorm 413458.125 (413458.125)	prob 0.173 (0.1726)	GS 29.594 (29.594)	mem 54.738
Train: [4][232/750]	BT 14.471 (1.293)	DT 14.362 (1.170)	loss 9.498 (9.498)	gnorm 325171.438 (325171.438)	prob 0.215 (0.2146)	GS 32.734 (32.734)	mem 57.959
Train: [4][233/750]	BT 0.086 (1.288)	DT 0.005 (1.165)	loss 9.076 (9.076)	gnorm 410412.906 (410412.906)	prob 0.232 (0.2316)	GS 28.469 (28.469)	mem 57.993
Train: [4][234/750]	BT 0.128 (1.283)	DT 0.001 (1.160)	loss 10.015 (10.015)	gnorm 387989.969 (387989.969)	prob -0.012 (-0.0116)	GS 31.484 (31.484)	mem 58.017
Train: [4][235/750]	BT 0.125 (1.278)	DT 0.011 (1.155)	loss 9.145 (9.145)	gnorm 377806.656 (377806.656)	prob 0.020 (0.0202)	GS 35.594 (35.594)	mem 58.071
Train: [4][236/750]	BT 0.153 (1.274)	DT 0.003 (1.150)	loss 8.836 (8.836)	gnorm 371771.281 (371771.281)	prob 0.440 (0.4404)	GS 28.734 (28.734)	mem 58.208
Train: [4][237/750]	BT 0.202 (1.269)	DT 0.002 (1.146)	loss 9.014 (9.014)	gnorm 353945.375 (353945.375)	prob 0.421 (0.4210)	GS 30.531 (30.531)	mem 58.296
Train: [4][238/750]	BT 0.204 (1.265)	DT 0.013 (1.141)	loss 9.400 (9.400)	gnorm 372269.719 (372269.719)	prob 0.083 (0.0831)	GS 33.953 (33.953)	mem 58.127
Train: [4][239/750]	BT 0.309 (1.261)	DT 0.026 (1.136)	loss 9.082 (9.082)	gnorm 393375.844 (393375.844)	prob 0.071 (0.0713)	GS 27.141 (27.141)	mem 58.164
Train: [4][240/750]	BT 0.087 (1.256)	DT 0.001 (1.131)	loss 9.575 (9.575)	gnorm 412687.344 (412687.344)	prob -0.248 (-0.2477)	GS 33.375 (33.375)	mem 58.173
Train: [4][241/750]	BT 0.180 (1.251)	DT 0.002 (1.127)	loss 9.337 (9.337)	gnorm 380956.812 (380956.812)	prob -0.263 (-0.2635)	GS 32.547 (32.547)	mem 58.297
Train: [4][242/750]	BT 0.156 (1.247)	DT 0.002 (1.122)	loss 10.122 (10.122)	gnorm 360492.938 (360492.938)	prob -1.234 (-1.2336)	GS 33.031 (33.031)	mem 58.415
Train: [4][243/750]	BT 0.139 (1.242)	DT 0.007 (1.117)	loss 9.228 (9.228)	gnorm 352460.562 (352460.562)	prob -0.455 (-0.4552)	GS 28.219 (28.219)	mem 58.421
Train: [4][244/750]	BT 12.623 (1.289)	DT 12.462 (1.164)	loss 9.612 (9.612)	gnorm 374251.344 (374251.344)	prob -0.643 (-0.6429)	GS 32.750 (32.750)	mem 65.118
Train: [4][245/750]	BT 0.176 (1.284)	DT 0.004 (1.159)	loss 9.087 (9.087)	gnorm 410024.531 (410024.531)	prob -0.254 (-0.2535)	GS 28.984 (28.984)	mem 65.087
Train: [4][246/750]	BT 0.104 (1.280)	DT 0.002 (1.155)	loss 10.182 (10.182)	gnorm 431638.781 (431638.781)	prob -0.612 (-0.6118)	GS 33.078 (33.078)	mem 65.025
Train: [4][247/750]	BT 0.183 (1.275)	DT 0.002 (1.150)	loss 9.347 (9.347)	gnorm 361222.062 (361222.062)	prob -0.536 (-0.5364)	GS 33.281 (33.281)	mem 65.035
Train: [4][248/750]	BT 0.218 (1.271)	DT 0.002 (1.145)	loss 9.528 (9.528)	gnorm 391277.188 (391277.188)	prob -0.091 (-0.0914)	GS 36.438 (36.438)	mem 65.033
Train: [4][249/750]	BT 0.207 (1.267)	DT 0.010 (1.141)	loss 9.261 (9.261)	gnorm 364995.969 (364995.969)	prob -0.326 (-0.3259)	GS 29.141 (29.141)	mem 65.034
Train: [4][250/750]	BT 0.141 (1.262)	DT 0.016 (1.136)	loss 9.866 (9.866)	gnorm 439484.688 (439484.688)	prob -0.637 (-0.6368)	GS 36.578 (36.578)	mem 65.036
Train: [4][251/750]	BT 0.122 (1.257)	DT 0.005 (1.132)	loss 10.113 (10.113)	gnorm 377103.000 (377103.000)	prob -0.758 (-0.7575)	GS 30.219 (30.219)	mem 65.036
Train: [4][252/750]	BT 0.252 (1.253)	DT 0.002 (1.127)	loss 9.576 (9.576)	gnorm 389857.188 (389857.188)	prob -0.391 (-0.3907)	GS 30.844 (30.844)	mem 65.075
Train: [4][253/750]	BT 0.142 (1.249)	DT 0.004 (1.123)	loss 9.263 (9.263)	gnorm 415867.281 (415867.281)	prob 0.194 (0.1944)	GS 33.859 (33.859)	mem 65.038
Train: [4][254/750]	BT 0.150 (1.245)	DT 0.013 (1.118)	loss 10.241 (10.241)	gnorm 437643.062 (437643.062)	prob -1.330 (-1.3299)	GS 32.203 (32.203)	mem 65.039
Train: [4][255/750]	BT 0.160 (1.241)	DT 0.004 (1.114)	loss 8.986 (8.986)	gnorm 408936.500 (408936.500)	prob -0.307 (-0.3068)	GS 46.469 (46.469)	mem 65.094
Train: [4][256/750]	BT 12.416 (1.284)	DT 12.252 (1.157)	loss 9.707 (9.707)	gnorm 367262.125 (367262.125)	prob -1.233 (-1.2331)	GS 33.891 (33.891)	mem 65.135
Train: [4][257/750]	BT 0.124 (1.280)	DT 0.017 (1.153)	loss 8.779 (8.779)	gnorm 381813.062 (381813.062)	prob -0.242 (-0.2424)	GS 33.688 (33.688)	mem 65.082
Train: [4][258/750]	BT 0.131 (1.275)	DT 0.002 (1.149)	loss 9.124 (9.124)	gnorm 345318.750 (345318.750)	prob -0.246 (-0.2456)	GS 35.562 (35.562)	mem 65.084
Train: [4][259/750]	BT 0.115 (1.271)	DT 0.002 (1.144)	loss 8.905 (8.905)	gnorm 347100.469 (347100.469)	prob -0.375 (-0.3753)	GS 29.750 (29.750)	mem 65.084
Train: [4][260/750]	BT 0.097 (1.266)	DT 0.001 (1.140)	loss 8.965 (8.965)	gnorm 381282.906 (381282.906)	prob -0.142 (-0.1416)	GS 38.297 (38.297)	mem 65.083
Train: [4][261/750]	BT 0.106 (1.262)	DT 0.002 (1.135)	loss 9.769 (9.769)	gnorm 398380.531 (398380.531)	prob -0.250 (-0.2502)	GS 31.781 (31.781)	mem 65.106
Train: [4][262/750]	BT 0.146 (1.258)	DT 0.002 (1.131)	loss 9.624 (9.624)	gnorm 363434.469 (363434.469)	prob -0.551 (-0.5506)	GS 31.953 (31.953)	mem 65.127
Train: [4][263/750]	BT 0.174 (1.253)	DT 0.003 (1.127)	loss 9.707 (9.707)	gnorm 387345.375 (387345.375)	prob -0.569 (-0.5687)	GS 34.734 (34.734)	mem 65.169
Train: [4][264/750]	BT 0.219 (1.249)	DT 0.008 (1.123)	loss 9.400 (9.400)	gnorm 396992.500 (396992.500)	prob -0.045 (-0.0447)	GS 38.266 (38.266)	mem 65.139
Train: [4][265/750]	BT 0.122 (1.245)	DT 0.003 (1.118)	loss 9.118 (9.118)	gnorm 382320.156 (382320.156)	prob -0.053 (-0.0530)	GS 32.438 (32.438)	mem 65.266
Train: [4][266/750]	BT 0.180 (1.241)	DT 0.002 (1.114)	loss 8.998 (8.998)	gnorm 367077.406 (367077.406)	prob 0.132 (0.1320)	GS 34.953 (34.953)	mem 65.328
Train: [4][267/750]	BT 0.183 (1.237)	DT 0.006 (1.110)	loss 9.133 (9.133)	gnorm 405896.406 (405896.406)	prob -0.443 (-0.4426)	GS 28.750 (28.750)	mem 65.309
Train: [4][268/750]	BT 10.959 (1.274)	DT 10.677 (1.146)	loss 9.836 (9.836)	gnorm 435854.250 (435854.250)	prob -0.813 (-0.8127)	GS 31.000 (31.000)	mem 65.145
Train: [4][269/750]	BT 0.204 (1.270)	DT 0.002 (1.141)	loss 9.781 (9.781)	gnorm 415025.969 (415025.969)	prob -1.060 (-1.0604)	GS 29.953 (29.953)	mem 65.122
Train: [4][270/750]	BT 0.255 (1.266)	DT 0.012 (1.137)	loss 9.732 (9.732)	gnorm 407999.062 (407999.062)	prob -0.966 (-0.9658)	GS 36.094 (36.094)	mem 65.155
Train: [4][271/750]	BT 0.124 (1.262)	DT 0.008 (1.133)	loss 9.319 (9.319)	gnorm 399934.906 (399934.906)	prob -0.322 (-0.3223)	GS 32.891 (32.891)	mem 65.167
Train: [4][272/750]	BT 0.113 (1.257)	DT 0.003 (1.129)	loss 9.501 (9.501)	gnorm 432995.125 (432995.125)	prob -0.423 (-0.4234)	GS 35.766 (35.766)	mem 65.123
Train: [4][273/750]	BT 0.176 (1.253)	DT 0.002 (1.125)	loss 9.621 (9.621)	gnorm 376844.750 (376844.750)	prob -0.592 (-0.5920)	GS 28.703 (28.703)	mem 65.186
Train: [4][274/750]	BT 0.164 (1.249)	DT 0.002 (1.121)	loss 9.632 (9.632)	gnorm 407118.594 (407118.594)	prob -1.166 (-1.1660)	GS 34.578 (34.578)	mem 65.158
Train: [4][275/750]	BT 0.134 (1.245)	DT 0.008 (1.117)	loss 9.319 (9.319)	gnorm 366483.812 (366483.812)	prob -0.904 (-0.9041)	GS 28.688 (28.688)	mem 65.201
Train: [4][276/750]	BT 0.222 (1.242)	DT 0.002 (1.113)	loss 9.631 (9.631)	gnorm 359301.281 (359301.281)	prob -0.862 (-0.8622)	GS 36.984 (36.984)	mem 65.173
Train: [4][277/750]	BT 0.118 (1.238)	DT 0.002 (1.109)	loss 9.676 (9.676)	gnorm 379672.719 (379672.719)	prob -1.246 (-1.2461)	GS 28.547 (28.547)	mem 65.139
Train: [4][278/750]	BT 0.085 (1.233)	DT 0.002 (1.105)	loss 9.408 (9.408)	gnorm 352279.844 (352279.844)	prob -1.069 (-1.0688)	GS 32.906 (32.906)	mem 65.154
Train: [4][279/750]	BT 0.094 (1.229)	DT 0.002 (1.101)	loss 9.233 (9.233)	gnorm 403213.281 (403213.281)	prob -0.247 (-0.2466)	GS 34.016 (34.016)	mem 65.155
Train: [4][280/750]	BT 10.718 (1.263)	DT 10.628 (1.135)	loss 9.744 (9.744)	gnorm 389122.562 (389122.562)	prob -0.950 (-0.9500)	GS 37.391 (37.391)	mem 59.308
Train: [4][281/750]	BT 0.106 (1.259)	DT 0.002 (1.131)	loss 9.111 (9.111)	gnorm 378879.656 (378879.656)	prob -0.136 (-0.1356)	GS 34.141 (34.141)	mem 58.352
Train: [4][282/750]	BT 0.162 (1.255)	DT 0.002 (1.127)	loss 9.257 (9.257)	gnorm 341926.844 (341926.844)	prob -0.582 (-0.5823)	GS 35.266 (35.266)	mem 58.281
Train: [4][283/750]	BT 0.151 (1.251)	DT 0.002 (1.123)	loss 9.175 (9.175)	gnorm 414155.000 (414155.000)	prob -0.079 (-0.0791)	GS 35.750 (35.750)	mem 58.305
Train: [4][284/750]	BT 0.202 (1.248)	DT 0.008 (1.119)	loss 9.262 (9.262)	gnorm 385862.250 (385862.250)	prob -0.026 (-0.0260)	GS 35.688 (35.688)	mem 58.349
Train: [4][285/750]	BT 0.179 (1.244)	DT 0.008 (1.115)	loss 9.590 (9.590)	gnorm 412330.094 (412330.094)	prob -0.426 (-0.4264)	GS 34.906 (34.906)	mem 58.439
Train: [4][286/750]	BT 0.178 (1.240)	DT 0.002 (1.111)	loss 9.652 (9.652)	gnorm 372215.844 (372215.844)	prob -0.868 (-0.8679)	GS 33.016 (33.016)	mem 58.571
Train: [4][287/750]	BT 0.149 (1.236)	DT 0.017 (1.107)	loss 9.534 (9.534)	gnorm 387795.844 (387795.844)	prob -0.716 (-0.7158)	GS 32.000 (32.000)	mem 58.488
Train: [4][288/750]	BT 0.142 (1.233)	DT 0.025 (1.103)	loss 9.013 (9.013)	gnorm 344768.219 (344768.219)	prob 0.022 (0.0216)	GS 35.453 (35.453)	mem 58.567
Train: [4][289/750]	BT 0.090 (1.229)	DT 0.003 (1.100)	loss 9.260 (9.260)	gnorm 371397.125 (371397.125)	prob -0.342 (-0.3417)	GS 30.859 (30.859)	mem 58.686
Train: [4][290/750]	BT 0.220 (1.225)	DT 0.003 (1.096)	loss 9.559 (9.559)	gnorm 354442.094 (354442.094)	prob -0.339 (-0.3391)	GS 31.156 (31.156)	mem 58.808
Train: [4][291/750]	BT 0.256 (1.222)	DT 0.006 (1.092)	loss 9.571 (9.571)	gnorm 408363.906 (408363.906)	prob -0.092 (-0.0916)	GS 34.328 (34.328)	mem 58.726
Train: [4][292/750]	BT 12.585 (1.261)	DT 12.484 (1.131)	loss 9.767 (9.767)	gnorm 387648.719 (387648.719)	prob -0.517 (-0.5165)	GS 33.750 (33.750)	mem 61.054
Train: [4][293/750]	BT 0.174 (1.257)	DT 0.002 (1.127)	loss 9.473 (9.473)	gnorm 373209.188 (373209.188)	prob -0.179 (-0.1786)	GS 33.703 (33.703)	mem 60.938
Train: [4][294/750]	BT 0.119 (1.253)	DT 0.008 (1.123)	loss 10.233 (10.233)	gnorm 401725.844 (401725.844)	prob -1.370 (-1.3696)	GS 40.047 (40.047)	mem 60.907
Train: [4][295/750]	BT 0.083 (1.249)	DT 0.002 (1.120)	loss 9.363 (9.363)	gnorm 382473.906 (382473.906)	prob -0.518 (-0.5178)	GS 31.906 (31.906)	mem 60.944
Train: [4][296/750]	BT 0.561 (1.247)	DT 0.429 (1.117)	loss 10.041 (10.041)	gnorm 376119.875 (376119.875)	prob -0.998 (-0.9979)	GS 34.000 (34.000)	mem 61.133
Train: [4][297/750]	BT 0.091 (1.243)	DT 0.002 (1.114)	loss 9.145 (9.145)	gnorm 360552.531 (360552.531)	prob -0.522 (-0.5216)	GS 33.688 (33.688)	mem 60.970
Train: [4][298/750]	BT 0.103 (1.239)	DT 0.002 (1.110)	loss 9.527 (9.527)	gnorm 414781.500 (414781.500)	prob -0.189 (-0.1886)	GS 39.875 (39.875)	mem 61.002
Train: [4][299/750]	BT 0.188 (1.236)	DT 0.010 (1.106)	loss 9.377 (9.377)	gnorm 374022.688 (374022.688)	prob -0.086 (-0.0859)	GS 37.297 (37.297)	mem 61.027
Train: [4][300/750]	BT 0.189 (1.232)	DT 0.011 (1.102)	loss 9.206 (9.206)	gnorm 402793.750 (402793.750)	prob -0.097 (-0.0966)	GS 37.547 (37.547)	mem 61.136
Train: [4][301/750]	BT 0.115 (1.228)	DT 0.007 (1.099)	loss 9.343 (9.343)	gnorm 394940.500 (394940.500)	prob -0.131 (-0.1307)	GS 29.781 (29.781)	mem 61.105
Train: [4][302/750]	BT 0.083 (1.225)	DT 0.002 (1.095)	loss 9.443 (9.443)	gnorm 375133.156 (375133.156)	prob -0.070 (-0.0697)	GS 35.047 (35.047)	mem 61.112
Train: [4][303/750]	BT 0.088 (1.221)	DT 0.002 (1.092)	loss 9.248 (9.248)	gnorm 446172.844 (446172.844)	prob -0.402 (-0.4015)	GS 32.141 (32.141)	mem 61.131
Train: [4][304/750]	BT 12.952 (1.259)	DT 12.861 (1.130)	loss 9.342 (9.342)	gnorm 425878.438 (425878.438)	prob 0.161 (0.1613)	GS 37.031 (37.031)	mem 63.408
Train: [4][305/750]	BT 0.189 (1.256)	DT 0.002 (1.127)	loss 8.835 (8.835)	gnorm 381973.219 (381973.219)	prob 0.677 (0.6768)	GS 26.391 (26.391)	mem 63.413
Train: [4][306/750]	BT 1.572 (1.257)	DT 1.458 (1.128)	loss 9.392 (9.392)	gnorm 403741.344 (403741.344)	prob 0.146 (0.1462)	GS 39.281 (39.281)	mem 63.636
Train: [4][307/750]	BT 0.141 (1.253)	DT 0.003 (1.124)	loss 9.535 (9.535)	gnorm 434937.656 (434937.656)	prob -0.476 (-0.4765)	GS 30.266 (30.266)	mem 63.658
Train: [4][308/750]	BT 2.598 (1.258)	DT 2.417 (1.128)	loss 9.495 (9.495)	gnorm 374156.344 (374156.344)	prob -0.201 (-0.2010)	GS 34.656 (34.656)	mem 63.964
Train: [4][309/750]	BT 0.090 (1.254)	DT 0.002 (1.125)	loss 9.615 (9.615)	gnorm 392996.406 (392996.406)	prob -0.605 (-0.6047)	GS 26.703 (26.703)	mem 63.990
Train: [4][310/750]	BT 0.093 (1.250)	DT 0.002 (1.121)	loss 9.258 (9.258)	gnorm 351649.281 (351649.281)	prob -0.497 (-0.4973)	GS 35.594 (35.594)	mem 64.022
Train: [4][311/750]	BT 0.296 (1.247)	DT 0.002 (1.117)	loss 9.104 (9.104)	gnorm 352618.938 (352618.938)	prob -0.048 (-0.0480)	GS 30.297 (30.297)	mem 64.143
Train: [4][312/750]	BT 0.140 (1.244)	DT 0.003 (1.114)	loss 9.398 (9.398)	gnorm 347769.188 (347769.188)	prob -0.585 (-0.5845)	GS 38.406 (38.406)	mem 64.074
Train: [4][313/750]	BT 0.150 (1.240)	DT 0.012 (1.110)	loss 9.011 (9.011)	gnorm 383458.000 (383458.000)	prob -0.085 (-0.0847)	GS 28.734 (28.734)	mem 64.089
Train: [4][314/750]	BT 0.129 (1.237)	DT 0.003 (1.107)	loss 9.633 (9.633)	gnorm 372321.281 (372321.281)	prob -1.299 (-1.2995)	GS 29.031 (29.031)	mem 64.101
Train: [4][315/750]	BT 0.189 (1.233)	DT 0.004 (1.103)	loss 9.179 (9.179)	gnorm 401377.094 (401377.094)	prob -0.268 (-0.2684)	GS 29.047 (29.047)	mem 64.132
Train: [4][316/750]	BT 11.736 (1.266)	DT 11.588 (1.136)	loss 9.659 (9.659)	gnorm 378715.000 (378715.000)	prob -1.012 (-1.0120)	GS 33.625 (33.625)	mem 59.630
Train: [4][317/750]	BT 0.142 (1.263)	DT 0.002 (1.133)	loss 9.634 (9.634)	gnorm 372325.500 (372325.500)	prob -0.752 (-0.7520)	GS 29.719 (29.719)	mem 59.655
Train: [4][318/750]	BT 0.602 (1.261)	DT 0.500 (1.131)	loss 9.411 (9.411)	gnorm 389213.500 (389213.500)	prob -0.465 (-0.4649)	GS 32.828 (32.828)	mem 59.708
Train: [4][319/750]	BT 0.245 (1.258)	DT 0.002 (1.127)	loss 9.213 (9.213)	gnorm 384472.406 (384472.406)	prob -0.410 (-0.4098)	GS 33.094 (33.094)	mem 59.804
Train: [4][320/750]	BT 2.904 (1.263)	DT 2.681 (1.132)	loss 9.816 (9.816)	gnorm 417278.562 (417278.562)	prob -0.841 (-0.8405)	GS 31.844 (31.844)	mem 60.565
Train: [4][321/750]	BT 0.159 (1.259)	DT 0.005 (1.129)	loss 9.193 (9.193)	gnorm 417363.188 (417363.188)	prob -0.002 (-0.0022)	GS 27.953 (27.953)	mem 60.608
Train: [4][322/750]	BT 0.169 (1.256)	DT 0.007 (1.125)	loss 8.995 (8.995)	gnorm 340354.938 (340354.938)	prob -0.050 (-0.0502)	GS 32.453 (32.453)	mem 60.653
Train: [4][323/750]	BT 0.101 (1.252)	DT 0.003 (1.122)	loss 8.891 (8.891)	gnorm 389981.344 (389981.344)	prob 0.178 (0.1776)	GS 33.922 (33.922)	mem 60.671
Train: [4][324/750]	BT 0.141 (1.249)	DT 0.003 (1.118)	loss 9.648 (9.648)	gnorm 350990.719 (350990.719)	prob -0.856 (-0.8559)	GS 34.688 (34.688)	mem 60.699
Train: [4][325/750]	BT 0.214 (1.246)	DT 0.004 (1.115)	loss 9.593 (9.593)	gnorm 409363.219 (409363.219)	prob -0.424 (-0.4239)	GS 32.688 (32.688)	mem 60.742
Train: [4][326/750]	BT 0.153 (1.242)	DT 0.009 (1.111)	loss 9.635 (9.635)	gnorm 375354.719 (375354.719)	prob -0.771 (-0.7714)	GS 35.062 (35.062)	mem 60.760
Train: [4][327/750]	BT 0.171 (1.239)	DT 0.016 (1.108)	loss 9.119 (9.119)	gnorm 367229.469 (367229.469)	prob -0.304 (-0.3045)	GS 31.125 (31.125)	mem 60.783
Train: [4][328/750]	BT 6.557 (1.255)	DT 6.403 (1.124)	loss 8.991 (8.991)	gnorm 358628.500 (358628.500)	prob -0.047 (-0.0472)	GS 33.172 (33.172)	mem 61.934
Train: [4][329/750]	BT 0.130 (1.252)	DT 0.002 (1.121)	loss 8.827 (8.827)	gnorm 350582.344 (350582.344)	prob 0.395 (0.3947)	GS 29.031 (29.031)	mem 61.881
Train: [4][330/750]	BT 3.843 (1.260)	DT 3.723 (1.129)	loss 9.338 (9.338)	gnorm 383388.562 (383388.562)	prob -0.372 (-0.3719)	GS 30.094 (30.094)	mem 62.510
Train: [4][331/750]	BT 0.124 (1.256)	DT 0.002 (1.125)	loss 9.326 (9.326)	gnorm 347787.969 (347787.969)	prob 0.224 (0.2243)	GS 33.891 (33.891)	mem 62.527
Train: [4][332/750]	BT 2.128 (1.259)	DT 2.018 (1.128)	loss 9.038 (9.038)	gnorm 350555.594 (350555.594)	prob 0.122 (0.1217)	GS 33.562 (33.562)	mem 62.943
Train: [4][333/750]	BT 0.119 (1.256)	DT 0.009 (1.125)	loss 9.027 (9.027)	gnorm 415628.531 (415628.531)	prob 0.108 (0.1079)	GS 32.984 (32.984)	mem 63.096
Train: [4][334/750]	BT 0.115 (1.252)	DT 0.003 (1.121)	loss 9.874 (9.874)	gnorm 367964.156 (367964.156)	prob -0.075 (-0.0751)	GS 35.156 (35.156)	mem 63.129
Train: [4][335/750]	BT 0.094 (1.249)	DT 0.002 (1.118)	loss 9.146 (9.146)	gnorm 361407.469 (361407.469)	prob 0.045 (0.0454)	GS 30.953 (30.953)	mem 63.127
Train: [4][336/750]	BT 0.122 (1.245)	DT 0.002 (1.115)	loss 9.862 (9.862)	gnorm 386386.438 (386386.438)	prob -0.388 (-0.3882)	GS 33.375 (33.375)	mem 63.099
Train: [4][337/750]	BT 0.213 (1.242)	DT 0.002 (1.111)	loss 8.978 (8.978)	gnorm 364185.781 (364185.781)	prob 0.285 (0.2847)	GS 31.125 (31.125)	mem 63.160
Train: [4][338/750]	BT 2.472 (1.246)	DT 2.328 (1.115)	loss 10.150 (10.150)	gnorm 420020.688 (420020.688)	prob -1.111 (-1.1113)	GS 39.078 (39.078)	mem 63.958
Train: [4][339/750]	BT 0.109 (1.243)	DT 0.002 (1.112)	loss 9.115 (9.115)	gnorm 374614.969 (374614.969)	prob -0.235 (-0.2351)	GS 29.938 (29.938)	mem 64.022
Train: [4][340/750]	BT 4.291 (1.252)	DT 4.029 (1.120)	loss 9.381 (9.381)	gnorm 379478.094 (379478.094)	prob -0.427 (-0.4270)	GS 34.594 (34.594)	mem 64.774
Train: [4][341/750]	BT 0.125 (1.248)	DT 0.009 (1.117)	loss 9.637 (9.637)	gnorm 382013.156 (382013.156)	prob -0.484 (-0.4839)	GS 30.953 (30.953)	mem 64.590
Train: [4][342/750]	BT 6.326 (1.263)	DT 6.159 (1.132)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.094 (32.094)	mem 64.993
Train: [4][343/750]	BT 0.180 (1.260)	DT 0.002 (1.128)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.812 (34.812)	mem 65.060
Train: [4][344/750]	BT 4.983 (1.271)	DT 4.873 (1.139)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.672 (33.672)	mem 65.005
Train: [4][345/750]	BT 0.272 (1.268)	DT 0.002 (1.136)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.797 (30.797)	mem 65.007
Train: [4][346/750]	BT 0.216 (1.265)	DT 0.006 (1.133)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.141 (31.141)	mem 65.008
Train: [4][347/750]	BT 0.181 (1.262)	DT 0.003 (1.129)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.844 (29.844)	mem 65.008
Train: [4][348/750]	BT 0.186 (1.259)	DT 0.001 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.516 (31.516)	mem 65.009
Train: [4][349/750]	BT 0.172 (1.255)	DT 0.005 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.641 (31.641)	mem 65.009
Train: [4][350/750]	BT 0.128 (1.252)	DT 0.003 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.516 (32.516)	mem 65.010
Train: [4][351/750]	BT 0.141 (1.249)	DT 0.005 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.062 (33.062)	mem 65.062
Train: [4][352/750]	BT 1.752 (1.251)	DT 1.560 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.516 (32.516)	mem 65.008
Train: [4][353/750]	BT 0.093 (1.247)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.922 (34.922)	mem 65.009
Train: [4][354/750]	BT 4.064 (1.255)	DT 3.966 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.125 (36.125)	mem 65.116
Train: [4][355/750]	BT 0.200 (1.252)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.062 (30.062)	mem 65.217
Train: [4][356/750]	BT 7.355 (1.269)	DT 7.142 (1.137)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.000 (34.000)	mem 65.064
Train: [4][357/750]	BT 0.125 (1.266)	DT 0.010 (1.133)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.438 (36.438)	mem 65.064
Train: [4][358/750]	BT 0.141 (1.263)	DT 0.002 (1.130)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.875 (33.875)	mem 65.066
Train: [4][359/750]	BT 0.174 (1.260)	DT 0.002 (1.127)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.141 (33.141)	mem 65.078
Train: [4][360/750]	BT 0.400 (1.258)	DT 0.040 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.109 (34.109)	mem 65.128
Train: [4][361/750]	BT 0.252 (1.255)	DT 0.021 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.047 (32.047)	mem 65.078
Train: [4][362/750]	BT 0.118 (1.252)	DT 0.001 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.109 (29.109)	mem 65.079
Train: [4][363/750]	BT 0.138 (1.249)	DT 0.003 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.594 (32.594)	mem 65.080
Train: [4][364/750]	BT 2.540 (1.252)	DT 2.433 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.984 (34.984)	mem 65.062
Train: [4][365/750]	BT 0.084 (1.249)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.703 (27.703)	mem 65.091
Train: [4][366/750]	BT 4.641 (1.258)	DT 4.447 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.844 (32.844)	mem 65.119
Train: [4][367/750]	BT 0.101 (1.255)	DT 0.002 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.953 (35.953)	mem 65.120
Train: [4][368/750]	BT 5.331 (1.266)	DT 5.201 (1.133)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.234 (34.234)	mem 64.944
Train: [4][369/750]	BT 0.124 (1.263)	DT 0.002 (1.129)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.109 (30.109)	mem 65.043
Train: [4][370/750]	BT 0.094 (1.260)	DT 0.002 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 65.263
Train: [4][371/750]	BT 0.209 (1.257)	DT 0.001 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.141 (34.141)	mem 65.203
Train: [4][372/750]	BT 0.159 (1.254)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.625 (37.625)	mem 64.925
Train: [4][373/750]	BT 0.104 (1.251)	DT 0.005 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.641 (30.641)	mem 64.934
Train: [4][374/750]	BT 0.090 (1.248)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.438 (32.438)	mem 64.932
Train: [4][375/750]	BT 0.090 (1.245)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.109 (34.109)	mem 64.957
Train: [4][376/750]	BT 4.427 (1.253)	DT 4.298 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.062 (33.062)	mem 65.310
Train: [4][377/750]	BT 0.257 (1.251)	DT 0.018 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.219 (31.219)	mem 65.091
Train: [4][378/750]	BT 2.533 (1.254)	DT 2.336 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.703 (33.703)	mem 58.328
Train: [4][379/750]	BT 0.146 (1.251)	DT 0.005 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.422 (27.422)	mem 58.349
Train: [4][380/750]	BT 7.926 (1.269)	DT 7.769 (1.135)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.703 (30.703)	mem 59.806
Train: [4][381/750]	BT 0.099 (1.266)	DT 0.002 (1.132)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.266 (35.266)	mem 59.881
Train: [4][382/750]	BT 0.185 (1.263)	DT 0.035 (1.129)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 59.945
Train: [4][383/750]	BT 0.246 (1.260)	DT 0.008 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.812 (31.812)	mem 59.934
Train: [4][384/750]	BT 0.178 (1.257)	DT 0.002 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.828 (31.828)	mem 60.028
Train: [4][385/750]	BT 0.131 (1.254)	DT 0.009 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.188 (30.188)	mem 60.037
Train: [4][386/750]	BT 0.120 (1.251)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.641 (30.641)	mem 60.064
Train: [4][387/750]	BT 0.236 (1.249)	DT 0.046 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.062 (29.062)	mem 60.119
Train: [4][388/750]	BT 3.167 (1.254)	DT 2.994 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.969 (36.969)	mem 60.700
Train: [4][389/750]	BT 0.151 (1.251)	DT 0.003 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.766 (33.766)	mem 60.728
Train: [4][390/750]	BT 0.078 (1.248)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.875 (35.875)	mem 60.759
Train: [4][391/750]	BT 0.130 (1.245)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.047 (33.047)	mem 60.825
Train: [4][392/750]	BT 9.368 (1.266)	DT 9.269 (1.132)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.016 (33.016)	mem 62.409
Train: [4][393/750]	BT 0.078 (1.263)	DT 0.001 (1.129)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.234 (30.234)	mem 62.424
Train: [4][394/750]	BT 0.081 (1.260)	DT 0.002 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.359 (31.359)	mem 62.460
Train: [4][395/750]	BT 0.180 (1.257)	DT 0.002 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.094 (33.094)	mem 62.485
Train: [4][396/750]	BT 0.119 (1.254)	DT 0.005 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.094 (38.094)	mem 62.496
Train: [4][397/750]	BT 0.157 (1.251)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.453 (29.453)	mem 62.519
Train: [4][398/750]	BT 0.154 (1.249)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.922 (34.922)	mem 62.582
Train: [4][399/750]	BT 0.277 (1.246)	DT 0.009 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.000 (34.000)	mem 62.602
Train: [4][400/750]	BT 4.044 (1.253)	DT 3.874 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.344 (33.344)	mem 63.486
Train: [4][401/750]	BT 0.238 (1.251)	DT 0.031 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.234 (32.234)	mem 63.519
Train: [4][402/750]	BT 0.273 (1.248)	DT 0.003 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.344 (32.344)	mem 63.545
Train: [4][403/750]	BT 0.275 (1.246)	DT 0.003 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.047 (28.047)	mem 63.564
Train: [4][404/750]	BT 7.977 (1.262)	DT 7.799 (1.127)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.188 (33.188)	mem 63.380
Train: [4][405/750]	BT 0.117 (1.260)	DT 0.002 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.078 (29.078)	mem 62.622
Train: [4][406/750]	BT 0.088 (1.257)	DT 0.002 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.953 (31.953)	mem 61.863
Train: [4][407/750]	BT 0.148 (1.254)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.031 (28.031)	mem 62.008
Train: [4][408/750]	BT 0.118 (1.251)	DT 0.003 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.516 (33.516)	mem 61.581
Train: [4][409/750]	BT 0.195 (1.249)	DT 0.012 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.516 (28.516)	mem 60.491
Train: [4][410/750]	BT 0.094 (1.246)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.750 (35.750)	mem 60.568
Train: [4][411/750]	BT 0.175 (1.243)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.016 (34.016)	mem 60.538
Train: [4][412/750]	BT 8.107 (1.260)	DT 7.952 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.250 (36.250)	mem 59.635
Train: [4][413/750]	BT 0.096 (1.257)	DT 0.002 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.234 (31.234)	mem 59.683
Train: [4][414/750]	BT 0.086 (1.254)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.312 (33.312)	mem 59.790
Train: [4][415/750]	BT 0.367 (1.252)	DT 0.009 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.516 (35.516)	mem 59.892
Train: [4][416/750]	BT 5.091 (1.261)	DT 4.970 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.547 (35.547)	mem 60.909
Train: [4][417/750]	BT 0.151 (1.259)	DT 0.021 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.047 (31.047)	mem 60.948
Train: [4][418/750]	BT 0.090 (1.256)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.406 (33.406)	mem 60.971
Train: [4][419/750]	BT 0.108 (1.253)	DT 0.003 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.406 (30.406)	mem 60.994
Train: [4][420/750]	BT 0.182 (1.251)	DT 0.003 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.484 (31.484)	mem 61.050
Train: [4][421/750]	BT 0.109 (1.248)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.281 (35.281)	mem 61.083
Train: [4][422/750]	BT 0.289 (1.246)	DT 0.029 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.703 (32.703)	mem 61.110
Train: [4][423/750]	BT 0.184 (1.243)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.844 (28.844)	mem 61.169
Train: [4][424/750]	BT 8.312 (1.260)	DT 8.099 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.953 (38.953)	mem 62.871
Train: [4][425/750]	BT 0.219 (1.257)	DT 0.002 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.969 (34.969)	mem 62.951
Train: [4][426/750]	BT 0.208 (1.255)	DT 0.008 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.656 (36.656)	mem 63.169
Train: [4][427/750]	BT 0.220 (1.252)	DT 0.027 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.719 (33.719)	mem 63.166
Train: [4][428/750]	BT 1.669 (1.253)	DT 1.508 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.188 (32.188)	mem 63.099
Train: [4][429/750]	BT 0.136 (1.251)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 25.734 (25.734)	mem 63.158
Train: [4][430/750]	BT 0.143 (1.248)	DT 0.003 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.719 (32.719)	mem 63.191
Train: [4][431/750]	BT 0.228 (1.246)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.922 (32.922)	mem 63.222
Train: [4][432/750]	BT 0.133 (1.243)	DT 0.008 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.531 (30.531)	mem 63.315
Train: [4][433/750]	BT 0.088 (1.241)	DT 0.002 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.219 (34.219)	mem 63.393
Train: [4][434/750]	BT 0.129 (1.238)	DT 0.002 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.406 (34.406)	mem 63.459
Train: [4][435/750]	BT 0.291 (1.236)	DT 0.028 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.531 (33.531)	mem 63.424
Train: [4][436/750]	BT 13.439 (1.264)	DT 13.300 (1.127)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.766 (38.766)	mem 64.984
Train: [4][437/750]	BT 0.117 (1.261)	DT 0.001 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.750 (28.750)	mem 64.985
Train: [4][438/750]	BT 0.099 (1.259)	DT 0.001 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.609 (37.609)	mem 64.984
Train: [4][439/750]	BT 0.100 (1.256)	DT 0.013 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.859 (30.859)	mem 65.117
Train: [4][440/750]	BT 0.124 (1.253)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.891 (34.891)	mem 65.075
Train: [4][441/750]	BT 0.185 (1.251)	DT 0.017 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.188 (32.188)	mem 64.987
Train: [4][442/750]	BT 0.261 (1.249)	DT 0.010 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.641 (40.641)	mem 64.988
Train: [4][443/750]	BT 0.194 (1.246)	DT 0.004 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.312 (30.312)	mem 64.988
Train: [4][444/750]	BT 0.178 (1.244)	DT 0.027 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.688 (34.688)	mem 64.987
Train: [4][445/750]	BT 0.201 (1.242)	DT 0.034 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.656 (31.656)	mem 64.987
Train: [4][446/750]	BT 0.135 (1.239)	DT 0.001 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.078 (39.078)	mem 65.020
Train: [4][447/750]	BT 0.202 (1.237)	DT 0.003 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.547 (34.547)	mem 64.986
Train: [4][448/750]	BT 11.342 (1.259)	DT 11.210 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.891 (34.891)	mem 64.984
Train: [4][449/750]	BT 0.152 (1.257)	DT 0.003 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.609 (29.609)	mem 65.018
Train: [4][450/750]	BT 0.200 (1.255)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.203 (32.203)	mem 65.295
Train: [4][451/750]	BT 0.205 (1.252)	DT 0.004 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.047 (31.047)	mem 65.229
Train: [4][452/750]	BT 0.147 (1.250)	DT 0.016 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.141 (35.141)	mem 64.982
Train: [4][453/750]	BT 0.111 (1.247)	DT 0.002 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.406 (32.406)	mem 64.984
Train: [4][454/750]	BT 0.115 (1.245)	DT 0.001 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.828 (34.828)	mem 65.094
Train: [4][455/750]	BT 0.188 (1.242)	DT 0.009 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.328 (31.328)	mem 64.988
Train: [4][456/750]	BT 1.451 (1.243)	DT 1.332 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.266 (34.266)	mem 64.999
Train: [4][457/750]	BT 0.138 (1.240)	DT 0.018 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.359 (30.359)	mem 65.000
Train: [4][458/750]	BT 0.091 (1.238)	DT 0.002 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.906 (32.906)	mem 65.000
Train: [4][459/750]	BT 0.185 (1.236)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.484 (32.484)	mem 64.999
Train: [4][460/750]	BT 7.442 (1.249)	DT 7.198 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.328 (28.328)	mem 65.011
Train: [4][461/750]	BT 0.200 (1.247)	DT 0.026 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.078 (31.078)	mem 65.039
Train: [4][462/750]	BT 0.241 (1.245)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.484 (33.484)	mem 65.118
Train: [4][463/750]	BT 0.214 (1.242)	DT 0.011 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.828 (28.828)	mem 65.140
Train: [4][464/750]	BT 0.183 (1.240)	DT 0.020 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.156 (36.156)	mem 65.117
Train: [4][465/750]	BT 0.173 (1.238)	DT 0.001 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.328 (33.328)	mem 65.021
Train: [4][466/750]	BT 0.181 (1.236)	DT 0.002 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.156 (31.156)	mem 65.073
Train: [4][467/750]	BT 0.293 (1.234)	DT 0.009 (1.095)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.516 (31.516)	mem 65.058
Train: [4][468/750]	BT 8.389 (1.249)	DT 8.279 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.062 (33.062)	mem 65.056
Train: [4][469/750]	BT 0.270 (1.247)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.016 (36.016)	mem 65.219
Train: [4][470/750]	BT 0.167 (1.245)	DT 0.004 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.469 (34.469)	mem 65.033
Train: [4][471/750]	BT 0.245 (1.242)	DT 0.022 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.844 (26.844)	mem 65.033
Train: [4][472/750]	BT 3.180 (1.246)	DT 3.076 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.406 (32.406)	mem 64.907
Train: [4][473/750]	BT 0.141 (1.244)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.672 (29.672)	mem 64.907
Train: [4][474/750]	BT 0.126 (1.242)	DT 0.011 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.609 (28.609)	mem 64.906
Train: [4][475/750]	BT 0.133 (1.239)	DT 0.003 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.812 (39.812)	mem 64.933
Train: [4][476/750]	BT 0.242 (1.237)	DT 0.084 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.688 (37.688)	mem 64.970
Train: [4][477/750]	BT 0.127 (1.235)	DT 0.010 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.953 (31.953)	mem 64.903
Train: [4][478/750]	BT 0.180 (1.233)	DT 0.004 (1.094)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.188 (33.188)	mem 64.898
Train: [4][479/750]	BT 0.125 (1.231)	DT 0.008 (1.091)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.484 (40.484)	mem 64.509
Train: [4][480/750]	BT 11.805 (1.253)	DT 11.724 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.266 (33.266)	mem 59.967
Train: [4][481/750]	BT 0.282 (1.251)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.438 (32.438)	mem 60.023
Train: [4][482/750]	BT 0.201 (1.248)	DT 0.025 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.938 (34.938)	mem 60.071
Train: [4][483/750]	BT 0.166 (1.246)	DT 0.013 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.688 (27.688)	mem 60.116
Train: [4][484/750]	BT 1.463 (1.247)	DT 1.253 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.641 (37.641)	mem 60.384
Train: [4][485/750]	BT 0.159 (1.244)	DT 0.005 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.547 (31.547)	mem 60.322
Train: [4][486/750]	BT 0.094 (1.242)	DT 0.002 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.047 (30.047)	mem 60.422
Train: [4][487/750]	BT 0.089 (1.240)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.672 (30.672)	mem 60.384
Train: [4][488/750]	BT 2.345 (1.242)	DT 2.187 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.469 (33.469)	mem 60.803
Train: [4][489/750]	BT 0.130 (1.240)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.422 (37.422)	mem 60.822
Train: [4][490/750]	BT 0.154 (1.237)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.422 (32.422)	mem 60.842
Train: [4][491/750]	BT 0.097 (1.235)	DT 0.002 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.125 (33.125)	mem 60.857
Train: [4][492/750]	BT 11.032 (1.255)	DT 10.957 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.000 (33.000)	mem 62.694
Train: [4][493/750]	BT 0.287 (1.253)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.000 (29.000)	mem 62.826
Train: [4][494/750]	BT 0.192 (1.251)	DT 0.009 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.703 (32.703)	mem 62.952
Train: [4][495/750]	BT 0.221 (1.249)	DT 0.011 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.516 (33.516)	mem 62.914
Train: [4][496/750]	BT 1.458 (1.249)	DT 1.323 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.859 (35.859)	mem 63.135
Train: [4][497/750]	BT 0.147 (1.247)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.531 (28.531)	mem 63.167
Train: [4][498/750]	BT 0.250 (1.245)	DT 0.039 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.406 (33.406)	mem 63.068
Train: [4][499/750]	BT 0.137 (1.243)	DT 0.019 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.172 (29.172)	mem 63.084
Train: [4][500/750]	BT 10.783 (1.262)	DT 10.708 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.375 (36.375)	mem 61.702
Train: [4][501/750]	BT 0.150 (1.260)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.688 (26.688)	mem 61.781
Train: [4][502/750]	BT 0.130 (1.257)	DT 0.005 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.438 (35.438)	mem 61.661
Train: [4][503/750]	BT 0.178 (1.255)	DT 0.002 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.578 (33.578)	mem 61.314
Train: [4][504/750]	BT 0.462 (1.254)	DT 0.301 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.453 (36.453)	mem 61.002
Train: [4][505/750]	BT 0.127 (1.251)	DT 0.007 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.797 (29.797)	mem 60.603
Train: [4][506/750]	BT 0.207 (1.249)	DT 0.029 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.312 (31.312)	mem 60.247
Train: [4][507/750]	BT 0.108 (1.247)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.953 (29.953)	mem 60.273
Train: [4][508/750]	BT 0.138 (1.245)	DT 0.006 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.719 (30.719)	mem 59.968
Train: [4][509/750]	BT 0.099 (1.243)	DT 0.002 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.000 (30.000)	mem 59.353
Train: [4][510/750]	BT 0.931 (1.242)	DT 0.784 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.531 (35.531)	mem 58.631
Train: [4][511/750]	BT 0.102 (1.240)	DT 0.001 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 43.734 (43.734)	mem 58.644
Train: [4][512/750]	BT 11.761 (1.260)	DT 11.664 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.781 (34.781)	mem 60.823
Train: [4][513/750]	BT 0.139 (1.258)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.328 (32.328)	mem 60.872
Train: [4][514/750]	BT 0.184 (1.256)	DT 0.017 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.219 (36.219)	mem 61.025
Train: [4][515/750]	BT 0.267 (1.254)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.656 (27.656)	mem 60.952
Train: [4][516/750]	BT 0.136 (1.252)	DT 0.017 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.812 (30.812)	mem 60.986
Train: [4][517/750]	BT 0.235 (1.250)	DT 0.006 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.516 (27.516)	mem 60.960
Train: [4][518/750]	BT 0.259 (1.248)	DT 0.028 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.406 (34.406)	mem 60.982
Train: [4][519/750]	BT 0.158 (1.246)	DT 0.003 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.000 (32.000)	mem 61.043
Train: [4][520/750]	BT 0.127 (1.244)	DT 0.022 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.828 (32.828)	mem 61.018
Train: [4][521/750]	BT 0.080 (1.242)	DT 0.004 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.656 (29.656)	mem 61.028
Train: [4][522/750]	BT 1.299 (1.242)	DT 1.079 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.891 (32.891)	mem 61.625
Train: [4][523/750]	BT 0.179 (1.240)	DT 0.008 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.875 (31.875)	mem 61.710
Train: [4][524/750]	BT 14.855 (1.266)	DT 14.762 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.391 (37.391)	mem 64.284
Train: [4][525/750]	BT 0.084 (1.263)	DT 0.002 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.484 (31.484)	mem 64.296
Train: [4][526/750]	BT 0.139 (1.261)	DT 0.002 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.000 (29.000)	mem 64.283
Train: [4][527/750]	BT 0.148 (1.259)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.719 (31.719)	mem 64.319
Train: [4][528/750]	BT 0.197 (1.257)	DT 0.003 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.891 (30.891)	mem 64.369
Train: [4][529/750]	BT 0.164 (1.255)	DT 0.009 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.375 (32.375)	mem 64.428
Train: [4][530/750]	BT 0.143 (1.253)	DT 0.005 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.281 (32.281)	mem 64.551
Train: [4][531/750]	BT 0.113 (1.251)	DT 0.025 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.312 (36.312)	mem 64.454
Train: [4][532/750]	BT 0.131 (1.249)	DT 0.001 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.422 (33.422)	mem 64.481
Train: [4][533/750]	BT 0.159 (1.247)	DT 0.012 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.078 (30.078)	mem 64.513
Train: [4][534/750]	BT 0.293 (1.245)	DT 0.064 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.891 (33.891)	mem 64.582
Train: [4][535/750]	BT 0.144 (1.243)	DT 0.007 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.531 (34.531)	mem 64.598
Train: [4][536/750]	BT 13.466 (1.266)	DT 13.371 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.391 (34.391)	mem 65.024
Train: [4][537/750]	BT 0.162 (1.264)	DT 0.002 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.781 (29.781)	mem 65.044
Train: [4][538/750]	BT 0.099 (1.261)	DT 0.002 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.250 (35.250)	mem 65.028
Train: [4][539/750]	BT 0.127 (1.259)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.250 (33.250)	mem 65.028
Train: [4][540/750]	BT 0.125 (1.257)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.938 (32.938)	mem 65.029
Train: [4][541/750]	BT 0.147 (1.255)	DT 0.024 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.328 (30.328)	mem 65.029
Train: [4][542/750]	BT 0.157 (1.253)	DT 0.004 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.219 (36.219)	mem 65.028
Train: [4][543/750]	BT 0.096 (1.251)	DT 0.001 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 24.969 (24.969)	mem 65.028
Train: [4][544/750]	BT 0.120 (1.249)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.797 (27.797)	mem 65.027
Train: [4][545/750]	BT 0.121 (1.247)	DT 0.006 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.891 (30.891)	mem 65.028
Train: [4][546/750]	BT 0.193 (1.245)	DT 0.049 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.688 (34.688)	mem 65.027
Train: [4][547/750]	BT 0.167 (1.243)	DT 0.003 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.859 (31.859)	mem 65.029
Train: [4][548/750]	BT 13.546 (1.265)	DT 13.427 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.938 (33.938)	mem 64.932
Train: [4][549/750]	BT 0.086 (1.263)	DT 0.002 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.875 (29.875)	mem 64.934
Train: [4][550/750]	BT 0.145 (1.261)	DT 0.002 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.469 (31.469)	mem 64.934
Train: [4][551/750]	BT 0.137 (1.259)	DT 0.010 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.641 (30.641)	mem 64.934
Train: [4][552/750]	BT 0.127 (1.257)	DT 0.001 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.609 (33.609)	mem 64.934
Train: [4][553/750]	BT 0.151 (1.255)	DT 0.004 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.781 (33.781)	mem 64.964
Train: [4][554/750]	BT 0.190 (1.253)	DT 0.015 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.281 (34.281)	mem 64.915
Train: [4][555/750]	BT 0.206 (1.251)	DT 0.008 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.406 (30.406)	mem 64.969
Train: [4][556/750]	BT 0.257 (1.250)	DT 0.010 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.672 (37.672)	mem 65.117
Train: [4][557/750]	BT 0.166 (1.248)	DT 0.018 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.781 (31.781)	mem 65.115
Train: [4][558/750]	BT 0.183 (1.246)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.297 (36.297)	mem 64.920
Train: [4][559/750]	BT 0.116 (1.244)	DT 0.012 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.250 (33.250)	mem 64.918
Train: [4][560/750]	BT 15.093 (1.268)	DT 15.013 (1.129)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.438 (34.438)	mem 58.594
Train: [4][561/750]	BT 0.099 (1.266)	DT 0.002 (1.127)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.859 (28.859)	mem 58.491
Train: [4][562/750]	BT 0.105 (1.264)	DT 0.001 (1.125)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.531 (35.531)	mem 58.531
Train: [4][563/750]	BT 0.088 (1.262)	DT 0.003 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.828 (35.828)	mem 58.564
Train: [4][564/750]	BT 0.116 (1.260)	DT 0.001 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.219 (36.219)	mem 58.601
Train: [4][565/750]	BT 0.113 (1.258)	DT 0.003 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.000 (26.000)	mem 58.633
Train: [4][566/750]	BT 0.170 (1.256)	DT 0.005 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.766 (34.766)	mem 58.702
Train: [4][567/750]	BT 0.089 (1.254)	DT 0.001 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.297 (32.297)	mem 58.671
Train: [4][568/750]	BT 0.143 (1.252)	DT 0.011 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.578 (33.578)	mem 58.693
Train: [4][569/750]	BT 0.099 (1.250)	DT 0.003 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.156 (32.156)	mem 58.717
Train: [4][570/750]	BT 0.107 (1.248)	DT 0.003 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.781 (30.781)	mem 58.738
Train: [4][571/750]	BT 0.103 (1.246)	DT 0.007 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.922 (28.922)	mem 58.747
Train: [4][572/750]	BT 10.974 (1.263)	DT 10.862 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.188 (32.188)	mem 60.859
Train: [4][573/750]	BT 0.110 (1.261)	DT 0.021 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.875 (34.875)	mem 60.877
Train: [4][574/750]	BT 0.091 (1.259)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.734 (32.734)	mem 60.898
Train: [4][575/750]	BT 0.105 (1.257)	DT 0.002 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.859 (32.859)	mem 60.953
Train: [4][576/750]	BT 0.178 (1.255)	DT 0.004 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.078 (33.078)	mem 60.950
Train: [4][577/750]	BT 0.081 (1.253)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.812 (29.812)	mem 61.017
Train: [4][578/750]	BT 0.195 (1.251)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.656 (31.656)	mem 61.177
Train: [4][579/750]	BT 0.204 (1.250)	DT 0.003 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.266 (30.266)	mem 61.136
Train: [4][580/750]	BT 0.110 (1.248)	DT 0.015 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.594 (37.594)	mem 61.084
Train: [4][581/750]	BT 0.093 (1.246)	DT 0.014 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.172 (29.172)	mem 61.120
Train: [4][582/750]	BT 0.263 (1.244)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.812 (33.812)	mem 61.166
Train: [4][583/750]	BT 0.155 (1.242)	DT 0.007 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.641 (32.641)	mem 61.188
Train: [4][584/750]	BT 12.102 (1.261)	DT 11.990 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.953 (32.953)	mem 63.508
Train: [4][585/750]	BT 0.115 (1.259)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.766 (28.766)	mem 63.601
Train: [4][586/750]	BT 0.082 (1.257)	DT 0.006 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.953 (33.953)	mem 63.996
Train: [4][587/750]	BT 0.149 (1.255)	DT 0.002 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.625 (27.625)	mem 63.839
Train: [4][588/750]	BT 0.089 (1.253)	DT 0.003 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.562 (32.562)	mem 63.854
Train: [4][589/750]	BT 0.106 (1.251)	DT 0.001 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.781 (34.781)	mem 63.714
Train: [4][590/750]	BT 0.093 (1.249)	DT 0.002 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.422 (30.422)	mem 63.733
Train: [4][591/750]	BT 0.167 (1.247)	DT 0.015 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.094 (28.094)	mem 63.699
Train: [4][592/750]	BT 0.085 (1.245)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.688 (34.688)	mem 63.726
Train: [4][593/750]	BT 0.303 (1.244)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.406 (30.406)	mem 63.815
Train: [4][594/750]	BT 0.117 (1.242)	DT 0.004 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.078 (38.078)	mem 63.826
Train: [4][595/750]	BT 0.197 (1.240)	DT 0.002 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.422 (31.422)	mem 63.796
Train: [4][596/750]	BT 13.613 (1.261)	DT 13.505 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.266 (33.266)	mem 59.700
Train: [4][597/750]	BT 0.155 (1.259)	DT 0.016 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.203 (29.203)	mem 59.669
Train: [4][598/750]	BT 0.148 (1.257)	DT 0.033 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.750 (33.750)	mem 59.717
Train: [4][599/750]	BT 0.119 (1.255)	DT 0.014 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.766 (33.766)	mem 59.773
Train: [4][600/750]	BT 0.136 (1.253)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.719 (32.719)	mem 59.776
Train: [4][601/750]	BT 0.198 (1.251)	DT 0.027 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.656 (35.656)	mem 59.832
Train: [4][602/750]	BT 0.105 (1.249)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.906 (33.906)	mem 59.960
Train: [4][603/750]	BT 0.170 (1.248)	DT 0.017 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.719 (29.719)	mem 59.908
Train: [4][604/750]	BT 0.158 (1.246)	DT 0.018 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.812 (31.812)	mem 60.119
Train: [4][605/750]	BT 0.119 (1.244)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.094 (30.094)	mem 60.058
Train: [4][606/750]	BT 0.235 (1.242)	DT 0.007 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.125 (37.125)	mem 60.090
Train: [4][607/750]	BT 0.141 (1.241)	DT 0.022 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.016 (28.016)	mem 60.147
Train: [4][608/750]	BT 11.434 (1.257)	DT 11.319 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.984 (31.984)	mem 62.510
Train: [4][609/750]	BT 0.139 (1.255)	DT 0.012 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.500 (31.500)	mem 62.551
Train: [4][610/750]	BT 0.156 (1.254)	DT 0.003 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.312 (35.312)	mem 62.563
Train: [4][611/750]	BT 0.102 (1.252)	DT 0.008 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.016 (27.016)	mem 62.576
Train: [4][612/750]	BT 0.089 (1.250)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.016 (33.016)	mem 62.551
Train: [4][613/750]	BT 0.174 (1.248)	DT 0.003 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.922 (30.922)	mem 62.577
Train: [4][614/750]	BT 0.168 (1.246)	DT 0.005 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.078 (33.078)	mem 62.616
Train: [4][615/750]	BT 0.166 (1.245)	DT 0.015 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.531 (26.531)	mem 62.704
Train: [4][616/750]	BT 0.171 (1.243)	DT 0.003 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.859 (29.859)	mem 62.678
Train: [4][617/750]	BT 0.116 (1.241)	DT 0.001 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.688 (30.688)	mem 62.765
Train: [4][618/750]	BT 2.051 (1.242)	DT 1.894 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.203 (37.203)	mem 62.979
Train: [4][619/750]	BT 0.131 (1.241)	DT 0.002 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 25.328 (25.328)	mem 63.038
Train: [4][620/750]	BT 11.661 (1.257)	DT 11.461 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.703 (34.703)	mem 65.127
Train: [4][621/750]	BT 0.170 (1.256)	DT 0.012 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.188 (31.188)	mem 65.062
Train: [4][622/750]	BT 0.136 (1.254)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.500 (31.500)	mem 65.062
Train: [4][623/750]	BT 0.167 (1.252)	DT 0.008 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.812 (29.812)	mem 65.062
Train: [4][624/750]	BT 0.097 (1.250)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.234 (33.234)	mem 65.060
Train: [4][625/750]	BT 0.170 (1.248)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.312 (30.312)	mem 65.062
Train: [4][626/750]	BT 0.174 (1.247)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.109 (32.109)	mem 65.063
Train: [4][627/750]	BT 0.193 (1.245)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.562 (28.562)	mem 65.064
Train: [4][628/750]	BT 0.146 (1.243)	DT 0.007 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.000 (29.000)	mem 65.064
Train: [4][629/750]	BT 0.091 (1.242)	DT 0.001 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.062 (32.062)	mem 65.064
Train: [4][630/750]	BT 3.862 (1.246)	DT 3.669 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.672 (36.672)	mem 65.077
Train: [4][631/750]	BT 0.208 (1.244)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.734 (32.734)	mem 65.131
Train: [4][632/750]	BT 6.287 (1.252)	DT 6.184 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.328 (34.328)	mem 65.029
Train: [4][633/750]	BT 0.125 (1.250)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.172 (30.172)	mem 65.074
Train: [4][634/750]	BT 0.154 (1.249)	DT 0.001 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.094 (34.094)	mem 65.231
Train: [4][635/750]	BT 0.153 (1.247)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.375 (30.375)	mem 65.298
Train: [4][636/750]	BT 0.177 (1.245)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.922 (31.922)	mem 65.165
Train: [4][637/750]	BT 0.189 (1.243)	DT 0.014 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.594 (30.594)	mem 64.967
Train: [4][638/750]	BT 0.164 (1.242)	DT 0.001 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.047 (33.047)	mem 64.968
Train: [4][639/750]	BT 0.176 (1.240)	DT 0.002 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.031 (29.031)	mem 64.971
Train: [4][640/750]	BT 0.114 (1.238)	DT 0.017 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.578 (31.578)	mem 64.971
Train: [4][641/750]	BT 0.176 (1.237)	DT 0.002 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.328 (30.328)	mem 64.984
Train: [4][642/750]	BT 11.355 (1.252)	DT 11.186 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.406 (35.406)	mem 65.200
Train: [4][643/750]	BT 0.218 (1.251)	DT 0.027 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.391 (38.391)	mem 65.071
Train: [4][644/750]	BT 2.906 (1.253)	DT 2.812 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.031 (33.031)	mem 64.970
Train: [4][645/750]	BT 0.190 (1.252)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.750 (28.750)	mem 64.971
Train: [4][646/750]	BT 0.149 (1.250)	DT 0.013 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.938 (32.938)	mem 64.972
Train: [4][647/750]	BT 0.182 (1.248)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.250 (30.250)	mem 64.973
Train: [4][648/750]	BT 0.132 (1.247)	DT 0.024 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.281 (32.281)	mem 64.974
Train: [4][649/750]	BT 0.178 (1.245)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.297 (32.297)	mem 64.986
Train: [4][650/750]	BT 0.179 (1.243)	DT 0.001 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.172 (33.172)	mem 65.173
Train: [4][651/750]	BT 0.325 (1.242)	DT 0.004 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.859 (31.859)	mem 65.121
Train: [4][652/750]	BT 0.221 (1.240)	DT 0.004 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.422 (34.422)	mem 65.197
Train: [4][653/750]	BT 0.125 (1.239)	DT 0.002 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.844 (28.844)	mem 65.311
Train: [4][654/750]	BT 12.544 (1.256)	DT 12.408 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.156 (35.156)	mem 58.798
Train: [4][655/750]	BT 0.098 (1.254)	DT 0.006 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.281 (31.281)	mem 58.877
Train: [4][656/750]	BT 3.706 (1.258)	DT 3.593 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.812 (32.812)	mem 59.661
Train: [4][657/750]	BT 0.129 (1.256)	DT 0.002 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.344 (32.344)	mem 59.685
Train: [4][658/750]	BT 0.099 (1.254)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.312 (34.312)	mem 59.732
Train: [4][659/750]	BT 0.078 (1.253)	DT 0.002 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.984 (30.984)	mem 59.783
Train: [4][660/750]	BT 0.124 (1.251)	DT 0.001 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.750 (37.750)	mem 59.830
Train: [4][661/750]	BT 0.181 (1.249)	DT 0.004 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.734 (33.734)	mem 59.883
Train: [4][662/750]	BT 0.267 (1.248)	DT 0.019 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.672 (34.672)	mem 59.939
Train: [4][663/750]	BT 0.178 (1.246)	DT 0.016 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.016 (29.016)	mem 60.006
Train: [4][664/750]	BT 0.123 (1.245)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.891 (33.891)	mem 60.040
Train: [4][665/750]	BT 0.178 (1.243)	DT 0.012 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.969 (30.969)	mem 60.095
Train: [4][666/750]	BT 5.965 (1.250)	DT 5.763 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.016 (35.016)	mem 60.982
Train: [4][667/750]	BT 0.256 (1.249)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.453 (34.453)	mem 61.063
Train: [4][668/750]	BT 3.278 (1.252)	DT 3.172 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.109 (33.109)	mem 61.657
Train: [4][669/750]	BT 0.139 (1.250)	DT 0.005 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.703 (30.703)	mem 61.680
Train: [4][670/750]	BT 0.118 (1.248)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.391 (35.391)	mem 61.713
Train: [4][671/750]	BT 0.158 (1.247)	DT 0.003 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.219 (31.219)	mem 61.757
Train: [4][672/750]	BT 0.203 (1.245)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.750 (33.750)	mem 61.814
Train: [4][673/750]	BT 0.276 (1.244)	DT 0.010 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.656 (31.656)	mem 61.832
Train: [4][674/750]	BT 0.105 (1.242)	DT 0.002 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.109 (30.109)	mem 61.870
Train: [4][675/750]	BT 0.095 (1.240)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.328 (32.328)	mem 61.900
Train: [4][676/750]	BT 0.186 (1.239)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.766 (35.766)	mem 61.957
Train: [4][677/750]	BT 0.199 (1.237)	DT 0.013 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.719 (31.719)	mem 61.987
Train: [4][678/750]	BT 8.755 (1.248)	DT 8.535 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.266 (36.266)	mem 63.186
Train: [4][679/750]	BT 0.240 (1.247)	DT 0.007 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.547 (30.547)	mem 63.219
Train: [4][680/750]	BT 5.403 (1.253)	DT 5.301 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.312 (35.312)	mem 63.979
Train: [4][681/750]	BT 0.086 (1.251)	DT 0.001 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.875 (28.875)	mem 64.001
Train: [4][682/750]	BT 0.074 (1.249)	DT 0.001 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.859 (31.859)	mem 64.026
Train: [4][683/750]	BT 0.107 (1.248)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.781 (29.781)	mem 64.047
Train: [4][684/750]	BT 0.103 (1.246)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.312 (34.312)	mem 64.060
Train: [4][685/750]	BT 0.150 (1.244)	DT 0.002 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.453 (28.453)	mem 64.085
arpack error, retry= 0
Train: [4][686/750]	BT 0.088 (1.243)	DT 0.002 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.844 (35.844)	mem 64.104
Train: [4][687/750]	BT 0.135 (1.241)	DT 0.003 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.344 (36.344)	mem 64.116
Train: [4][688/750]	BT 0.087 (1.239)	DT 0.003 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.812 (36.812)	mem 64.127
Train: [4][689/750]	BT 0.191 (1.238)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.172 (35.172)	mem 64.142
Train: [4][690/750]	BT 9.449 (1.250)	DT 9.286 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.922 (33.922)	mem 59.073
Train: [4][691/750]	BT 0.155 (1.248)	DT 0.008 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.359 (28.359)	mem 59.102
Train: [4][692/750]	BT 2.539 (1.250)	DT 2.352 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.391 (32.391)	mem 59.857
Train: [4][693/750]	BT 0.224 (1.249)	DT 0.005 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.031 (30.031)	mem 59.980
Train: [4][694/750]	BT 0.137 (1.247)	DT 0.015 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.141 (32.141)	mem 59.819
Train: [4][695/750]	BT 0.081 (1.245)	DT 0.003 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.406 (32.406)	mem 59.920
Train: [4][696/750]	BT 0.094 (1.244)	DT 0.003 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.922 (33.922)	mem 59.935
Train: [4][697/750]	BT 0.092 (1.242)	DT 0.003 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.969 (30.969)	mem 59.832
Train: [4][698/750]	BT 0.093 (1.240)	DT 0.003 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.562 (31.562)	mem 59.849
Train: [4][699/750]	BT 0.177 (1.239)	DT 0.022 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 44.031 (44.031)	mem 59.922
Train: [4][700/750]	BT 0.252 (1.238)	DT 0.014 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.391 (30.391)	mem 59.919
Train: [4][701/750]	BT 0.140 (1.236)	DT 0.025 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.797 (29.797)	mem 59.949
Train: [4][702/750]	BT 11.740 (1.251)	DT 11.642 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.172 (38.172)	mem 62.350
Train: [4][703/750]	BT 0.125 (1.249)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.734 (27.734)	mem 62.361
Train: [4][704/750]	BT 0.218 (1.248)	DT 0.008 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.219 (33.219)	mem 62.428
Train: [4][705/750]	BT 0.194 (1.246)	DT 0.023 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 25.328 (25.328)	mem 62.388
Train: [4][706/750]	BT 1.240 (1.246)	DT 1.101 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.703 (39.703)	mem 62.578
Train: [4][707/750]	BT 0.152 (1.245)	DT 0.003 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.219 (33.219)	mem 62.613
Train: [4][708/750]	BT 0.136 (1.243)	DT 0.011 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.422 (31.422)	mem 62.688
Train: [4][709/750]	BT 0.159 (1.242)	DT 0.018 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.516 (29.516)	mem 62.769
Train: [4][710/750]	BT 0.118 (1.240)	DT 0.003 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.031 (34.031)	mem 62.676
Train: [4][711/750]	BT 0.244 (1.239)	DT 0.060 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.188 (30.188)	mem 62.716
Train: [4][712/750]	BT 0.190 (1.237)	DT 0.015 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.453 (34.453)	mem 62.746
Train: [4][713/750]	BT 0.124 (1.236)	DT 0.002 (1.095)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.281 (29.281)	mem 62.770
Train: [4][714/750]	BT 11.037 (1.249)	DT 10.901 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.750 (30.750)	mem 64.686
Train: [4][715/750]	BT 0.144 (1.248)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.109 (34.109)	mem 64.716
Train: [4][716/750]	BT 1.062 (1.248)	DT 0.965 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.094 (35.094)	mem 64.719
Train: [4][717/750]	BT 0.096 (1.246)	DT 0.008 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.250 (27.250)	mem 64.733
Train: [4][718/750]	BT 2.217 (1.247)	DT 2.135 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.062 (34.062)	mem 64.968
Train: [4][719/750]	BT 0.107 (1.246)	DT 0.001 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.391 (31.391)	mem 64.977
Train: [4][720/750]	BT 0.144 (1.244)	DT 0.002 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.094 (31.094)	mem 65.095
Train: [4][721/750]	BT 0.143 (1.243)	DT 0.013 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.156 (34.156)	mem 64.976
Train: [4][722/750]	BT 0.136 (1.241)	DT 0.001 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.438 (36.438)	mem 65.119
Train: [4][723/750]	BT 0.206 (1.240)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.844 (36.844)	mem 65.076
Train: [4][724/750]	BT 0.149 (1.238)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.078 (32.078)	mem 65.075
Train: [4][725/750]	BT 0.147 (1.237)	DT 0.002 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.625 (30.625)	mem 65.001
Train: [4][726/750]	BT 9.080 (1.248)	DT 8.877 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.094 (34.094)	mem 64.873
Train: [4][727/750]	BT 0.248 (1.246)	DT 0.013 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.281 (33.281)	mem 64.834
Train: [4][728/750]	BT 1.296 (1.246)	DT 1.141 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.719 (27.719)	mem 64.839
Train: [4][729/750]	BT 0.130 (1.245)	DT 0.002 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.641 (30.641)	mem 64.770
Train: [4][730/750]	BT 4.313 (1.249)	DT 4.090 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.719 (31.719)	mem 64.822
Train: [4][731/750]	BT 0.141 (1.247)	DT 0.006 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.922 (33.922)	mem 64.725
Train: [4][732/750]	BT 0.227 (1.246)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.859 (37.859)	mem 64.756
Train: [4][733/750]	BT 0.149 (1.244)	DT 0.013 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.188 (31.188)	mem 64.755
Train: [4][734/750]	BT 0.179 (1.243)	DT 0.002 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.375 (31.375)	mem 64.814
Train: [4][735/750]	BT 0.182 (1.242)	DT 0.002 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.047 (30.047)	mem 64.711
Train: [4][736/750]	BT 0.161 (1.240)	DT 0.002 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.406 (29.406)	mem 64.711
Train: [4][737/750]	BT 0.196 (1.239)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.281 (34.281)	mem 64.744
Train: [4][738/750]	BT 7.266 (1.247)	DT 7.103 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.391 (35.391)	mem 41.664
Train: [4][739/750]	BT 0.138 (1.245)	DT 0.003 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.484 (28.484)	mem 41.678
Train: [4][740/750]	BT 0.183 (1.244)	DT 0.003 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.109 (35.109)	mem 41.734
Train: [4][741/750]	BT 0.118 (1.242)	DT 0.008 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 48.203 (48.203)	mem 41.733
Train: [4][742/750]	BT 0.836 (1.242)	DT 0.723 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.000 (32.000)	mem 38.724
Train: [4][743/750]	BT 0.103 (1.240)	DT 0.001 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.703 (32.703)	mem 38.702
Train: [4][744/750]	BT 0.084 (1.239)	DT 0.003 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.141 (34.141)	mem 38.700
Train: [4][745/750]	BT 0.095 (1.237)	DT 0.003 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.094 (32.094)	mem 38.700
Train: [4][746/750]	BT 0.103 (1.236)	DT 0.002 (1.095)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.344 (33.344)	mem 38.700
Train: [4][747/750]	BT 0.146 (1.234)	DT 0.002 (1.094)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.344 (36.344)	mem 38.723
Train: [4][748/750]	BT 0.078 (1.233)	DT 0.003 (1.092)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.219 (33.219)	mem 38.723
Train: [4][749/750]	BT 0.143 (1.231)	DT 0.001 (1.091)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 22.844 (22.844)	mem 38.725
Train: [4][750/750]	BT 2.278 (1.233)	DT 2.172 (1.092)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.688 (27.688)	mem 35.701
Train: [4][751/750]	BT 0.066 (1.231)	DT 0.001 (1.091)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.094 (32.094)	mem 35.702
Train: [4][752/750]	BT 0.063 (1.230)	DT 0.001 (1.089)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.875 (32.875)	mem 35.702
Train: [4][753/750]	BT 0.065 (1.228)	DT 0.001 (1.088)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.750 (29.750)	mem 35.702
Train: [4][754/750]	BT 1.411 (1.228)	DT 1.309 (1.088)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.188 (37.188)	mem 32.702
Train: [4][755/750]	BT 0.088 (1.227)	DT 0.002 (1.087)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.250 (28.250)	mem 29.989
Train: [4][756/750]	BT 0.100 (1.225)	DT 0.002 (1.085)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.656 (38.656)	mem 27.844
epoch 4, total time 926.52
==> Saving...
==> Saving...
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [5][1/750]	BT 20.530 (20.530)	DT 20.351 (20.351)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.453 (28.453)	mem 63.233
Train: [5][2/750]	BT 2.333 (11.432)	DT 2.071 (11.211)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.141 (35.141)	mem 63.333
Train: [5][3/750]	BT 0.554 (7.806)	DT 0.159 (7.527)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.406 (35.406)	mem 63.407
Train: [5][4/750]	BT 0.148 (5.891)	DT 0.009 (5.647)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.875 (29.875)	mem 63.351
Train: [5][5/750]	BT 0.183 (4.750)	DT 0.002 (4.518)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.859 (26.859)	mem 63.355
Train: [5][6/750]	BT 0.236 (3.997)	DT 0.013 (3.767)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.828 (34.828)	mem 63.498
Train: [5][7/750]	BT 1.093 (3.582)	DT 0.905 (3.358)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.172 (33.172)	mem 63.412
Train: [5][8/750]	BT 0.183 (3.158)	DT 0.022 (2.941)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.141 (32.141)	mem 63.376
Train: [5][9/750]	BT 1.710 (2.997)	DT 1.615 (2.794)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.141 (34.141)	mem 58.544
Train: [5][10/750]	BT 0.222 (2.719)	DT 0.004 (2.515)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.344 (35.344)	mem 58.723
Train: [5][11/750]	BT 0.097 (2.481)	DT 0.008 (2.287)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.656 (32.656)	mem 58.628
Train: [5][12/750]	BT 0.096 (2.282)	DT 0.002 (2.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.000 (30.000)	mem 58.197
Train: [5][13/750]	BT 8.630 (2.770)	DT 8.393 (2.581)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.750 (37.750)	mem 59.630
Train: [5][14/750]	BT 1.156 (2.655)	DT 1.040 (2.471)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.656 (32.656)	mem 60.022
Train: [5][15/750]	BT 0.218 (2.493)	DT 0.002 (2.306)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.547 (27.547)	mem 60.127
Train: [5][16/750]	BT 0.121 (2.344)	DT 0.007 (2.163)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.422 (34.422)	mem 60.002
Train: [5][17/750]	BT 0.170 (2.217)	DT 0.002 (2.035)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.859 (32.859)	mem 60.070
Train: [5][18/750]	BT 0.185 (2.104)	DT 0.003 (1.923)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.797 (34.797)	mem 60.060
Train: [5][19/750]	BT 0.176 (2.002)	DT 0.076 (1.825)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.047 (30.047)	mem 60.029
Train: [5][20/750]	BT 0.140 (1.909)	DT 0.002 (1.734)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.203 (35.203)	mem 60.083
Train: [5][21/750]	BT 0.985 (1.865)	DT 0.902 (1.695)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.047 (28.047)	mem 60.205
Train: [5][22/750]	BT 2.711 (1.904)	DT 2.492 (1.731)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.875 (29.875)	mem 60.808
Train: [5][23/750]	BT 0.206 (1.830)	DT 0.006 (1.656)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.328 (28.328)	mem 60.862
Train: [5][24/750]	BT 5.123 (1.967)	DT 5.035 (1.797)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.547 (37.547)	mem 61.830
Train: [5][25/750]	BT 0.093 (1.892)	DT 0.002 (1.725)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.453 (36.453)	mem 61.809
Train: [5][26/750]	BT 2.219 (1.905)	DT 1.949 (1.733)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.766 (35.766)	mem 62.118
Train: [5][27/750]	BT 0.171 (1.840)	DT 0.035 (1.670)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.328 (33.328)	mem 62.125
Train: [5][28/750]	BT 3.565 (1.902)	DT 3.450 (1.734)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.844 (34.844)	mem 62.788
Train: [5][29/750]	BT 0.191 (1.843)	DT 0.002 (1.674)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.984 (30.984)	mem 62.816
Train: [5][30/750]	BT 0.961 (1.814)	DT 0.779 (1.644)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.844 (33.844)	mem 63.106
Train: [5][31/750]	BT 0.110 (1.759)	DT 0.005 (1.592)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.953 (31.953)	mem 63.226
Train: [5][32/750]	BT 0.376 (1.715)	DT 0.037 (1.543)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.375 (37.375)	mem 63.211
Train: [5][33/750]	BT 0.150 (1.668)	DT 0.002 (1.496)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.234 (28.234)	mem 63.261
Train: [5][34/750]	BT 3.808 (1.731)	DT 3.707 (1.561)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.688 (34.688)	mem 63.974
Train: [5][35/750]	BT 0.118 (1.685)	DT 0.006 (1.517)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.859 (33.859)	mem 64.036
Train: [5][36/750]	BT 4.354 (1.759)	DT 4.213 (1.592)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.750 (36.750)	mem 64.594
Train: [5][37/750]	BT 0.091 (1.714)	DT 0.010 (1.549)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.047 (31.047)	mem 64.564
Train: [5][38/750]	BT 1.475 (1.708)	DT 1.300 (1.542)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.453 (34.453)	mem 64.568
Train: [5][39/750]	BT 0.120 (1.667)	DT 0.002 (1.503)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.938 (29.938)	mem 64.569
Train: [5][40/750]	BT 6.964 (1.799)	DT 6.666 (1.632)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.016 (38.016)	mem 64.802
Train: [5][41/750]	BT 0.100 (1.758)	DT 0.002 (1.592)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.281 (29.281)	mem 64.812
Train: [5][42/750]	BT 0.130 (1.719)	DT 0.003 (1.554)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.172 (30.172)	mem 64.764
Train: [5][43/750]	BT 0.222 (1.684)	DT 0.017 (1.519)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.594 (33.594)	mem 64.657
Train: [5][44/750]	BT 0.180 (1.650)	DT 0.019 (1.485)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.812 (34.812)	mem 64.730
Train: [5][45/750]	BT 0.236 (1.619)	DT 0.012 (1.452)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.516 (32.516)	mem 64.656
Train: [5][46/750]	BT 0.696 (1.599)	DT 0.497 (1.431)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.891 (35.891)	mem 64.657
Train: [5][47/750]	BT 0.164 (1.568)	DT 0.003 (1.401)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.141 (30.141)	mem 64.655
Train: [5][48/750]	BT 7.320 (1.688)	DT 7.050 (1.518)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.094 (35.094)	mem 64.643
Train: [5][49/750]	BT 0.240 (1.658)	DT 0.031 (1.488)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.906 (29.906)	mem 64.643
Train: [5][50/750]	BT 0.235 (1.630)	DT 0.013 (1.459)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.109 (33.109)	mem 64.767
Train: [5][51/750]	BT 9.416 (1.783)	DT 9.304 (1.612)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 43.766 (43.766)	mem 64.818
Train: [5][52/750]	BT 0.201 (1.752)	DT 0.002 (1.581)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.094 (36.094)	mem 64.801
Train: [5][53/750]	BT 0.105 (1.721)	DT 0.002 (1.552)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.109 (31.109)	mem 64.749
Train: [5][54/750]	BT 0.160 (1.692)	DT 0.002 (1.523)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.766 (33.766)	mem 64.673
Train: [5][55/750]	BT 0.152 (1.664)	DT 0.011 (1.495)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.828 (32.828)	mem 64.727
Train: [5][56/750]	BT 0.133 (1.637)	DT 0.005 (1.469)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.844 (31.844)	mem 64.752
Train: [5][57/750]	BT 0.271 (1.613)	DT 0.050 (1.444)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.250 (29.250)	mem 64.751
Train: [5][58/750]	BT 0.144 (1.588)	DT 0.010 (1.419)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.016 (30.016)	mem 64.674
Train: [5][59/750]	BT 0.094 (1.562)	DT 0.002 (1.395)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.141 (30.141)	mem 64.677
Train: [5][60/750]	BT 4.669 (1.614)	DT 4.507 (1.447)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.188 (35.188)	mem 64.695
Train: [5][61/750]	BT 0.292 (1.592)	DT 0.011 (1.423)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 24.938 (24.938)	mem 64.712
Train: [5][62/750]	BT 0.202 (1.570)	DT 0.002 (1.401)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.984 (37.984)	mem 64.773
Train: [5][63/750]	BT 2.870 (1.591)	DT 2.727 (1.422)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.422 (26.422)	mem 64.705
Train: [5][64/750]	BT 1.116 (1.583)	DT 0.896 (1.413)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.234 (35.234)	mem 64.711
Train: [5][65/750]	BT 0.264 (1.563)	DT 0.002 (1.392)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.750 (33.750)	mem 64.710
Train: [5][66/750]	BT 0.390 (1.545)	DT 0.009 (1.371)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.562 (31.562)	mem 64.710
Train: [5][67/750]	BT 0.235 (1.526)	DT 0.009 (1.350)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.500 (28.500)	mem 64.789
Train: [5][68/750]	BT 2.728 (1.543)	DT 2.559 (1.368)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.062 (34.062)	mem 64.737
Train: [5][69/750]	BT 0.225 (1.524)	DT 0.002 (1.348)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.969 (34.969)	mem 64.737
Train: [5][70/750]	BT 0.476 (1.509)	DT 0.297 (1.333)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.969 (33.969)	mem 64.737
Train: [5][71/750]	BT 0.134 (1.490)	DT 0.001 (1.315)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.812 (32.812)	mem 64.738
Train: [5][72/750]	BT 5.506 (1.546)	DT 5.292 (1.370)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.828 (31.828)	mem 58.042
Train: [5][73/750]	BT 0.093 (1.526)	DT 0.003 (1.351)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.719 (32.719)	mem 58.128
Train: [5][74/750]	BT 0.188 (1.508)	DT 0.003 (1.333)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.328 (33.328)	mem 58.039
Train: [5][75/750]	BT 0.208 (1.490)	DT 0.048 (1.316)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.156 (29.156)	mem 58.149
Train: [5][76/750]	BT 5.915 (1.548)	DT 5.737 (1.374)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.359 (31.359)	mem 59.233
Train: [5][77/750]	BT 0.100 (1.530)	DT 0.014 (1.356)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.141 (30.141)	mem 59.263
Train: [5][78/750]	BT 0.099 (1.511)	DT 0.002 (1.339)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.422 (32.422)	mem 59.278
Train: [5][79/750]	BT 0.121 (1.494)	DT 0.003 (1.322)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.969 (33.969)	mem 59.289
Train: [5][80/750]	BT 0.212 (1.478)	DT 0.089 (1.307)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.656 (31.656)	mem 59.316
Train: [5][81/750]	BT 0.090 (1.461)	DT 0.002 (1.290)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.250 (30.250)	mem 59.355
Train: [5][82/750]	BT 0.132 (1.444)	DT 0.012 (1.275)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.344 (29.344)	mem 59.450
Train: [5][83/750]	BT 0.124 (1.428)	DT 0.003 (1.260)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.859 (33.859)	mem 59.477
Train: [5][84/750]	BT 7.676 (1.503)	DT 7.544 (1.334)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.297 (31.297)	mem 60.793
Train: [5][85/750]	BT 0.156 (1.487)	DT 0.005 (1.319)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.406 (31.406)	mem 61.005
Train: [5][86/750]	BT 0.188 (1.472)	DT 0.031 (1.304)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.375 (34.375)	mem 60.927
Train: [5][87/750]	BT 0.139 (1.457)	DT 0.013 (1.289)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.719 (26.719)	mem 60.930
Train: [5][88/750]	BT 4.769 (1.494)	DT 4.576 (1.326)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.688 (33.688)	mem 61.747
Train: [5][89/750]	BT 0.242 (1.480)	DT 0.008 (1.311)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.344 (28.344)	mem 61.802
Train: [5][90/750]	BT 0.100 (1.465)	DT 0.006 (1.297)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.250 (38.250)	mem 61.875
Train: [5][91/750]	BT 0.210 (1.451)	DT 0.003 (1.283)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.875 (33.875)	mem 61.911
Train: [5][92/750]	BT 0.226 (1.438)	DT 0.006 (1.269)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.500 (35.500)	mem 61.835
Train: [5][93/750]	BT 0.189 (1.424)	DT 0.002 (1.255)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.812 (30.812)	mem 61.859
Train: [5][94/750]	BT 0.339 (1.413)	DT 0.160 (1.244)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.422 (37.422)	mem 61.987
Train: [5][95/750]	BT 0.172 (1.400)	DT 0.002 (1.231)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.703 (32.703)	mem 62.028
Train: [5][96/750]	BT 10.704 (1.497)	DT 10.568 (1.328)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.859 (38.859)	mem 63.901
Train: [5][97/750]	BT 0.137 (1.483)	DT 0.009 (1.314)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.141 (31.141)	mem 63.981
Train: [5][98/750]	BT 0.121 (1.469)	DT 0.003 (1.301)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.297 (33.297)	mem 63.928
Train: [5][99/750]	BT 0.118 (1.455)	DT 0.002 (1.288)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.391 (31.391)	mem 63.981
Train: [5][100/750]	BT 2.180 (1.462)	DT 2.041 (1.295)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.109 (40.109)	mem 63.855
Train: [5][101/750]	BT 0.170 (1.449)	DT 0.011 (1.282)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.547 (26.547)	mem 63.862
Train: [5][102/750]	BT 0.177 (1.437)	DT 0.002 (1.270)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.016 (35.016)	mem 63.902
Train: [5][103/750]	BT 0.202 (1.425)	DT 0.026 (1.258)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.641 (33.641)	mem 63.966
Train: [5][104/750]	BT 0.188 (1.413)	DT 0.007 (1.246)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.047 (34.047)	mem 63.910
Train: [5][105/750]	BT 0.206 (1.402)	DT 0.002 (1.234)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.656 (30.656)	mem 64.001
Train: [5][106/750]	BT 0.278 (1.391)	DT 0.016 (1.222)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.297 (36.297)	mem 63.530
Train: [5][107/750]	BT 0.364 (1.381)	DT 0.003 (1.211)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.250 (33.250)	mem 62.143
Train: [5][108/750]	BT 8.544 (1.448)	DT 8.424 (1.278)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.250 (35.250)	mem 59.430
Train: [5][109/750]	BT 0.131 (1.436)	DT 0.002 (1.266)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.062 (33.062)	mem 59.515
Train: [5][110/750]	BT 0.091 (1.423)	DT 0.003 (1.255)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.312 (34.312)	mem 59.584
Train: [5][111/750]	BT 0.242 (1.413)	DT 0.005 (1.243)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.391 (28.391)	mem 59.599
Train: [5][112/750]	BT 5.249 (1.447)	DT 5.149 (1.278)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.297 (35.297)	mem 60.875
Train: [5][113/750]	BT 0.175 (1.436)	DT 0.003 (1.267)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.141 (33.141)	mem 60.795
Train: [5][114/750]	BT 0.618 (1.429)	DT 0.403 (1.259)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.672 (37.672)	mem 60.875
Train: [5][115/750]	BT 0.401 (1.420)	DT 0.051 (1.249)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.984 (30.984)	mem 60.950
Train: [5][116/750]	BT 0.208 (1.409)	DT 0.007 (1.238)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.062 (31.062)	mem 60.966
Train: [5][117/750]	BT 0.118 (1.398)	DT 0.002 (1.228)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.156 (33.156)	mem 60.980
Train: [5][118/750]	BT 0.174 (1.388)	DT 0.002 (1.217)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.875 (32.875)	mem 61.006
Train: [5][119/750]	BT 0.149 (1.377)	DT 0.011 (1.207)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.391 (28.391)	mem 61.030
Train: [5][120/750]	BT 6.105 (1.417)	DT 5.894 (1.246)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.422 (40.422)	mem 62.698
Train: [5][121/750]	BT 0.142 (1.406)	DT 0.008 (1.236)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.141 (27.141)	mem 62.766
Train: [5][122/750]	BT 0.236 (1.397)	DT 0.012 (1.226)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.828 (33.828)	mem 62.813
Train: [5][123/750]	BT 0.225 (1.387)	DT 0.002 (1.216)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.922 (29.922)	mem 62.864
Train: [5][124/750]	BT 8.860 (1.447)	DT 8.727 (1.277)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.422 (30.422)	mem 64.280
Train: [5][125/750]	BT 0.216 (1.438)	DT 0.002 (1.266)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.875 (30.875)	mem 64.444
Train: [5][126/750]	BT 0.182 (1.428)	DT 0.003 (1.256)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.891 (32.891)	mem 64.333
Train: [5][127/750]	BT 0.112 (1.417)	DT 0.002 (1.246)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.562 (31.562)	mem 64.354
Train: [5][128/750]	BT 0.109 (1.407)	DT 0.002 (1.237)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.375 (34.375)	mem 64.377
Train: [5][129/750]	BT 0.128 (1.397)	DT 0.002 (1.227)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.406 (36.406)	mem 64.398
Train: [5][130/750]	BT 0.214 (1.388)	DT 0.015 (1.218)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.375 (37.375)	mem 64.422
Train: [5][131/750]	BT 0.109 (1.378)	DT 0.009 (1.209)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.391 (32.391)	mem 64.434
Train: [5][132/750]	BT 2.161 (1.384)	DT 2.066 (1.215)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.344 (36.344)	mem 64.876
Train: [5][133/750]	BT 0.144 (1.375)	DT 0.012 (1.206)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.219 (32.219)	mem 65.319
Train: [5][134/750]	BT 0.125 (1.366)	DT 0.007 (1.197)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.859 (34.859)	mem 65.473
Train: [5][135/750]	BT 0.150 (1.357)	DT 0.003 (1.188)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.547 (27.547)	mem 65.370
Train: [5][136/750]	BT 9.760 (1.418)	DT 9.595 (1.250)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.578 (33.578)	mem 64.941
Train: [5][137/750]	BT 0.184 (1.409)	DT 0.002 (1.241)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.953 (29.953)	mem 64.949
Train: [5][138/750]	BT 0.773 (1.405)	DT 0.659 (1.237)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.859 (32.859)	mem 64.807
Train: [5][139/750]	BT 0.118 (1.395)	DT 0.002 (1.228)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.531 (31.531)	mem 64.808
Train: [5][140/750]	BT 0.112 (1.386)	DT 0.002 (1.219)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.812 (30.812)	mem 64.809
Train: [5][141/750]	BT 0.133 (1.377)	DT 0.001 (1.210)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.297 (33.297)	mem 64.835
Train: [5][142/750]	BT 0.190 (1.369)	DT 0.020 (1.202)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.828 (31.828)	mem 65.098
Train: [5][143/750]	BT 0.172 (1.361)	DT 0.011 (1.194)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.438 (28.438)	mem 65.073
Train: [5][144/750]	BT 0.105 (1.352)	DT 0.003 (1.185)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.266 (33.266)	mem 65.073
Train: [5][145/750]	BT 0.094 (1.343)	DT 0.001 (1.177)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.406 (35.406)	mem 64.890
Train: [5][146/750]	BT 0.162 (1.335)	DT 0.002 (1.169)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.781 (32.781)	mem 64.891
Train: [5][147/750]	BT 0.099 (1.327)	DT 0.001 (1.161)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.969 (30.969)	mem 64.977
Train: [5][148/750]	BT 12.043 (1.399)	DT 11.900 (1.234)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.422 (32.422)	mem 64.993
Train: [5][149/750]	BT 0.097 (1.390)	DT 0.002 (1.226)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.109 (29.109)	mem 64.995
Train: [5][150/750]	BT 1.912 (1.394)	DT 1.678 (1.229)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.656 (34.656)	mem 64.908
Train: [5][151/750]	BT 0.225 (1.386)	DT 0.002 (1.221)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.828 (32.828)	mem 64.911
Train: [5][152/750]	BT 0.101 (1.378)	DT 0.002 (1.213)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.828 (30.828)	mem 64.912
Train: [5][153/750]	BT 0.087 (1.369)	DT 0.002 (1.205)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.812 (27.812)	mem 64.912
Train: [5][154/750]	BT 0.106 (1.361)	DT 0.002 (1.197)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.828 (31.828)	mem 64.929
Train: [5][155/750]	BT 0.178 (1.353)	DT 0.002 (1.189)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.125 (31.125)	mem 65.119
Train: [5][156/750]	BT 0.262 (1.346)	DT 0.044 (1.182)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.406 (32.406)	mem 64.999
Train: [5][157/750]	BT 0.094 (1.338)	DT 0.002 (1.174)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.672 (35.672)	mem 64.942
Train: [5][158/750]	BT 0.132 (1.331)	DT 0.002 (1.167)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.031 (31.031)	mem 64.943
Train: [5][159/750]	BT 0.186 (1.324)	DT 0.022 (1.160)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.719 (30.719)	mem 65.109
Train: [5][160/750]	BT 9.544 (1.375)	DT 9.360 (1.211)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.438 (36.438)	mem 65.053
Train: [5][161/750]	BT 0.111 (1.367)	DT 0.002 (1.203)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.984 (28.984)	mem 65.107
Train: [5][162/750]	BT 2.491 (1.374)	DT 2.321 (1.210)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.594 (28.594)	mem 65.007
Train: [5][163/750]	BT 0.193 (1.367)	DT 0.003 (1.203)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.547 (32.547)	mem 65.007
Train: [5][164/750]	BT 0.199 (1.360)	DT 0.002 (1.196)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.844 (30.844)	mem 65.049
Train: [5][165/750]	BT 0.161 (1.352)	DT 0.010 (1.188)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.094 (32.094)	mem 65.014
Train: [5][166/750]	BT 0.121 (1.345)	DT 0.001 (1.181)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.703 (30.703)	mem 65.016
Train: [5][167/750]	BT 0.139 (1.338)	DT 0.002 (1.174)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.734 (33.734)	mem 65.209
Train: [5][168/750]	BT 0.225 (1.331)	DT 0.007 (1.167)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.344 (35.344)	mem 65.104
Train: [5][169/750]	BT 0.162 (1.324)	DT 0.002 (1.160)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.609 (29.609)	mem 65.199
Train: [5][170/750]	BT 0.214 (1.318)	DT 0.009 (1.153)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.891 (35.891)	mem 65.143
Train: [5][171/750]	BT 0.179 (1.311)	DT 0.039 (1.147)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.938 (33.938)	mem 65.022
Train: [5][172/750]	BT 14.171 (1.386)	DT 14.025 (1.222)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.969 (35.969)	mem 60.285
Train: [5][173/750]	BT 0.146 (1.379)	DT 0.002 (1.215)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.297 (29.297)	mem 60.381
Train: [5][174/750]	BT 1.460 (1.379)	DT 1.374 (1.216)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.594 (35.594)	mem 60.673
Train: [5][175/750]	BT 0.175 (1.372)	DT 0.002 (1.209)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.812 (37.812)	mem 60.689
Train: [5][176/750]	BT 0.084 (1.365)	DT 0.003 (1.202)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.797 (34.797)	mem 60.711
Train: [5][177/750]	BT 0.125 (1.358)	DT 0.014 (1.195)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.797 (33.797)	mem 60.733
Train: [5][178/750]	BT 0.093 (1.351)	DT 0.005 (1.189)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.344 (34.344)	mem 60.780
Train: [5][179/750]	BT 0.157 (1.344)	DT 0.003 (1.182)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.078 (31.078)	mem 60.792
Train: [5][180/750]	BT 0.099 (1.337)	DT 0.002 (1.175)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.578 (33.578)	mem 60.808
Train: [5][181/750]	BT 0.165 (1.331)	DT 0.002 (1.169)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.000 (33.000)	mem 60.834
Train: [5][182/750]	BT 0.130 (1.324)	DT 0.010 (1.162)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.609 (30.609)	mem 60.851
Train: [5][183/750]	BT 0.196 (1.318)	DT 0.006 (1.156)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.750 (29.750)	mem 60.875
Train: [5][184/750]	BT 11.531 (1.374)	DT 11.424 (1.212)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.766 (35.766)	mem 63.131
Train: [5][185/750]	BT 0.147 (1.367)	DT 0.001 (1.205)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.922 (29.922)	mem 63.174
Train: [5][186/750]	BT 0.113 (1.360)	DT 0.008 (1.199)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.453 (32.453)	mem 63.219
Train: [5][187/750]	BT 0.138 (1.354)	DT 0.002 (1.193)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.531 (33.531)	mem 63.247
Train: [5][188/750]	BT 0.097 (1.347)	DT 0.002 (1.186)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.641 (33.641)	mem 63.277
Train: [5][189/750]	BT 0.152 (1.341)	DT 0.001 (1.180)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.812 (32.812)	mem 63.315
Train: [5][190/750]	BT 0.098 (1.334)	DT 0.003 (1.174)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.422 (31.422)	mem 63.334
Train: [5][191/750]	BT 0.265 (1.328)	DT 0.002 (1.168)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.688 (30.688)	mem 63.365
Train: [5][192/750]	BT 0.111 (1.322)	DT 0.001 (1.162)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.250 (30.250)	mem 63.418
Train: [5][193/750]	BT 0.298 (1.317)	DT 0.002 (1.156)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.828 (31.828)	mem 63.440
Train: [5][194/750]	BT 0.259 (1.311)	DT 0.007 (1.150)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 63.574
Train: [5][195/750]	BT 0.109 (1.305)	DT 0.020 (1.144)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.594 (28.594)	mem 63.474
Train: [5][196/750]	BT 16.854 (1.385)	DT 16.756 (1.224)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.266 (36.266)	mem 59.976
Train: [5][197/750]	BT 0.140 (1.378)	DT 0.006 (1.217)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.094 (30.094)	mem 59.999
Train: [5][198/750]	BT 0.105 (1.372)	DT 0.003 (1.211)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.281 (36.281)	mem 59.964
Train: [5][199/750]	BT 0.107 (1.365)	DT 0.002 (1.205)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.484 (28.484)	mem 60.016
Train: [5][200/750]	BT 0.101 (1.359)	DT 0.002 (1.199)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.547 (33.547)	mem 60.074
Train: [5][201/750]	BT 0.089 (1.353)	DT 0.002 (1.193)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.250 (28.250)	mem 60.186
Train: [5][202/750]	BT 0.118 (1.347)	DT 0.002 (1.187)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.578 (34.578)	mem 60.227
Train: [5][203/750]	BT 0.214 (1.341)	DT 0.026 (1.182)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.531 (31.531)	mem 60.276
Train: [5][204/750]	BT 0.187 (1.335)	DT 0.003 (1.176)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.359 (33.359)	mem 60.322
Train: [5][205/750]	BT 0.215 (1.330)	DT 0.007 (1.170)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.547 (31.547)	mem 60.498
Train: [5][206/750]	BT 0.128 (1.324)	DT 0.006 (1.164)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.328 (34.328)	mem 60.413
Train: [5][207/750]	BT 0.129 (1.318)	DT 0.003 (1.159)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.562 (33.562)	mem 60.455
Train: [5][208/750]	BT 13.785 (1.378)	DT 13.605 (1.219)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.719 (36.719)	mem 63.062
Train: [5][209/750]	BT 0.102 (1.372)	DT 0.003 (1.213)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.438 (29.438)	mem 63.123
Train: [5][210/750]	BT 0.093 (1.366)	DT 0.002 (1.207)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.000 (35.000)	mem 63.226
Train: [5][211/750]	BT 0.179 (1.360)	DT 0.002 (1.201)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.719 (29.719)	mem 63.291
Train: [5][212/750]	BT 0.151 (1.355)	DT 0.002 (1.196)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.016 (30.016)	mem 63.188
Train: [5][213/750]	BT 0.131 (1.349)	DT 0.001 (1.190)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.719 (31.719)	mem 63.227
Train: [5][214/750]	BT 0.097 (1.343)	DT 0.010 (1.185)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.344 (29.344)	mem 63.304
Train: [5][215/750]	BT 0.110 (1.337)	DT 0.002 (1.179)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.078 (34.078)	mem 63.389
Train: [5][216/750]	BT 0.108 (1.332)	DT 0.001 (1.174)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.344 (37.344)	mem 63.439
Train: [5][217/750]	BT 0.120 (1.326)	DT 0.010 (1.168)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.422 (33.422)	mem 63.542
Train: [5][218/750]	BT 0.115 (1.321)	DT 0.001 (1.163)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.188 (34.188)	mem 63.607
Train: [5][219/750]	BT 0.172 (1.315)	DT 0.008 (1.158)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.625 (31.625)	mem 63.614
Train: [5][220/750]	BT 11.763 (1.363)	DT 11.571 (1.205)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.484 (31.484)	mem 65.016
Train: [5][221/750]	BT 0.178 (1.358)	DT 0.008 (1.200)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.375 (35.375)	mem 65.018
Train: [5][222/750]	BT 0.157 (1.352)	DT 0.011 (1.194)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.688 (34.688)	mem 65.019
Train: [5][223/750]	BT 0.110 (1.347)	DT 0.002 (1.189)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.219 (26.219)	mem 65.057
Train: [5][224/750]	BT 0.102 (1.341)	DT 0.002 (1.184)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.547 (31.547)	mem 65.016
Train: [5][225/750]	BT 0.212 (1.336)	DT 0.002 (1.178)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.531 (34.531)	mem 65.070
Train: [5][226/750]	BT 0.112 (1.331)	DT 0.013 (1.173)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.766 (34.766)	mem 65.070
Train: [5][227/750]	BT 0.103 (1.325)	DT 0.002 (1.168)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.609 (31.609)	mem 65.036
Train: [5][228/750]	BT 0.218 (1.320)	DT 0.002 (1.163)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.438 (34.438)	mem 65.038
Train: [5][229/750]	BT 0.185 (1.315)	DT 0.011 (1.158)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.562 (28.562)	mem 65.040
Train: [5][230/750]	BT 0.152 (1.310)	DT 0.002 (1.153)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.734 (34.734)	mem 65.039
Train: [5][231/750]	BT 0.208 (1.305)	DT 0.002 (1.148)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.734 (36.734)	mem 65.037
Train: [5][232/750]	BT 11.708 (1.350)	DT 11.618 (1.193)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.812 (35.812)	mem 65.014
Train: [5][233/750]	BT 0.134 (1.345)	DT 0.002 (1.188)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.953 (27.953)	mem 65.013
Train: [5][234/750]	BT 0.168 (1.340)	DT 0.001 (1.183)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.047 (37.047)	mem 65.014
Train: [5][235/750]	BT 0.198 (1.335)	DT 0.005 (1.178)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.297 (26.297)	mem 65.036
Train: [5][236/750]	BT 0.105 (1.330)	DT 0.002 (1.173)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.703 (34.703)	mem 65.073
Train: [5][237/750]	BT 0.121 (1.325)	DT 0.002 (1.168)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.203 (30.203)	mem 65.040
Train: [5][238/750]	BT 0.104 (1.320)	DT 0.003 (1.163)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.656 (33.656)	mem 65.040
Train: [5][239/750]	BT 0.143 (1.315)	DT 0.002 (1.158)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.750 (30.750)	mem 65.041
Train: [5][240/750]	BT 0.244 (1.310)	DT 0.005 (1.153)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.672 (31.672)	mem 65.041
Train: [5][241/750]	BT 0.236 (1.306)	DT 0.011 (1.149)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.938 (32.938)	mem 65.041
Train: [5][242/750]	BT 0.176 (1.301)	DT 0.002 (1.144)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.562 (31.562)	mem 65.041
Train: [5][243/750]	BT 0.147 (1.297)	DT 0.009 (1.139)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.922 (32.922)	mem 65.042
Train: [5][244/750]	BT 10.479 (1.334)	DT 10.357 (1.177)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.656 (31.656)	mem 65.113
Train: [5][245/750]	BT 0.208 (1.330)	DT 0.011 (1.172)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.047 (34.047)	mem 65.235
Train: [5][246/750]	BT 0.146 (1.325)	DT 0.003 (1.167)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.812 (31.812)	mem 65.023
Train: [5][247/750]	BT 0.198 (1.320)	DT 0.023 (1.163)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.062 (30.062)	mem 65.024
Train: [5][248/750]	BT 0.213 (1.316)	DT 0.002 (1.158)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.688 (33.688)	mem 65.023
Train: [5][249/750]	BT 0.155 (1.311)	DT 0.012 (1.153)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.812 (27.812)	mem 65.023
Train: [5][250/750]	BT 0.217 (1.307)	DT 0.002 (1.149)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.156 (34.156)	mem 65.023
Train: [5][251/750]	BT 0.140 (1.302)	DT 0.006 (1.144)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.734 (30.734)	mem 65.022
Train: [5][252/750]	BT 0.285 (1.298)	DT 0.011 (1.140)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.219 (33.219)	mem 65.110
Train: [5][253/750]	BT 0.247 (1.294)	DT 0.006 (1.135)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.141 (30.141)	mem 65.025
Train: [5][254/750]	BT 0.128 (1.289)	DT 0.003 (1.131)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.094 (35.094)	mem 65.025
Train: [5][255/750]	BT 0.086 (1.285)	DT 0.002 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.250 (27.250)	mem 65.089
Train: [5][256/750]	BT 9.706 (1.317)	DT 9.579 (1.159)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.375 (37.375)	mem 65.033
Train: [5][257/750]	BT 0.125 (1.313)	DT 0.018 (1.155)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.156 (30.156)	mem 65.101
Train: [5][258/750]	BT 0.287 (1.309)	DT 0.002 (1.151)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.547 (34.547)	mem 65.067
Train: [5][259/750]	BT 0.316 (1.305)	DT 0.007 (1.146)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.109 (30.109)	mem 65.051
Train: [5][260/750]	BT 0.157 (1.301)	DT 0.005 (1.142)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.797 (33.797)	mem 65.052
Train: [5][261/750]	BT 0.143 (1.296)	DT 0.002 (1.137)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.375 (35.375)	mem 65.058
Train: [5][262/750]	BT 0.120 (1.292)	DT 0.013 (1.133)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.250 (32.250)	mem 65.114
Train: [5][263/750]	BT 0.143 (1.287)	DT 0.011 (1.129)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.391 (36.391)	mem 65.180
Train: [5][264/750]	BT 0.180 (1.283)	DT 0.003 (1.125)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.984 (31.984)	mem 65.144
Train: [5][265/750]	BT 0.126 (1.279)	DT 0.006 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.656 (28.656)	mem 65.143
Train: [5][266/750]	BT 0.159 (1.274)	DT 0.003 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.484 (31.484)	mem 65.053
Train: [5][267/750]	BT 0.124 (1.270)	DT 0.003 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.141 (33.141)	mem 65.052
Train: [5][268/750]	BT 16.679 (1.328)	DT 16.567 (1.170)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.328 (35.328)	mem 60.783
Train: [5][269/750]	BT 0.159 (1.323)	DT 0.019 (1.165)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.469 (30.469)	mem 60.818
Train: [5][270/750]	BT 0.132 (1.319)	DT 0.003 (1.161)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.594 (31.594)	mem 60.839
Train: [5][271/750]	BT 0.078 (1.314)	DT 0.002 (1.157)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.141 (34.141)	mem 60.855
Train: [5][272/750]	BT 0.138 (1.310)	DT 0.002 (1.153)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.766 (34.766)	mem 60.881
Train: [5][273/750]	BT 0.082 (1.306)	DT 0.002 (1.148)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.188 (31.188)	mem 61.057
Train: [5][274/750]	BT 0.134 (1.301)	DT 0.001 (1.144)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.531 (36.531)	mem 61.032
Train: [5][275/750]	BT 0.095 (1.297)	DT 0.002 (1.140)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.141 (32.141)	mem 60.943
Train: [5][276/750]	BT 0.097 (1.293)	DT 0.002 (1.136)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.719 (33.719)	mem 61.016
Train: [5][277/750]	BT 0.147 (1.288)	DT 0.003 (1.132)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.688 (28.688)	mem 61.175
Train: [5][278/750]	BT 0.245 (1.285)	DT 0.015 (1.128)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.766 (36.766)	mem 61.087
Train: [5][279/750]	BT 0.102 (1.280)	DT 0.012 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.719 (27.719)	mem 61.058
Train: [5][280/750]	BT 10.917 (1.315)	DT 10.824 (1.158)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.844 (34.844)	mem 63.178
Train: [5][281/750]	BT 0.114 (1.311)	DT 0.002 (1.154)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.516 (29.516)	mem 63.228
Train: [5][282/750]	BT 0.194 (1.307)	DT 0.001 (1.150)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.719 (34.719)	mem 63.396
Train: [5][283/750]	BT 0.134 (1.302)	DT 0.005 (1.146)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.891 (32.891)	mem 63.426
Train: [5][284/750]	BT 0.143 (1.298)	DT 0.002 (1.142)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.109 (31.109)	mem 63.368
Train: [5][285/750]	BT 0.215 (1.295)	DT 0.025 (1.138)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.562 (29.562)	mem 63.459
Train: [5][286/750]	BT 0.221 (1.291)	DT 0.006 (1.134)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.203 (33.203)	mem 63.410
Train: [5][287/750]	BT 0.136 (1.287)	DT 0.007 (1.130)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.000 (31.000)	mem 63.443
Train: [5][288/750]	BT 0.148 (1.283)	DT 0.002 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.547 (36.547)	mem 63.569
Train: [5][289/750]	BT 0.129 (1.279)	DT 0.017 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.391 (32.391)	mem 63.619
Train: [5][290/750]	BT 0.171 (1.275)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.000 (37.000)	mem 63.517
Train: [5][291/750]	BT 0.171 (1.271)	DT 0.005 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.344 (32.344)	mem 63.541
Train: [5][292/750]	BT 9.101 (1.298)	DT 8.988 (1.142)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.625 (29.625)	mem 61.424
Train: [5][293/750]	BT 0.195 (1.294)	DT 0.008 (1.138)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.875 (33.875)	mem 61.577
Train: [5][294/750]	BT 1.820 (1.296)	DT 1.692 (1.140)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.938 (34.938)	mem 60.051
Train: [5][295/750]	BT 0.145 (1.292)	DT 0.017 (1.136)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.234 (34.234)	mem 60.183
Train: [5][296/750]	BT 1.248 (1.292)	DT 1.166 (1.136)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.438 (39.438)	mem 58.820
Train: [5][297/750]	BT 0.133 (1.288)	DT 0.005 (1.132)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.703 (32.703)	mem 59.006
Train: [5][298/750]	BT 0.130 (1.284)	DT 0.015 (1.129)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.594 (33.594)	mem 59.016
Train: [5][299/750]	BT 0.162 (1.280)	DT 0.013 (1.125)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.031 (28.031)	mem 58.939
Train: [5][300/750]	BT 0.157 (1.277)	DT 0.003 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.281 (31.281)	mem 58.992
Train: [5][301/750]	BT 0.259 (1.273)	DT 0.020 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.391 (30.391)	mem 59.120
Train: [5][302/750]	BT 0.102 (1.269)	DT 0.008 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.609 (35.609)	mem 59.219
Train: [5][303/750]	BT 0.129 (1.266)	DT 0.002 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.562 (30.562)	mem 59.228
Train: [5][304/750]	BT 7.744 (1.287)	DT 7.645 (1.132)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.656 (36.656)	mem 60.653
Train: [5][305/750]	BT 0.092 (1.283)	DT 0.003 (1.128)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.453 (30.453)	mem 60.668
Train: [5][306/750]	BT 1.962 (1.285)	DT 1.795 (1.130)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.562 (32.562)	mem 60.960
Train: [5][307/750]	BT 0.228 (1.282)	DT 0.002 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.359 (28.359)	mem 61.010
Train: [5][308/750]	BT 2.276 (1.285)	DT 2.189 (1.130)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.641 (31.641)	mem 61.626
Train: [5][309/750]	BT 0.120 (1.281)	DT 0.003 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.781 (31.781)	mem 61.614
Train: [5][310/750]	BT 0.162 (1.278)	DT 0.001 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.875 (35.875)	mem 61.641
Train: [5][311/750]	BT 0.081 (1.274)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.016 (32.016)	mem 61.668
Train: [5][312/750]	BT 0.140 (1.270)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.844 (31.844)	mem 61.855
Train: [5][313/750]	BT 0.164 (1.267)	DT 0.013 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.891 (30.891)	mem 61.866
Train: [5][314/750]	BT 0.091 (1.263)	DT 0.003 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.016 (29.016)	mem 61.901
Train: [5][315/750]	BT 0.121 (1.259)	DT 0.003 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.984 (29.984)	mem 61.932
Train: [5][316/750]	BT 7.323 (1.278)	DT 7.156 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.344 (34.344)	mem 63.142
Train: [5][317/750]	BT 0.120 (1.275)	DT 0.005 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.625 (32.625)	mem 63.157
Train: [5][318/750]	BT 2.086 (1.277)	DT 1.838 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.328 (34.328)	mem 63.397
Train: [5][319/750]	BT 0.172 (1.274)	DT 0.004 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.734 (29.734)	mem 63.523
Train: [5][320/750]	BT 4.537 (1.284)	DT 4.458 (1.130)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.047 (35.047)	mem 64.620
Train: [5][321/750]	BT 0.083 (1.280)	DT 0.001 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.953 (34.953)	mem 64.647
Train: [5][322/750]	BT 0.164 (1.277)	DT 0.013 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.906 (40.906)	mem 64.682
Train: [5][323/750]	BT 0.094 (1.273)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.594 (27.594)	mem 64.744
Train: [5][324/750]	BT 0.110 (1.270)	DT 0.004 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.938 (33.938)	mem 64.728
Train: [5][325/750]	BT 0.087 (1.266)	DT 0.005 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.172 (30.172)	mem 64.755
Train: [5][326/750]	BT 0.138 (1.263)	DT 0.003 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.125 (33.125)	mem 64.776
Train: [5][327/750]	BT 0.192 (1.259)	DT 0.003 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.531 (35.531)	mem 64.813
Train: [5][328/750]	BT 1.761 (1.261)	DT 1.566 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.391 (35.391)	mem 65.050
Train: [5][329/750]	BT 0.347 (1.258)	DT 0.015 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 45.500 (45.500)	mem 65.052
Train: [5][330/750]	BT 2.870 (1.263)	DT 2.714 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.594 (31.594)	mem 65.136
Train: [5][331/750]	BT 0.221 (1.260)	DT 0.012 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.156 (28.156)	mem 65.217
Train: [5][332/750]	BT 5.829 (1.274)	DT 5.565 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.000 (37.000)	mem 65.231
Train: [5][333/750]	BT 0.147 (1.270)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.234 (30.234)	mem 65.114
Train: [5][334/750]	BT 0.206 (1.267)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.578 (33.578)	mem 65.087
Train: [5][335/750]	BT 0.123 (1.264)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.172 (30.172)	mem 65.087
Train: [5][336/750]	BT 0.124 (1.260)	DT 0.010 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.281 (33.281)	mem 65.087
Train: [5][337/750]	BT 0.292 (1.257)	DT 0.003 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.125 (26.125)	mem 65.179
Train: [5][338/750]	BT 0.234 (1.254)	DT 0.010 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.172 (34.172)	mem 65.127
Train: [5][339/750]	BT 0.185 (1.251)	DT 0.002 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.703 (29.703)	mem 65.127
Train: [5][340/750]	BT 4.509 (1.261)	DT 4.422 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.078 (35.078)	mem 65.115
Train: [5][341/750]	BT 0.114 (1.257)	DT 0.003 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.906 (28.906)	mem 65.074
Train: [5][342/750]	BT 3.339 (1.263)	DT 3.219 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.500 (36.500)	mem 65.094
Train: [5][343/750]	BT 0.264 (1.261)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.375 (29.375)	mem 65.188
Train: [5][344/750]	BT 7.560 (1.279)	DT 7.474 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.297 (38.297)	mem 65.115
Train: [5][345/750]	BT 0.224 (1.276)	DT 0.015 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.094 (32.094)	mem 65.115
Train: [5][346/750]	BT 0.259 (1.273)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.719 (31.719)	mem 65.123
Train: [5][347/750]	BT 0.195 (1.270)	DT 0.011 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.000 (30.000)	mem 65.124
Train: [5][348/750]	BT 0.159 (1.267)	DT 0.001 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.641 (31.641)	mem 64.981
Train: [5][349/750]	BT 0.115 (1.263)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.859 (26.859)	mem 64.913
Train: [5][350/750]	BT 0.092 (1.260)	DT 0.002 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.000 (29.000)	mem 64.917
Train: [5][351/750]	BT 0.223 (1.257)	DT 0.002 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.812 (28.812)	mem 64.938
Train: [5][352/750]	BT 1.233 (1.257)	DT 1.100 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.516 (31.516)	mem 65.026
Train: [5][353/750]	BT 0.268 (1.254)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.438 (28.438)	mem 65.145
Train: [5][354/750]	BT 5.297 (1.265)	DT 5.182 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.625 (34.625)	mem 64.956
Train: [5][355/750]	BT 0.222 (1.263)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.828 (30.828)	mem 64.956
Train: [5][356/750]	BT 5.720 (1.275)	DT 5.502 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.750 (30.750)	mem 64.982
Train: [5][357/750]	BT 0.191 (1.272)	DT 0.009 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.609 (30.609)	mem 65.054
Train: [5][358/750]	BT 0.151 (1.269)	DT 0.002 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.281 (32.281)	mem 65.223
Train: [5][359/750]	BT 0.117 (1.266)	DT 0.002 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.219 (33.219)	mem 65.161
Train: [5][360/750]	BT 0.121 (1.263)	DT 0.003 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.594 (35.594)	mem 65.171
Train: [5][361/750]	BT 0.196 (1.260)	DT 0.014 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.750 (30.750)	mem 65.114
Train: [5][362/750]	BT 0.207 (1.257)	DT 0.004 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.562 (29.562)	mem 64.981
Train: [5][363/750]	BT 0.121 (1.254)	DT 0.008 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.484 (28.484)	mem 64.983
Train: [5][364/750]	BT 2.584 (1.257)	DT 2.418 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.781 (38.781)	mem 65.015
Train: [5][365/750]	BT 0.180 (1.254)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 65.239
Train: [5][366/750]	BT 5.468 (1.266)	DT 5.342 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.016 (33.016)	mem 58.801
Train: [5][367/750]	BT 0.217 (1.263)	DT 0.009 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.109 (35.109)	mem 58.706
Train: [5][368/750]	BT 6.444 (1.277)	DT 6.295 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.875 (35.875)	mem 59.865
Train: [5][369/750]	BT 0.135 (1.274)	DT 0.002 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.016 (32.016)	mem 59.905
Train: [5][370/750]	BT 0.131 (1.271)	DT 0.001 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.781 (35.781)	mem 60.027
Train: [5][371/750]	BT 0.148 (1.268)	DT 0.005 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.484 (39.484)	mem 59.990
Train: [5][372/750]	BT 0.113 (1.265)	DT 0.012 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.625 (36.625)	mem 60.021
Train: [5][373/750]	BT 0.094 (1.262)	DT 0.003 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.406 (33.406)	mem 60.036
Train: [5][374/750]	BT 0.090 (1.258)	DT 0.003 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.609 (33.609)	mem 60.063
Train: [5][375/750]	BT 0.142 (1.255)	DT 0.003 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.672 (31.672)	mem 60.098
Train: [5][376/750]	BT 1.791 (1.257)	DT 1.681 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.703 (33.703)	mem 60.367
Train: [5][377/750]	BT 0.091 (1.254)	DT 0.002 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.047 (29.047)	mem 60.379
Train: [5][378/750]	BT 6.337 (1.267)	DT 6.150 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.734 (31.734)	mem 61.301
Train: [5][379/750]	BT 0.260 (1.265)	DT 0.013 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.109 (29.109)	mem 61.335
Train: [5][380/750]	BT 5.600 (1.276)	DT 5.496 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.312 (32.312)	mem 62.330
Train: [5][381/750]	BT 0.157 (1.273)	DT 0.005 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.984 (31.984)	mem 62.392
Train: [5][382/750]	BT 0.235 (1.270)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.469 (35.469)	mem 62.457
Train: [5][383/750]	BT 0.118 (1.267)	DT 0.014 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.000 (31.000)	mem 62.427
Train: [5][384/750]	BT 0.121 (1.264)	DT 0.005 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.578 (29.578)	mem 62.443
Train: [5][385/750]	BT 0.089 (1.261)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.188 (30.188)	mem 62.455
Train: [5][386/750]	BT 0.086 (1.258)	DT 0.002 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.250 (36.250)	mem 62.484
Train: [5][387/750]	BT 0.118 (1.255)	DT 0.015 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.422 (27.422)	mem 62.556
Train: [5][388/750]	BT 1.564 (1.256)	DT 1.451 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.438 (31.438)	mem 62.850
Train: [5][389/750]	BT 0.191 (1.253)	DT 0.002 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.109 (34.109)	mem 63.034
Train: [5][390/750]	BT 6.729 (1.267)	DT 6.637 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.531 (35.531)	mem 63.991
Train: [5][391/750]	BT 0.156 (1.264)	DT 0.002 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.281 (27.281)	mem 64.152
Train: [5][392/750]	BT 3.679 (1.271)	DT 3.534 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.203 (31.203)	mem 64.274
Train: [5][393/750]	BT 0.163 (1.268)	DT 0.010 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.734 (33.734)	mem 64.253
Train: [5][394/750]	BT 0.273 (1.265)	DT 0.016 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.141 (33.141)	mem 64.215
Train: [5][395/750]	BT 0.193 (1.263)	DT 0.011 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.516 (36.516)	mem 64.216
Train: [5][396/750]	BT 0.215 (1.260)	DT 0.051 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.672 (31.672)	mem 64.215
Train: [5][397/750]	BT 0.274 (1.257)	DT 0.003 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.422 (26.422)	mem 64.211
Train: [5][398/750]	BT 0.198 (1.255)	DT 0.003 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.609 (34.609)	mem 63.476
Train: [5][399/750]	BT 0.229 (1.252)	DT 0.027 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.453 (30.453)	mem 63.446
Train: [5][400/750]	BT 6.769 (1.266)	DT 6.518 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.750 (33.750)	mem 59.403
Train: [5][401/750]	BT 0.209 (1.263)	DT 0.004 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.688 (30.688)	mem 59.457
Train: [5][402/750]	BT 4.934 (1.273)	DT 4.812 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.516 (33.516)	mem 60.516
Train: [5][403/750]	BT 0.182 (1.270)	DT 0.027 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.219 (29.219)	mem 60.533
Train: [5][404/750]	BT 0.242 (1.267)	DT 0.016 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.922 (31.922)	mem 60.535
Train: [5][405/750]	BT 0.180 (1.265)	DT 0.008 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.500 (28.500)	mem 60.562
Train: [5][406/750]	BT 0.146 (1.262)	DT 0.010 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.625 (31.625)	mem 60.598
Train: [5][407/750]	BT 0.288 (1.259)	DT 0.012 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.547 (33.547)	mem 60.700
Train: [5][408/750]	BT 0.151 (1.257)	DT 0.004 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.609 (33.609)	mem 60.667
Train: [5][409/750]	BT 0.094 (1.254)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.734 (30.734)	mem 60.687
Train: [5][410/750]	BT 0.123 (1.251)	DT 0.001 (1.095)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.516 (34.516)	mem 60.745
Train: [5][411/750]	BT 0.260 (1.249)	DT 0.002 (1.093)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.453 (28.453)	mem 60.759
Train: [5][412/750]	BT 8.704 (1.267)	DT 8.592 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 62.545
Train: [5][413/750]	BT 0.123 (1.264)	DT 0.017 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.000 (33.000)	mem 62.592
Train: [5][414/750]	BT 1.831 (1.265)	DT 1.742 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.469 (35.469)	mem 62.949
Train: [5][415/750]	BT 0.093 (1.263)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.875 (29.875)	mem 62.953
Train: [5][416/750]	BT 5.555 (1.273)	DT 5.363 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.375 (35.375)	mem 64.348
Train: [5][417/750]	BT 0.151 (1.270)	DT 0.019 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.891 (30.891)	mem 64.322
Train: [5][418/750]	BT 0.167 (1.268)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.656 (33.656)	mem 64.542
Train: [5][419/750]	BT 0.275 (1.265)	DT 0.009 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.266 (32.266)	mem 64.574
Train: [5][420/750]	BT 0.191 (1.263)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.188 (33.188)	mem 64.315
Train: [5][421/750]	BT 0.115 (1.260)	DT 0.002 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.516 (32.516)	mem 64.339
Train: [5][422/750]	BT 0.166 (1.257)	DT 0.002 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.703 (36.703)	mem 64.386
Train: [5][423/750]	BT 0.201 (1.255)	DT 0.027 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.531 (27.531)	mem 64.335
Train: [5][424/750]	BT 2.821 (1.259)	DT 2.685 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.172 (33.172)	mem 64.830
Train: [5][425/750]	BT 0.080 (1.256)	DT 0.003 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.312 (33.312)	mem 64.765
Train: [5][426/750]	BT 4.355 (1.263)	DT 4.182 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.047 (34.047)	mem 65.002
Train: [5][427/750]	BT 0.112 (1.260)	DT 0.001 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.547 (35.547)	mem 65.074
Train: [5][428/750]	BT 5.304 (1.270)	DT 5.149 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.641 (32.641)	mem 64.973
Train: [5][429/750]	BT 0.272 (1.267)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.531 (32.531)	mem 65.000
Train: [5][430/750]	BT 0.224 (1.265)	DT 0.005 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.266 (33.266)	mem 65.002
Train: [5][431/750]	BT 0.164 (1.262)	DT 0.012 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.469 (32.469)	mem 65.002
Train: [5][432/750]	BT 0.241 (1.260)	DT 0.003 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.000 (34.000)	mem 65.002
Train: [5][433/750]	BT 0.098 (1.257)	DT 0.003 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.078 (30.078)	mem 65.003
Train: [5][434/750]	BT 0.260 (1.255)	DT 0.006 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.547 (36.547)	mem 65.001
Train: [5][435/750]	BT 0.297 (1.253)	DT 0.026 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.859 (30.859)	mem 65.001
Train: [5][436/750]	BT 3.871 (1.259)	DT 3.756 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.031 (33.031)	mem 64.978
Train: [5][437/750]	BT 0.227 (1.257)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.078 (33.078)	mem 64.978
Train: [5][438/750]	BT 3.138 (1.261)	DT 2.990 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.078 (36.078)	mem 65.155
Train: [5][439/750]	BT 0.162 (1.258)	DT 0.008 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.844 (32.844)	mem 64.985
Train: [5][440/750]	BT 10.307 (1.279)	DT 10.194 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.094 (35.094)	mem 64.978
Train: [5][441/750]	BT 0.195 (1.276)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 24.562 (24.562)	mem 65.090
Train: [5][442/750]	BT 0.112 (1.274)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.766 (37.766)	mem 65.284
Train: [5][443/750]	BT 0.180 (1.271)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 52.797 (52.797)	mem 65.197
Train: [5][444/750]	BT 0.147 (1.269)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.875 (33.875)	mem 65.099
Train: [5][445/750]	BT 0.268 (1.267)	DT 0.005 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.531 (29.531)	mem 65.031
Train: [5][446/750]	BT 0.196 (1.264)	DT 0.003 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.312 (34.312)	mem 64.986
Train: [5][447/750]	BT 0.129 (1.262)	DT 0.008 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.094 (34.094)	mem 65.097
Train: [5][448/750]	BT 1.081 (1.261)	DT 0.935 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.891 (32.891)	mem 64.976
Train: [5][449/750]	BT 0.141 (1.259)	DT 0.017 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.688 (31.688)	mem 64.977
Train: [5][450/750]	BT 0.097 (1.256)	DT 0.011 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.422 (33.422)	mem 64.978
Train: [5][451/750]	BT 0.227 (1.254)	DT 0.001 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.422 (35.422)	mem 65.054
Train: [5][452/750]	BT 9.057 (1.271)	DT 8.868 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.109 (35.109)	mem 65.062
Train: [5][453/750]	BT 0.108 (1.269)	DT 0.003 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.125 (32.125)	mem 65.064
Train: [5][454/750]	BT 0.121 (1.266)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.844 (33.844)	mem 65.065
Train: [5][455/750]	BT 0.094 (1.263)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.438 (30.438)	mem 65.065
Train: [5][456/750]	BT 0.130 (1.261)	DT 0.001 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.688 (35.688)	mem 65.126
Train: [5][457/750]	BT 0.124 (1.258)	DT 0.014 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.391 (32.391)	mem 65.064
Train: [5][458/750]	BT 0.255 (1.256)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.641 (38.641)	mem 65.117
Train: [5][459/750]	BT 0.133 (1.254)	DT 0.002 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.062 (34.062)	mem 65.100
Train: [5][460/750]	BT 3.567 (1.259)	DT 3.449 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.328 (34.328)	mem 65.058
Train: [5][461/750]	BT 0.082 (1.256)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.453 (32.453)	mem 65.054
Train: [5][462/750]	BT 3.131 (1.260)	DT 2.902 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.062 (34.062)	mem 58.599
Train: [5][463/750]	BT 0.112 (1.258)	DT 0.008 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.734 (34.734)	mem 58.659
Train: [5][464/750]	BT 7.231 (1.271)	DT 7.013 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.297 (34.297)	mem 59.988
Train: [5][465/750]	BT 0.175 (1.268)	DT 0.007 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.641 (29.641)	mem 59.929
Train: [5][466/750]	BT 0.178 (1.266)	DT 0.008 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.156 (31.156)	mem 59.906
Train: [5][467/750]	BT 0.223 (1.264)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.531 (32.531)	mem 59.948
Train: [5][468/750]	BT 0.099 (1.261)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.234 (35.234)	mem 60.085
Train: [5][469/750]	BT 0.153 (1.259)	DT 0.026 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.062 (30.062)	mem 60.082
Train: [5][470/750]	BT 0.119 (1.257)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.109 (33.109)	mem 60.164
Train: [5][471/750]	BT 0.268 (1.254)	DT 0.007 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.438 (32.438)	mem 60.126
Train: [5][472/750]	BT 3.634 (1.260)	DT 3.483 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.766 (34.766)	mem 60.742
Train: [5][473/750]	BT 0.139 (1.257)	DT 0.003 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.656 (27.656)	mem 60.836
Train: [5][474/750]	BT 1.089 (1.257)	DT 0.853 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.125 (32.125)	mem 60.918
Train: [5][475/750]	BT 0.367 (1.255)	DT 0.006 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.125 (29.125)	mem 60.966
Train: [5][476/750]	BT 7.034 (1.267)	DT 6.879 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.172 (29.172)	mem 62.265
Train: [5][477/750]	BT 0.156 (1.265)	DT 0.039 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.734 (30.734)	mem 62.152
Train: [5][478/750]	BT 0.107 (1.262)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.109 (33.109)	mem 62.214
Train: [5][479/750]	BT 0.120 (1.260)	DT 0.002 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.719 (39.719)	mem 62.386
Train: [5][480/750]	BT 0.112 (1.258)	DT 0.009 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.125 (38.125)	mem 62.407
Train: [5][481/750]	BT 0.161 (1.255)	DT 0.017 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.953 (30.953)	mem 62.438
Train: [5][482/750]	BT 0.181 (1.253)	DT 0.008 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.812 (29.812)	mem 62.318
Train: [5][483/750]	BT 0.145 (1.251)	DT 0.028 (1.094)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.359 (34.359)	mem 62.397
Train: [5][484/750]	BT 8.165 (1.265)	DT 8.049 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.531 (37.531)	mem 63.691
Train: [5][485/750]	BT 0.161 (1.263)	DT 0.017 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.047 (32.047)	mem 63.831
Train: [5][486/750]	BT 3.502 (1.267)	DT 3.317 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.766 (34.766)	mem 64.036
Train: [5][487/750]	BT 0.096 (1.265)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.656 (30.656)	mem 64.049
Train: [5][488/750]	BT 5.760 (1.274)	DT 5.492 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.594 (35.594)	mem 62.408
Train: [5][489/750]	BT 0.265 (1.272)	DT 0.012 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.875 (27.875)	mem 60.878
Train: [5][490/750]	BT 0.246 (1.270)	DT 0.011 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.531 (38.531)	mem 60.165
Train: [5][491/750]	BT 0.272 (1.268)	DT 0.012 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.844 (28.844)	mem 59.474
Train: [5][492/750]	BT 0.194 (1.266)	DT 0.008 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.422 (38.422)	mem 58.724
Train: [5][493/750]	BT 0.239 (1.264)	DT 0.018 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.391 (33.391)	mem 58.408
Train: [5][494/750]	BT 0.161 (1.261)	DT 0.010 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.625 (32.625)	mem 58.641
Train: [5][495/750]	BT 0.209 (1.259)	DT 0.014 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.797 (35.797)	mem 58.517
Train: [5][496/750]	BT 5.115 (1.267)	DT 4.904 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.609 (35.609)	mem 59.255
Train: [5][497/750]	BT 0.113 (1.265)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.391 (29.391)	mem 59.285
Train: [5][498/750]	BT 4.092 (1.270)	DT 3.980 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.969 (28.969)	mem 59.843
Train: [5][499/750]	BT 0.152 (1.268)	DT 0.018 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.156 (40.156)	mem 59.951
Train: [5][500/750]	BT 8.160 (1.282)	DT 8.058 (1.125)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.422 (35.422)	mem 61.701
Train: [5][501/750]	BT 0.201 (1.280)	DT 0.002 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.344 (30.344)	mem 61.872
Train: [5][502/750]	BT 0.135 (1.278)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.281 (33.281)	mem 61.882
Train: [5][503/750]	BT 0.152 (1.275)	DT 0.014 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.828 (28.828)	mem 61.957
Train: [5][504/750]	BT 0.129 (1.273)	DT 0.024 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.547 (36.547)	mem 62.013
Train: [5][505/750]	BT 0.118 (1.271)	DT 0.003 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 42.188 (42.188)	mem 62.112
Train: [5][506/750]	BT 0.144 (1.269)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.188 (39.188)	mem 62.062
Train: [5][507/750]	BT 0.122 (1.266)	DT 0.003 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.844 (27.844)	mem 62.157
Train: [5][508/750]	BT 0.161 (1.264)	DT 0.013 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.766 (31.766)	mem 62.209
Train: [5][509/750]	BT 0.175 (1.262)	DT 0.008 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.453 (28.453)	mem 62.181
Train: [5][510/750]	BT 6.332 (1.272)	DT 6.254 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.641 (34.641)	mem 63.345
Train: [5][511/750]	BT 0.211 (1.270)	DT 0.001 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.469 (32.469)	mem 63.384
Train: [5][512/750]	BT 6.016 (1.279)	DT 5.883 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.094 (32.094)	mem 64.750
Train: [5][513/750]	BT 0.130 (1.277)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.719 (28.719)	mem 64.778
Train: [5][514/750]	BT 0.108 (1.275)	DT 0.010 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.312 (33.312)	mem 64.804
Train: [5][515/750]	BT 0.094 (1.272)	DT 0.002 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.125 (31.125)	mem 64.812
Train: [5][516/750]	BT 0.116 (1.270)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.750 (33.750)	mem 64.871
Train: [5][517/750]	BT 0.124 (1.268)	DT 0.005 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.062 (33.062)	mem 64.850
Train: [5][518/750]	BT 0.216 (1.266)	DT 0.014 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.969 (32.969)	mem 64.876
Train: [5][519/750]	BT 0.133 (1.264)	DT 0.018 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.422 (31.422)	mem 64.919
Train: [5][520/750]	BT 0.798 (1.263)	DT 0.667 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.406 (40.406)	mem 64.981
Train: [5][521/750]	BT 0.102 (1.260)	DT 0.002 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.875 (31.875)	mem 64.995
Train: [5][522/750]	BT 5.666 (1.269)	DT 5.462 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.625 (31.625)	mem 65.123
Train: [5][523/750]	BT 0.192 (1.267)	DT 0.006 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.391 (30.391)	mem 65.167
Train: [5][524/750]	BT 4.570 (1.273)	DT 4.324 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.203 (34.203)	mem 65.174
Train: [5][525/750]	BT 0.094 (1.271)	DT 0.005 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.969 (28.969)	mem 65.156
Train: [5][526/750]	BT 0.168 (1.269)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.500 (30.500)	mem 65.301
Train: [5][527/750]	BT 0.131 (1.267)	DT 0.021 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.047 (29.047)	mem 65.217
Train: [5][528/750]	BT 0.190 (1.265)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.016 (36.016)	mem 65.359
Train: [5][529/750]	BT 0.108 (1.262)	DT 0.003 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.141 (31.141)	mem 65.109
Train: [5][530/750]	BT 0.122 (1.260)	DT 0.001 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.344 (35.344)	mem 65.142
Train: [5][531/750]	BT 0.161 (1.258)	DT 0.001 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 21.141 (21.141)	mem 65.240
Train: [5][532/750]	BT 6.535 (1.268)	DT 6.328 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.703 (31.703)	mem 65.170
Train: [5][533/750]	BT 0.161 (1.266)	DT 0.003 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.969 (26.969)	mem 65.208
Train: [5][534/750]	BT 3.034 (1.269)	DT 2.851 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.641 (36.641)	mem 65.256
Train: [5][535/750]	BT 0.204 (1.267)	DT 0.029 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.594 (32.594)	mem 65.076
Train: [5][536/750]	BT 2.998 (1.271)	DT 2.880 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.812 (32.812)	mem 65.129
Train: [5][537/750]	BT 0.218 (1.269)	DT 0.004 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.016 (31.016)	mem 65.128
Train: [5][538/750]	BT 0.150 (1.267)	DT 0.004 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.688 (35.688)	mem 65.156
Train: [5][539/750]	BT 0.194 (1.265)	DT 0.004 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.656 (32.656)	mem 65.240
Train: [5][540/750]	BT 0.304 (1.263)	DT 0.012 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.250 (34.250)	mem 65.173
Train: [5][541/750]	BT 0.139 (1.261)	DT 0.009 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.750 (28.750)	mem 65.130
Train: [5][542/750]	BT 0.140 (1.259)	DT 0.003 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.234 (33.234)	mem 65.130
Train: [5][543/750]	BT 0.171 (1.257)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.641 (32.641)	mem 65.229
Train: [5][544/750]	BT 6.482 (1.266)	DT 6.310 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.500 (33.500)	mem 65.140
Train: [5][545/750]	BT 0.224 (1.264)	DT 0.009 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.641 (32.641)	mem 65.011
Train: [5][546/750]	BT 0.317 (1.263)	DT 0.022 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.438 (30.438)	mem 65.012
Train: [5][547/750]	BT 0.177 (1.261)	DT 0.017 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.406 (32.406)	mem 65.011
Train: [5][548/750]	BT 6.447 (1.270)	DT 6.270 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.672 (32.672)	mem 65.176
Train: [5][549/750]	BT 0.220 (1.268)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.859 (29.859)	mem 65.157
Train: [5][550/750]	BT 0.172 (1.266)	DT 0.008 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.250 (29.250)	mem 65.066
Train: [5][551/750]	BT 0.182 (1.264)	DT 0.006 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.203 (35.203)	mem 65.065
Train: [5][552/750]	BT 0.262 (1.262)	DT 0.020 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.734 (32.734)	mem 65.117
Train: [5][553/750]	BT 0.181 (1.260)	DT 0.011 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.938 (29.938)	mem 65.086
Train: [5][554/750]	BT 0.170 (1.258)	DT 0.001 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.203 (36.203)	mem 65.087
Train: [5][555/750]	BT 0.270 (1.257)	DT 0.025 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.672 (33.672)	mem 65.087
Train: [5][556/750]	BT 6.194 (1.266)	DT 6.102 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.172 (33.172)	mem 58.696
Train: [5][557/750]	BT 0.165 (1.264)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.703 (36.703)	mem 58.722
Train: [5][558/750]	BT 2.239 (1.265)	DT 2.092 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.422 (33.422)	mem 59.102
Train: [5][559/750]	BT 0.173 (1.263)	DT 0.011 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.578 (34.578)	mem 59.066
Train: [5][560/750]	BT 9.821 (1.279)	DT 9.745 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.719 (39.719)	mem 61.087
Train: [5][561/750]	BT 0.089 (1.277)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.359 (33.359)	mem 61.111
Train: [5][562/750]	BT 0.132 (1.275)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.094 (33.094)	mem 61.195
Train: [5][563/750]	BT 0.170 (1.273)	DT 0.028 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.000 (38.000)	mem 61.252
Train: [5][564/750]	BT 0.103 (1.270)	DT 0.001 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.875 (33.875)	mem 61.359
Train: [5][565/750]	BT 0.111 (1.268)	DT 0.003 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.578 (27.578)	mem 61.363
Train: [5][566/750]	BT 0.129 (1.266)	DT 0.002 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.875 (35.875)	mem 61.440
Train: [5][567/750]	BT 0.152 (1.264)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.969 (36.969)	mem 61.513
Train: [5][568/750]	BT 0.135 (1.262)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.641 (35.641)	mem 61.484
Train: [5][569/750]	BT 0.129 (1.260)	DT 0.001 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.266 (29.266)	mem 61.519
Train: [5][570/750]	BT 0.150 (1.259)	DT 0.011 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.750 (31.750)	mem 61.562
Train: [5][571/750]	BT 0.122 (1.257)	DT 0.002 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.656 (27.656)	mem 61.612
Train: [5][572/750]	BT 13.883 (1.279)	DT 13.774 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.078 (33.078)	mem 64.042
Train: [5][573/750]	BT 0.093 (1.277)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.797 (33.797)	mem 64.080
Train: [5][574/750]	BT 0.136 (1.275)	DT 0.009 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.250 (34.250)	mem 64.085
Train: [5][575/750]	BT 0.322 (1.273)	DT 0.002 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.781 (32.781)	mem 64.123
Train: [5][576/750]	BT 0.100 (1.271)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.641 (33.641)	mem 64.130
Train: [5][577/750]	BT 0.143 (1.269)	DT 0.018 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.250 (34.250)	mem 64.139
Train: [5][578/750]	BT 0.191 (1.267)	DT 0.001 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.203 (35.203)	mem 64.190
Train: [5][579/750]	BT 0.121 (1.265)	DT 0.002 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.375 (28.375)	mem 64.420
Train: [5][580/750]	BT 0.283 (1.263)	DT 0.005 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.172 (37.172)	mem 64.208
Train: [5][581/750]	BT 0.173 (1.261)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.875 (29.875)	mem 64.289
Train: [5][582/750]	BT 0.405 (1.260)	DT 0.049 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.922 (32.922)	mem 64.435
Train: [5][583/750]	BT 0.236 (1.258)	DT 0.010 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.156 (31.156)	mem 64.215
Train: [5][584/750]	BT 14.759 (1.281)	DT 14.673 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.734 (35.734)	mem 60.619
Train: [5][585/750]	BT 0.155 (1.279)	DT 0.002 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.328 (32.328)	mem 60.658
Train: [5][586/750]	BT 0.159 (1.278)	DT 0.003 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.797 (32.797)	mem 60.645
Train: [5][587/750]	BT 0.201 (1.276)	DT 0.003 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.906 (31.906)	mem 60.655
Train: [5][588/750]	BT 0.109 (1.274)	DT 0.010 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.062 (34.062)	mem 60.677
Train: [5][589/750]	BT 0.200 (1.272)	DT 0.027 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.703 (30.703)	mem 60.796
Train: [5][590/750]	BT 0.165 (1.270)	DT 0.002 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.109 (34.109)	mem 60.856
Train: [5][591/750]	BT 0.085 (1.268)	DT 0.006 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.344 (31.344)	mem 60.796
Train: [5][592/750]	BT 0.097 (1.266)	DT 0.001 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.906 (36.906)	mem 60.916
Train: [5][593/750]	BT 0.159 (1.264)	DT 0.003 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.781 (38.781)	mem 60.898
Train: [5][594/750]	BT 0.104 (1.262)	DT 0.003 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.391 (30.391)	mem 60.909
Train: [5][595/750]	BT 0.164 (1.260)	DT 0.002 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.109 (34.109)	mem 60.910
Train: [5][596/750]	BT 13.277 (1.281)	DT 13.196 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.438 (37.438)	mem 63.351
Train: [5][597/750]	BT 0.131 (1.279)	DT 0.002 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.422 (33.422)	mem 63.405
Train: [5][598/750]	BT 0.164 (1.277)	DT 0.002 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.297 (32.297)	mem 63.446
Train: [5][599/750]	BT 0.120 (1.275)	DT 0.002 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.688 (34.688)	mem 63.524
Train: [5][600/750]	BT 0.083 (1.273)	DT 0.002 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.828 (36.828)	mem 63.682
Train: [5][601/750]	BT 0.425 (1.271)	DT 0.012 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.578 (30.578)	mem 63.867
Train: [5][602/750]	BT 0.130 (1.270)	DT 0.003 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.031 (36.031)	mem 63.675
Train: [5][603/750]	BT 0.170 (1.268)	DT 0.005 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.156 (35.156)	mem 63.742
Train: [5][604/750]	BT 0.205 (1.266)	DT 0.008 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.438 (28.438)	mem 63.812
Train: [5][605/750]	BT 0.188 (1.264)	DT 0.008 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.141 (35.141)	mem 63.901
Train: [5][606/750]	BT 0.183 (1.262)	DT 0.030 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.375 (37.375)	mem 63.882
Train: [5][607/750]	BT 0.099 (1.260)	DT 0.001 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.922 (33.922)	mem 63.906
Train: [5][608/750]	BT 13.803 (1.281)	DT 13.687 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.734 (34.734)	mem 65.222
Train: [5][609/750]	BT 0.138 (1.279)	DT 0.006 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 65.099
Train: [5][610/750]	BT 0.150 (1.277)	DT 0.013 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.172 (33.172)	mem 65.176
Train: [5][611/750]	BT 0.158 (1.276)	DT 0.003 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.312 (30.312)	mem 65.259
Train: [5][612/750]	BT 0.173 (1.274)	DT 0.008 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.547 (36.547)	mem 65.100
Train: [5][613/750]	BT 0.148 (1.272)	DT 0.017 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.812 (33.812)	mem 65.099
Train: [5][614/750]	BT 0.103 (1.270)	DT 0.012 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.234 (32.234)	mem 65.144
Train: [5][615/750]	BT 0.100 (1.268)	DT 0.003 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.312 (28.312)	mem 65.100
Train: [5][616/750]	BT 0.101 (1.266)	DT 0.003 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.547 (34.547)	mem 65.100
Train: [5][617/750]	BT 0.183 (1.264)	DT 0.023 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.141 (29.141)	mem 65.100
Train: [5][618/750]	BT 1.711 (1.265)	DT 1.543 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.062 (33.062)	mem 65.066
Train: [5][619/750]	BT 0.190 (1.263)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.484 (29.484)	mem 65.313
Train: [5][620/750]	BT 13.314 (1.283)	DT 13.180 (1.126)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.422 (35.422)	mem 65.004
Train: [5][621/750]	BT 0.111 (1.281)	DT 0.001 (1.125)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.672 (29.672)	mem 65.057
Train: [5][622/750]	BT 0.135 (1.279)	DT 0.014 (1.123)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.516 (36.516)	mem 65.094
Train: [5][623/750]	BT 0.141 (1.277)	DT 0.002 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.859 (29.859)	mem 64.989
Train: [5][624/750]	BT 0.124 (1.275)	DT 0.010 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.250 (32.250)	mem 65.021
Train: [5][625/750]	BT 0.127 (1.274)	DT 0.002 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.344 (32.344)	mem 64.992
Train: [5][626/750]	BT 0.079 (1.272)	DT 0.001 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.969 (35.969)	mem 65.062
Train: [5][627/750]	BT 0.236 (1.270)	DT 0.016 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.078 (31.078)	mem 65.029
Train: [5][628/750]	BT 0.147 (1.268)	DT 0.010 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.312 (35.312)	mem 65.030
Train: [5][629/750]	BT 0.169 (1.267)	DT 0.005 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.812 (31.812)	mem 65.054
Train: [5][630/750]	BT 4.438 (1.272)	DT 4.297 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.297 (36.297)	mem 65.146
Train: [5][631/750]	BT 0.121 (1.270)	DT 0.012 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.859 (27.859)	mem 65.050
Train: [5][632/750]	BT 8.558 (1.281)	DT 8.457 (1.125)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.109 (35.109)	mem 65.088
Train: [5][633/750]	BT 0.089 (1.279)	DT 0.002 (1.124)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.047 (32.047)	mem 65.087
Train: [5][634/750]	BT 0.114 (1.278)	DT 0.002 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.250 (32.250)	mem 65.078
Train: [5][635/750]	BT 0.118 (1.276)	DT 0.006 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.797 (33.797)	mem 63.364
Train: [5][636/750]	BT 0.147 (1.274)	DT 0.005 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.078 (34.078)	mem 60.178
Train: [5][637/750]	BT 0.289 (1.272)	DT 0.045 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.828 (26.828)	mem 58.390
Train: [5][638/750]	BT 0.119 (1.271)	DT 0.028 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.078 (34.078)	mem 58.335
Train: [5][639/750]	BT 0.151 (1.269)	DT 0.002 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.266 (30.266)	mem 58.322
Train: [5][640/750]	BT 0.200 (1.267)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.172 (30.172)	mem 58.392
Train: [5][641/750]	BT 0.086 (1.265)	DT 0.015 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.750 (27.750)	mem 58.410
Train: [5][642/750]	BT 4.439 (1.270)	DT 4.331 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.766 (34.766)	mem 59.217
Train: [5][643/750]	BT 0.212 (1.269)	DT 0.006 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.656 (29.656)	mem 59.185
Train: [5][644/750]	BT 6.504 (1.277)	DT 6.388 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.938 (35.938)	mem 60.855
Train: [5][645/750]	BT 0.141 (1.275)	DT 0.012 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.625 (32.625)	mem 60.877
Train: [5][646/750]	BT 0.168 (1.273)	DT 0.002 (1.118)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.391 (32.391)	mem 60.588
Train: [5][647/750]	BT 0.104 (1.271)	DT 0.002 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.141 (31.141)	mem 60.633
Train: [5][648/750]	BT 0.115 (1.270)	DT 0.001 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.234 (37.234)	mem 60.682
Train: [5][649/750]	BT 0.128 (1.268)	DT 0.005 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.547 (27.547)	mem 60.715
Train: [5][650/750]	BT 0.161 (1.266)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.438 (30.438)	mem 60.716
Train: [5][651/750]	BT 0.147 (1.264)	DT 0.018 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.328 (30.328)	mem 60.724
Train: [5][652/750]	BT 0.097 (1.263)	DT 0.010 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.844 (27.844)	mem 60.779
Train: [5][653/750]	BT 0.103 (1.261)	DT 0.001 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.953 (28.953)	mem 60.854
Train: [5][654/750]	BT 7.534 (1.271)	DT 7.321 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.156 (31.156)	mem 62.226
Train: [5][655/750]	BT 0.316 (1.269)	DT 0.044 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.703 (27.703)	mem 62.161
Train: [5][656/750]	BT 6.823 (1.278)	DT 6.715 (1.122)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.219 (35.219)	mem 63.731
Train: [5][657/750]	BT 0.098 (1.276)	DT 0.002 (1.121)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.141 (34.141)	mem 63.747
Train: [5][658/750]	BT 0.119 (1.274)	DT 0.004 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.016 (36.016)	mem 63.612
Train: [5][659/750]	BT 0.127 (1.272)	DT 0.006 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.297 (31.297)	mem 63.629
Train: [5][660/750]	BT 0.103 (1.270)	DT 0.007 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.562 (31.562)	mem 63.693
Train: [5][661/750]	BT 0.136 (1.269)	DT 0.001 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.406 (30.406)	mem 63.711
Train: [5][662/750]	BT 0.116 (1.267)	DT 0.016 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.984 (28.984)	mem 63.778
Train: [5][663/750]	BT 0.191 (1.265)	DT 0.002 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.766 (31.766)	mem 63.791
Train: [5][664/750]	BT 0.165 (1.264)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.031 (39.031)	mem 63.813
Train: [5][665/750]	BT 0.199 (1.262)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.578 (33.578)	mem 63.917
Train: [5][666/750]	BT 1.820 (1.263)	DT 1.737 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.906 (32.906)	mem 64.122
Train: [5][667/750]	BT 0.118 (1.261)	DT 0.001 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.078 (30.078)	mem 64.171
Train: [5][668/750]	BT 9.413 (1.273)	DT 9.302 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.781 (31.781)	mem 59.179
Train: [5][669/750]	BT 0.094 (1.272)	DT 0.006 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.703 (32.703)	mem 59.216
Train: [5][670/750]	BT 0.120 (1.270)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.688 (33.688)	mem 59.277
Train: [5][671/750]	BT 0.108 (1.268)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.562 (33.562)	mem 59.267
Train: [5][672/750]	BT 0.106 (1.267)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.000 (35.000)	mem 59.282
Train: [5][673/750]	BT 0.223 (1.265)	DT 0.009 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.953 (34.953)	mem 59.307
Train: [5][674/750]	BT 0.184 (1.263)	DT 0.002 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.484 (31.484)	mem 59.323
Train: [5][675/750]	BT 0.218 (1.262)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.609 (31.609)	mem 59.445
Train: [5][676/750]	BT 0.186 (1.260)	DT 0.025 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.312 (33.312)	mem 59.422
Train: [5][677/750]	BT 0.109 (1.259)	DT 0.007 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.422 (32.422)	mem 59.439
Train: [5][678/750]	BT 4.160 (1.263)	DT 4.045 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.562 (31.562)	mem 60.353
Train: [5][679/750]	BT 0.278 (1.261)	DT 0.020 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.531 (31.531)	mem 60.603
Train: [5][680/750]	BT 10.606 (1.275)	DT 10.458 (1.120)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.766 (34.766)	mem 62.580
Train: [5][681/750]	BT 0.075 (1.273)	DT 0.002 (1.119)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.781 (31.781)	mem 62.605
Train: [5][682/750]	BT 0.074 (1.272)	DT 0.001 (1.117)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.344 (31.344)	mem 62.646
Train: [5][683/750]	BT 0.095 (1.270)	DT 0.002 (1.115)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.312 (33.312)	mem 62.673
Train: [5][684/750]	BT 0.137 (1.268)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.750 (32.750)	mem 62.710
Train: [5][685/750]	BT 0.087 (1.266)	DT 0.002 (1.112)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.672 (26.672)	mem 62.731
Train: [5][686/750]	BT 0.099 (1.265)	DT 0.002 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.859 (31.859)	mem 62.803
Train: [5][687/750]	BT 0.122 (1.263)	DT 0.004 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.438 (30.438)	mem 62.786
Train: [5][688/750]	BT 0.105 (1.261)	DT 0.007 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.281 (30.281)	mem 62.814
Train: [5][689/750]	BT 0.199 (1.260)	DT 0.006 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.797 (27.797)	mem 62.843
Train: [5][690/750]	BT 4.655 (1.265)	DT 4.358 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.984 (34.984)	mem 63.853
Train: [5][691/750]	BT 0.166 (1.263)	DT 0.003 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.422 (28.422)	mem 63.874
Train: [5][692/750]	BT 6.033 (1.270)	DT 5.866 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.641 (29.641)	mem 64.944
Train: [5][693/750]	BT 0.180 (1.269)	DT 0.002 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.922 (32.922)	mem 65.028
Train: [5][694/750]	BT 0.102 (1.267)	DT 0.007 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.250 (35.250)	mem 65.121
Train: [5][695/750]	BT 0.278 (1.265)	DT 0.017 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.203 (32.203)	mem 65.123
Train: [5][696/750]	BT 0.243 (1.264)	DT 0.031 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.750 (32.750)	mem 65.131
Train: [5][697/750]	BT 0.168 (1.262)	DT 0.013 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.609 (29.609)	mem 65.078
Train: [5][698/750]	BT 0.234 (1.261)	DT 0.002 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.359 (33.359)	mem 65.064
Train: [5][699/750]	BT 0.252 (1.259)	DT 0.013 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.828 (33.828)	mem 65.074
Train: [5][700/750]	BT 0.183 (1.258)	DT 0.014 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.797 (31.797)	mem 65.097
Train: [5][701/750]	BT 0.301 (1.257)	DT 0.013 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.422 (27.422)	mem 65.408
Train: [5][702/750]	BT 6.840 (1.265)	DT 6.489 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.750 (30.750)	mem 65.440
Train: [5][703/750]	BT 0.170 (1.263)	DT 0.027 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.688 (28.688)	mem 65.109
Train: [5][704/750]	BT 6.855 (1.271)	DT 6.744 (1.116)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.766 (39.766)	mem 65.157
Train: [5][705/750]	BT 0.182 (1.269)	DT 0.003 (1.114)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.297 (31.297)	mem 65.158
Train: [5][706/750]	BT 0.107 (1.268)	DT 0.001 (1.113)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.375 (35.375)	mem 65.158
Train: [5][707/750]	BT 0.173 (1.266)	DT 0.010 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.609 (29.609)	mem 65.157
Train: [5][708/750]	BT 0.136 (1.265)	DT 0.001 (1.109)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.750 (37.750)	mem 65.157
Train: [5][709/750]	BT 0.119 (1.263)	DT 0.001 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.016 (33.016)	mem 65.156
Train: [5][710/750]	BT 0.107 (1.261)	DT 0.003 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 65.155
Train: [5][711/750]	BT 0.146 (1.260)	DT 0.002 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.859 (28.859)	mem 65.157
Train: [5][712/750]	BT 0.215 (1.258)	DT 0.022 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.719 (31.719)	mem 65.158
Train: [5][713/750]	BT 0.284 (1.257)	DT 0.002 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 65.158
Train: [5][714/750]	BT 2.530 (1.259)	DT 2.228 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.562 (34.562)	mem 65.157
Train: [5][715/750]	BT 0.204 (1.257)	DT 0.003 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.656 (29.656)	mem 65.149
Train: [5][716/750]	BT 7.985 (1.267)	DT 7.825 (1.111)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.125 (32.125)	mem 65.251
Train: [5][717/750]	BT 0.154 (1.265)	DT 0.017 (1.110)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.906 (31.906)	mem 65.199
Train: [5][718/750]	BT 0.163 (1.264)	DT 0.009 (1.108)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.562 (35.562)	mem 65.131
Train: [5][719/750]	BT 0.096 (1.262)	DT 0.002 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.422 (30.422)	mem 65.130
Train: [5][720/750]	BT 0.083 (1.260)	DT 0.001 (1.105)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.531 (35.531)	mem 65.131
Train: [5][721/750]	BT 0.182 (1.259)	DT 0.002 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.828 (26.828)	mem 65.276
Train: [5][722/750]	BT 0.138 (1.257)	DT 0.002 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.203 (30.203)	mem 65.273
Train: [5][723/750]	BT 0.200 (1.256)	DT 0.014 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.188 (30.188)	mem 65.135
Train: [5][724/750]	BT 0.203 (1.254)	DT 0.002 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.469 (31.469)	mem 65.250
Train: [5][725/750]	BT 0.261 (1.253)	DT 0.053 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.672 (32.672)	mem 65.136
Train: [5][726/750]	BT 3.879 (1.257)	DT 3.756 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.391 (32.391)	mem 65.061
Train: [5][727/750]	BT 0.175 (1.255)	DT 0.012 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.828 (26.828)	mem 65.062
Train: [5][728/750]	BT 6.749 (1.263)	DT 6.664 (1.107)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.750 (31.750)	mem 64.849
Train: [5][729/750]	BT 0.201 (1.261)	DT 0.020 (1.106)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.172 (35.172)	mem 64.776
Train: [5][730/750]	BT 0.259 (1.260)	DT 0.027 (1.104)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.922 (36.922)	mem 64.769
Train: [5][731/750]	BT 0.134 (1.258)	DT 0.011 (1.103)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.078 (29.078)	mem 64.764
Train: [5][732/750]	BT 0.102 (1.257)	DT 0.002 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.141 (33.141)	mem 64.761
Train: [5][733/750]	BT 0.091 (1.255)	DT 0.003 (1.100)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.812 (31.812)	mem 64.761
Train: [5][734/750]	BT 0.100 (1.253)	DT 0.002 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.266 (35.266)	mem 64.689
Train: [5][735/750]	BT 0.098 (1.252)	DT 0.002 (1.097)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.266 (28.266)	mem 64.102
Train: [5][736/750]	BT 0.098 (1.250)	DT 0.002 (1.095)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.750 (34.750)	mem 62.328
Train: [5][737/750]	BT 0.176 (1.249)	DT 0.002 (1.094)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.656 (30.656)	mem 59.474
Train: [5][738/750]	BT 2.997 (1.251)	DT 2.845 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.781 (33.781)	mem 41.352
Train: [5][739/750]	BT 0.111 (1.250)	DT 0.002 (1.095)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.812 (33.812)	mem 41.378
Train: [5][740/750]	BT 6.571 (1.257)	DT 6.499 (1.102)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.094 (38.094)	mem 28.460
Train: [5][741/750]	BT 0.073 (1.255)	DT 0.001 (1.101)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.984 (31.984)	mem 28.505
Train: [5][742/750]	BT 0.076 (1.254)	DT 0.001 (1.099)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.844 (29.844)	mem 28.548
Train: [5][743/750]	BT 0.079 (1.252)	DT 0.001 (1.098)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.906 (29.906)	mem 28.653
Train: [5][744/750]	BT 0.093 (1.251)	DT 0.002 (1.096)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.609 (30.609)	mem 28.584
Train: [5][745/750]	BT 0.076 (1.249)	DT 0.004 (1.095)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.031 (38.031)	mem 28.610
Train: [5][746/750]	BT 0.058 (1.247)	DT 0.001 (1.093)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.094 (40.094)	mem 28.637
Train: [5][747/750]	BT 0.078 (1.246)	DT 0.001 (1.092)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.719 (32.719)	mem 28.809
Train: [5][748/750]	BT 0.098 (1.244)	DT 0.001 (1.090)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.406 (32.406)	mem 28.686
Train: [5][749/750]	BT 0.066 (1.243)	DT 0.003 (1.089)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.219 (33.219)	mem 28.698
Train: [5][750/750]	BT 0.068 (1.241)	DT 0.001 (1.087)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.656 (39.656)	mem 28.736
Train: [5][751/750]	BT 0.131 (1.240)	DT 0.003 (1.086)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.000 (34.000)	mem 28.760
Train: [5][752/750]	BT 2.648 (1.242)	DT 2.570 (1.088)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 40.938 (40.938)	mem 29.865
Train: [5][753/750]	BT 0.071 (1.240)	DT 0.002 (1.086)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.281 (31.281)	mem 29.902
Train: [5][754/750]	BT 0.071 (1.238)	DT 0.002 (1.085)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.344 (31.344)	mem 29.920
Train: [5][755/750]	BT 0.072 (1.237)	DT 0.002 (1.084)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.000 (29.000)	mem 29.943
Train: [5][756/750]	BT 0.072 (1.235)	DT 0.002 (1.082)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.625 (35.625)	mem 29.964
epoch 5, total time 934.16
==> Saving...
==> Saving...
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [6][1/750]	BT 26.317 (26.317)	DT 26.087 (26.087)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.344 (34.344)	mem 61.248
Train: [6][2/750]	BT 0.232 (13.275)	DT 0.008 (13.048)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.422 (32.422)	mem 61.255
Train: [6][3/750]	BT 0.184 (8.911)	DT 0.009 (8.702)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.109 (33.109)	mem 61.283
Train: [6][4/750]	BT 1.176 (6.977)	DT 1.060 (6.791)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.391 (35.391)	mem 61.391
Train: [6][5/750]	BT 0.091 (5.600)	DT 0.002 (5.433)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.703 (29.703)	mem 61.423
Train: [6][6/750]	BT 0.280 (4.714)	DT 0.016 (4.530)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.266 (34.266)	mem 61.532
Train: [6][7/750]	BT 0.149 (4.061)	DT 0.018 (3.886)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.516 (33.516)	mem 61.743
Train: [6][8/750]	BT 0.274 (3.588)	DT 0.003 (3.400)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.672 (33.672)	mem 61.761
Train: [6][9/750]	BT 0.205 (3.212)	DT 0.011 (3.024)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.188 (32.188)	mem 61.616
Train: [6][10/750]	BT 0.158 (2.907)	DT 0.008 (2.722)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.156 (34.156)	mem 61.722
Train: [6][11/750]	BT 0.099 (2.651)	DT 0.011 (2.476)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.328 (32.328)	mem 61.753
Train: [6][12/750]	BT 0.167 (2.444)	DT 0.002 (2.270)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.750 (37.750)	mem 61.700
Train: [6][13/750]	BT 9.958 (3.022)	DT 9.789 (2.848)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.984 (32.984)	mem 63.901
Train: [6][14/750]	BT 8.128 (3.387)	DT 7.962 (3.213)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.141 (35.141)	mem 64.056
Train: [6][15/750]	BT 0.181 (3.173)	DT 0.027 (3.001)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.641 (27.641)	mem 64.058
Train: [6][16/750]	BT 3.058 (3.166)	DT 2.918 (2.996)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.344 (36.344)	mem 63.927
Train: [6][17/750]	BT 0.194 (2.991)	DT 0.003 (2.820)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 27.875 (27.875)	mem 63.928
Train: [6][18/750]	BT 0.124 (2.832)	DT 0.002 (2.663)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.016 (31.016)	mem 63.927
Train: [6][19/750]	BT 0.124 (2.689)	DT 0.002 (2.523)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.875 (30.875)	mem 63.928
Train: [6][20/750]	BT 0.112 (2.561)	DT 0.002 (2.397)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.375 (35.375)	mem 63.994
Train: [6][21/750]	BT 0.342 (2.455)	DT 0.007 (2.283)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.750 (32.750)	mem 63.952
Train: [6][22/750]	BT 0.156 (2.350)	DT 0.002 (2.179)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.969 (29.969)	mem 63.985
Train: [6][23/750]	BT 0.243 (2.259)	DT 0.020 (2.086)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.516 (30.516)	mem 63.988
Train: [6][24/750]	BT 0.240 (2.175)	DT 0.016 (1.999)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.750 (35.750)	mem 64.038
Train: [6][25/750]	BT 0.145 (2.094)	DT 0.002 (1.919)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.531 (29.531)	mem 63.955
Train: [6][26/750]	BT 11.350 (2.450)	DT 11.145 (2.274)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.500 (30.500)	mem 64.007
Train: [6][27/750]	BT 0.142 (2.364)	DT 0.002 (2.190)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.766 (29.766)	mem 64.010
Train: [6][28/750]	BT 1.315 (2.327)	DT 1.169 (2.154)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.469 (34.469)	mem 64.015
Train: [6][29/750]	BT 0.169 (2.252)	DT 0.002 (2.080)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.859 (28.859)	mem 63.956
Train: [6][30/750]	BT 0.256 (2.186)	DT 0.002 (2.010)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.141 (36.141)	mem 64.022
Train: [6][31/750]	BT 0.188 (2.121)	DT 0.002 (1.945)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.312 (30.312)	mem 63.959
Train: [6][32/750]	BT 0.313 (2.065)	DT 0.004 (1.885)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.250 (33.250)	mem 64.056
Train: [6][33/750]	BT 0.142 (2.007)	DT 0.019 (1.828)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.062 (34.062)	mem 63.962
Train: [6][34/750]	BT 0.158 (1.952)	DT 0.001 (1.775)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.359 (32.359)	mem 63.983
Train: [6][35/750]	BT 0.103 (1.899)	DT 0.002 (1.724)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.125 (29.125)	mem 63.983
Train: [6][36/750]	BT 0.306 (1.855)	DT 0.002 (1.676)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.438 (29.438)	mem 64.033
Train: [6][37/750]	BT 0.129 (1.808)	DT 0.013 (1.631)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.016 (36.016)	mem 64.002
Train: [6][38/750]	BT 12.363 (2.086)	DT 12.279 (1.911)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.906 (33.906)	mem 64.198
Train: [6][39/750]	BT 0.150 (2.036)	DT 0.002 (1.862)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.391 (32.391)	mem 64.230
Train: [6][40/750]	BT 0.203 (1.991)	DT 0.006 (1.816)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.688 (33.688)	mem 64.119
Train: [6][41/750]	BT 0.182 (1.947)	DT 0.027 (1.772)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.703 (31.703)	mem 64.080
Train: [6][42/750]	BT 0.106 (1.903)	DT 0.002 (1.730)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.594 (36.594)	mem 64.082
Train: [6][43/750]	BT 0.149 (1.862)	DT 0.002 (1.690)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.250 (33.250)	mem 64.124
Train: [6][44/750]	BT 0.122 (1.822)	DT 0.002 (1.652)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.109 (35.109)	mem 64.082
Train: [6][45/750]	BT 0.093 (1.784)	DT 0.002 (1.615)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.062 (31.062)	mem 64.082
Train: [6][46/750]	BT 0.186 (1.749)	DT 0.003 (1.580)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.797 (32.797)	mem 64.142
Train: [6][47/750]	BT 0.250 (1.717)	DT 0.003 (1.546)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.312 (28.312)	mem 64.114
Train: [6][48/750]	BT 0.241 (1.687)	DT 0.002 (1.514)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.562 (34.562)	mem 64.083
Train: [6][49/750]	BT 0.127 (1.655)	DT 0.002 (1.483)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.375 (31.375)	mem 64.174
Train: [6][50/750]	BT 10.315 (1.828)	DT 10.134 (1.656)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.703 (29.703)	mem 58.154
Train: [6][51/750]	BT 0.185 (1.796)	DT 0.010 (1.624)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.750 (33.750)	mem 57.933
Train: [6][52/750]	BT 7.474 (1.905)	DT 7.342 (1.734)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.250 (38.250)	mem 59.679
Train: [6][53/750]	BT 0.118 (1.871)	DT 0.006 (1.701)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.172 (33.172)	mem 59.558
Train: [6][54/750]	BT 0.106 (1.839)	DT 0.002 (1.670)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.609 (36.609)	mem 59.589
Train: [6][55/750]	BT 0.096 (1.807)	DT 0.010 (1.640)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.094 (34.094)	mem 59.650
Train: [6][56/750]	BT 0.172 (1.778)	DT 0.029 (1.611)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.281 (31.281)	mem 59.626
Train: [6][57/750]	BT 0.113 (1.748)	DT 0.002 (1.583)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.969 (31.969)	mem 59.667
Train: [6][58/750]	BT 0.129 (1.721)	DT 0.002 (1.555)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.297 (35.297)	mem 59.694
Train: [6][59/750]	BT 0.140 (1.694)	DT 0.016 (1.529)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.969 (31.969)	mem 59.762
Train: [6][60/750]	BT 0.103 (1.667)	DT 0.002 (1.504)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.875 (34.875)	mem 59.801
Train: [6][61/750]	BT 0.175 (1.643)	DT 0.002 (1.479)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.656 (39.656)	mem 59.863
Train: [6][62/750]	BT 2.360 (1.654)	DT 2.098 (1.489)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.609 (32.609)	mem 60.093
Train: [6][63/750]	BT 0.112 (1.630)	DT 0.022 (1.466)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.125 (35.125)	mem 60.176
Train: [6][64/750]	BT 12.937 (1.807)	DT 12.847 (1.644)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.109 (31.109)	mem 62.858
Train: [6][65/750]	BT 0.148 (1.781)	DT 0.002 (1.619)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.469 (32.469)	mem 62.955
Train: [6][66/750]	BT 0.103 (1.756)	DT 0.002 (1.594)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.094 (30.094)	mem 63.217
Train: [6][67/750]	BT 0.211 (1.733)	DT 0.038 (1.571)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.156 (32.156)	mem 63.213
Train: [6][68/750]	BT 0.097 (1.708)	DT 0.007 (1.548)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.953 (29.953)	mem 63.239
Train: [6][69/750]	BT 0.112 (1.685)	DT 0.009 (1.526)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.688 (26.688)	mem 63.000
Train: [6][70/750]	BT 0.159 (1.664)	DT 0.003 (1.504)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 41.328 (41.328)	mem 63.036
Train: [6][71/750]	BT 0.161 (1.642)	DT 0.002 (1.483)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.484 (26.484)	mem 63.087
Train: [6][72/750]	BT 0.146 (1.622)	DT 0.002 (1.462)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.516 (35.516)	mem 63.106
Train: [6][73/750]	BT 0.186 (1.602)	DT 0.011 (1.442)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.406 (32.406)	mem 63.221
Train: [6][74/750]	BT 0.307 (1.584)	DT 0.086 (1.424)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.000 (35.000)	mem 63.172
Train: [6][75/750]	BT 0.237 (1.566)	DT 0.003 (1.405)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.234 (32.234)	mem 63.209
Train: [6][76/750]	BT 11.572 (1.698)	DT 11.456 (1.537)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.797 (33.797)	mem 58.545
Train: [6][77/750]	BT 0.143 (1.678)	DT 0.012 (1.517)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.188 (33.188)	mem 58.568
Train: [6][78/750]	BT 0.088 (1.658)	DT 0.001 (1.498)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.328 (32.328)	mem 58.611
Train: [6][79/750]	BT 0.143 (1.638)	DT 0.003 (1.479)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.172 (31.172)	mem 58.631
Train: [6][80/750]	BT 0.166 (1.620)	DT 0.013 (1.461)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.484 (32.484)	mem 58.637
Train: [6][81/750]	BT 0.101 (1.601)	DT 0.002 (1.443)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.969 (31.969)	mem 58.582
Train: [6][82/750]	BT 0.199 (1.584)	DT 0.002 (1.425)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.297 (36.297)	mem 58.591
Train: [6][83/750]	BT 0.114 (1.566)	DT 0.003 (1.408)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.438 (30.438)	mem 58.615
Train: [6][84/750]	BT 0.199 (1.550)	DT 0.002 (1.391)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.562 (39.562)	mem 58.640
Train: [6][85/750]	BT 0.104 (1.533)	DT 0.017 (1.375)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.938 (28.938)	mem 58.667
Train: [6][86/750]	BT 1.307 (1.530)	DT 1.190 (1.373)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.219 (32.219)	mem 58.859
Train: [6][87/750]	BT 0.122 (1.514)	DT 0.002 (1.357)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.172 (30.172)	mem 58.975
Train: [6][88/750]	BT 11.092 (1.623)	DT 11.012 (1.467)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.188 (28.188)	mem 61.265
Train: [6][89/750]	BT 0.085 (1.606)	DT 0.001 (1.450)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.359 (31.359)	mem 61.283
Train: [6][90/750]	BT 0.166 (1.590)	DT 0.002 (1.434)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.906 (34.906)	mem 61.321
Train: [6][91/750]	BT 0.135 (1.574)	DT 0.010 (1.419)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.453 (32.453)	mem 61.355
Train: [6][92/750]	BT 0.181 (1.559)	DT 0.006 (1.403)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.609 (31.609)	mem 61.387
Train: [6][93/750]	BT 0.092 (1.543)	DT 0.002 (1.388)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.891 (33.891)	mem 61.452
Train: [6][94/750]	BT 0.142 (1.528)	DT 0.003 (1.373)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.359 (34.359)	mem 61.530
Train: [6][95/750]	BT 0.265 (1.515)	DT 0.008 (1.359)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.875 (34.875)	mem 61.757
Train: [6][96/750]	BT 0.257 (1.502)	DT 0.002 (1.345)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.156 (31.156)	mem 61.571
Train: [6][97/750]	BT 0.145 (1.488)	DT 0.002 (1.331)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.719 (31.719)	mem 61.607
Train: [6][98/750]	BT 2.231 (1.495)	DT 2.101 (1.339)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.156 (33.156)	mem 61.938
Train: [6][99/750]	BT 0.241 (1.483)	DT 0.015 (1.326)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.281 (35.281)	mem 62.030
Train: [6][100/750]	BT 10.140 (1.569)	DT 10.016 (1.412)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.875 (29.875)	mem 64.188
Train: [6][101/750]	BT 0.117 (1.555)	DT 0.011 (1.399)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.938 (31.938)	mem 64.169
Train: [6][102/750]	BT 0.086 (1.540)	DT 0.001 (1.385)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.688 (34.688)	mem 64.179
Train: [6][103/750]	BT 0.153 (1.527)	DT 0.002 (1.371)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.500 (30.500)	mem 64.215
Train: [6][104/750]	BT 0.112 (1.513)	DT 0.015 (1.358)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.656 (35.656)	mem 64.211
Train: [6][105/750]	BT 0.161 (1.500)	DT 0.010 (1.346)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.156 (26.156)	mem 64.248
Train: [6][106/750]	BT 0.159 (1.488)	DT 0.001 (1.333)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.172 (36.172)	mem 64.211
Train: [6][107/750]	BT 0.158 (1.475)	DT 0.001 (1.320)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.625 (33.625)	mem 64.262
Train: [6][108/750]	BT 0.087 (1.462)	DT 0.001 (1.308)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.422 (35.422)	mem 64.278
Train: [6][109/750]	BT 0.194 (1.451)	DT 0.002 (1.296)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.125 (31.125)	mem 64.468
Train: [6][110/750]	BT 1.914 (1.455)	DT 1.607 (1.299)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.750 (28.750)	mem 64.694
Train: [6][111/750]	BT 0.406 (1.446)	DT 0.006 (1.287)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.938 (30.938)	mem 64.416
Train: [6][112/750]	BT 13.426 (1.553)	DT 13.253 (1.394)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.922 (36.922)	mem 64.422
Train: [6][113/750]	BT 0.096 (1.540)	DT 0.002 (1.382)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.188 (30.188)	mem 64.423
Train: [6][114/750]	BT 0.179 (1.528)	DT 0.001 (1.370)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.578 (34.578)	mem 64.378
Train: [6][115/750]	BT 0.192 (1.516)	DT 0.005 (1.358)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.188 (29.188)	mem 64.382
Train: [6][116/750]	BT 0.140 (1.504)	DT 0.025 (1.346)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.328 (32.328)	mem 64.381
Train: [6][117/750]	BT 0.233 (1.493)	DT 0.002 (1.335)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.547 (34.547)	mem 64.418
Train: [6][118/750]	BT 0.214 (1.483)	DT 0.002 (1.324)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.625 (36.625)	mem 64.469
Train: [6][119/750]	BT 0.220 (1.472)	DT 0.002 (1.313)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.609 (29.609)	mem 64.343
Train: [6][120/750]	BT 0.223 (1.462)	DT 0.001 (1.302)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.859 (34.859)	mem 64.407
Train: [6][121/750]	BT 0.250 (1.451)	DT 0.016 (1.291)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.625 (31.625)	mem 64.351
Train: [6][122/750]	BT 0.178 (1.441)	DT 0.046 (1.281)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.344 (34.344)	mem 64.367
Train: [6][123/750]	BT 0.118 (1.430)	DT 0.002 (1.270)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.688 (31.688)	mem 64.451
Train: [6][124/750]	BT 13.410 (1.527)	DT 13.269 (1.367)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.344 (33.344)	mem 64.520
Train: [6][125/750]	BT 0.154 (1.516)	DT 0.001 (1.356)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.859 (30.859)	mem 64.520
Train: [6][126/750]	BT 0.151 (1.505)	DT 0.002 (1.346)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.984 (33.984)	mem 64.521
Train: [6][127/750]	BT 0.191 (1.495)	DT 0.005 (1.335)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.625 (31.625)	mem 64.538
Train: [6][128/750]	BT 0.107 (1.484)	DT 0.005 (1.325)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.719 (39.719)	mem 64.537
Train: [6][129/750]	BT 0.199 (1.474)	DT 0.002 (1.314)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.125 (34.125)	mem 64.539
Train: [6][130/750]	BT 0.206 (1.464)	DT 0.006 (1.304)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.875 (32.875)	mem 64.541
Train: [6][131/750]	BT 0.191 (1.454)	DT 0.002 (1.294)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.406 (30.406)	mem 64.587
Train: [6][132/750]	BT 0.138 (1.445)	DT 0.010 (1.285)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.641 (34.641)	mem 64.555
Train: [6][133/750]	BT 0.236 (1.435)	DT 0.004 (1.275)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.625 (29.625)	mem 64.688
Train: [6][134/750]	BT 0.805 (1.431)	DT 0.672 (1.270)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.016 (35.016)	mem 64.590
Train: [6][135/750]	BT 0.252 (1.422)	DT 0.007 (1.261)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.562 (32.562)	mem 64.590
Train: [6][136/750]	BT 12.235 (1.501)	DT 12.167 (1.341)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.016 (35.016)	mem 58.547
Train: [6][137/750]	BT 0.137 (1.492)	DT 0.002 (1.332)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.203 (32.203)	mem 58.561
Train: [6][138/750]	BT 0.107 (1.482)	DT 0.002 (1.322)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 39.031 (39.031)	mem 58.681
Train: [6][139/750]	BT 0.144 (1.472)	DT 0.003 (1.312)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.375 (29.375)	mem 58.599
Train: [6][140/750]	BT 0.130 (1.462)	DT 0.001 (1.303)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.062 (36.062)	mem 58.553
Train: [6][141/750]	BT 0.138 (1.453)	DT 0.005 (1.294)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.703 (29.703)	mem 58.626
Train: [6][142/750]	BT 0.128 (1.444)	DT 0.016 (1.285)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.188 (33.188)	mem 58.640
Train: [6][143/750]	BT 0.195 (1.435)	DT 0.002 (1.276)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.453 (31.453)	mem 58.693
Train: [6][144/750]	BT 0.242 (1.427)	DT 0.009 (1.267)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.812 (30.812)	mem 58.811
Train: [6][145/750]	BT 0.114 (1.418)	DT 0.017 (1.258)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.125 (32.125)	mem 58.845
Train: [6][146/750]	BT 0.117 (1.409)	DT 0.036 (1.250)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.719 (35.719)	mem 58.776
Train: [6][147/750]	BT 0.227 (1.401)	DT 0.002 (1.242)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.688 (32.688)	mem 58.857
Train: [6][148/750]	BT 15.252 (1.494)	DT 15.126 (1.335)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 37.016 (37.016)	mem 61.956
Train: [6][149/750]	BT 0.079 (1.485)	DT 0.002 (1.326)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.078 (31.078)	mem 61.975
Train: [6][150/750]	BT 0.072 (1.475)	DT 0.001 (1.318)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.125 (35.125)	mem 62.007
Train: [6][151/750]	BT 0.125 (1.466)	DT 0.001 (1.309)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.406 (36.406)	mem 62.057
Train: [6][152/750]	BT 0.142 (1.458)	DT 0.026 (1.300)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.391 (36.391)	mem 62.238
Train: [6][153/750]	BT 0.153 (1.449)	DT 0.002 (1.292)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.828 (34.828)	mem 62.242
Train: [6][154/750]	BT 0.091 (1.440)	DT 0.001 (1.284)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.172 (36.172)	mem 62.119
Train: [6][155/750]	BT 0.089 (1.432)	DT 0.002 (1.275)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.312 (32.312)	mem 62.181
Train: [6][156/750]	BT 0.109 (1.423)	DT 0.002 (1.267)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.891 (36.891)	mem 62.234
Train: [6][157/750]	BT 0.096 (1.415)	DT 0.003 (1.259)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.062 (33.062)	mem 62.247
Train: [6][158/750]	BT 0.127 (1.406)	DT 0.003 (1.251)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.047 (32.047)	mem 62.201
Train: [6][159/750]	BT 0.121 (1.398)	DT 0.003 (1.243)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.578 (33.578)	mem 62.215
Train: [6][160/750]	BT 16.127 (1.490)	DT 15.987 (1.335)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.672 (35.672)	mem 58.791
Train: [6][161/750]	BT 0.081 (1.482)	DT 0.002 (1.327)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.516 (35.516)	mem 58.822
Train: [6][162/750]	BT 0.108 (1.473)	DT 0.002 (1.319)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.953 (31.953)	mem 58.844
Train: [6][163/750]	BT 0.098 (1.465)	DT 0.001 (1.311)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.969 (28.969)	mem 58.927
Train: [6][164/750]	BT 0.109 (1.456)	DT 0.006 (1.303)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.734 (31.734)	mem 58.837
Train: [6][165/750]	BT 0.148 (1.449)	DT 0.006 (1.295)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.266 (33.266)	mem 58.878
Train: [6][166/750]	BT 0.110 (1.440)	DT 0.004 (1.287)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.812 (29.812)	mem 58.892
Train: [6][167/750]	BT 0.091 (1.432)	DT 0.003 (1.280)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.391 (34.391)	mem 58.910
Train: [6][168/750]	BT 0.084 (1.424)	DT 0.003 (1.272)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.953 (33.953)	mem 58.932
Train: [6][169/750]	BT 0.090 (1.416)	DT 0.002 (1.264)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.453 (29.453)	mem 58.947
Train: [6][170/750]	BT 0.089 (1.409)	DT 0.003 (1.257)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.047 (31.047)	mem 58.960
Train: [6][171/750]	BT 0.233 (1.402)	DT 0.025 (1.250)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.250 (29.250)	mem 59.003
Train: [6][172/750]	BT 9.492 (1.449)	DT 9.402 (1.297)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 33.453 (33.453)	mem 60.874
Train: [6][173/750]	BT 0.189 (1.442)	DT 0.002 (1.290)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.594 (32.594)	mem 61.017
Train: [6][174/750]	BT 0.109 (1.434)	DT 0.002 (1.282)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.094 (32.094)	mem 60.966
Train: [6][175/750]	BT 0.186 (1.427)	DT 0.002 (1.275)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 36.938 (36.938)	mem 60.790
Train: [6][176/750]	BT 0.128 (1.419)	DT 0.002 (1.268)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.531 (35.531)	mem 60.810
Train: [6][177/750]	BT 0.179 (1.412)	DT 0.003 (1.261)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.797 (32.797)	mem 60.843
Train: [6][178/750]	BT 0.098 (1.405)	DT 0.002 (1.254)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.672 (34.672)	mem 60.868
Train: [6][179/750]	BT 0.197 (1.398)	DT 0.003 (1.247)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.172 (32.172)	mem 60.923
Train: [6][180/750]	BT 0.116 (1.391)	DT 0.014 (1.240)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.641 (35.641)	mem 61.074
Train: [6][181/750]	BT 0.203 (1.385)	DT 0.002 (1.233)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.703 (30.703)	mem 61.051
Train: [6][182/750]	BT 0.165 (1.378)	DT 0.002 (1.226)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.109 (32.109)	mem 61.021
Train: [6][183/750]	BT 0.163 (1.371)	DT 0.004 (1.219)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.188 (28.188)	mem 61.094
Train: [6][184/750]	BT 12.221 (1.430)	DT 12.066 (1.278)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 32.703 (32.703)	mem 63.130
Train: [6][185/750]	BT 0.122 (1.423)	DT 0.009 (1.272)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 28.781 (28.781)	mem 63.028
Train: [6][186/750]	BT 0.106 (1.416)	DT 0.002 (1.265)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.672 (30.672)	mem 63.118
Train: [6][187/750]	BT 0.120 (1.409)	DT 0.002 (1.258)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.875 (30.875)	mem 63.141
Train: [6][188/750]	BT 0.101 (1.402)	DT 0.011 (1.251)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 34.156 (34.156)	mem 63.171
Train: [6][189/750]	BT 0.101 (1.395)	DT 0.002 (1.245)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 26.031 (26.031)	mem 63.205
Train: [6][190/750]	BT 0.124 (1.389)	DT 0.002 (1.238)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 31.578 (31.578)	mem 63.279
Train: [6][191/750]	BT 0.101 (1.382)	DT 0.002 (1.232)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 30.109 (30.109)	mem 63.299
Train: [6][192/750]	BT 0.127 (1.375)	DT 0.002 (1.225)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 38.016 (38.016)	mem 63.318
Train: [6][193/750]	BT 0.152 (1.369)	DT 0.010 (1.219)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.219 (35.219)	mem 63.367
Train: [6][194/750]	BT 0.202 (1.363)	DT 0.005 (1.213)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 35.969 (35.969)	mem 63.509
Train: [6][195/750]	BT 0.279 (1.357)	DT 0.004 (1.207)	loss nan (nan)	gnorm nan (nan)	prob nan (nan)	GS 29.734 (29.734)	mem 63.490
Traceback (most recent call last):
  File "/home/shakir/simplical_complices_gcc/train.py", line 953, in <module>
    main(args)
  File "/home/shakir/simplical_complices_gcc/train.py", line 837, in main
    loss = train_moco(
  File "/home/shakir/simplical_complices_gcc/train.py", line 447, in train_moco
    for idx, batch in enumerate(train_loader):
  File "/home/shakir/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/shakir/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1316, in _next_data
    idx, data = self._get_data()
  File "/home/shakir/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1282, in _get_data
    success, data = self._try_get_data()
  File "/home/shakir/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1120, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/usr/local/anaconda3/lib/python3.9/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
  File "/usr/local/anaconda3/lib/python3.9/multiprocessing/connection.py", line 262, in poll
    return self._poll(timeout)
  File "/usr/local/anaconda3/lib/python3.9/multiprocessing/connection.py", line 429, in _poll
    r = wait([self], timeout)
  File "/usr/local/anaconda3/lib/python3.9/multiprocessing/connection.py", line 936, in wait
    ready = selector.select(timeout)
  File "/usr/local/anaconda3/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
