nohup: ignoring input
Graph(num_nodes=20, num_edges=292,
      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)})
cuda:1
Namespace(print_freq=1, tb_freq=1, save_freq=1, batch_size=32, num_workers=12, num_copies=6, num_samples=2000, epochs=100, optimizer='adam', learning_rate=0.005, lr_decay_epochs=[120, 160, 200], lr_decay_rate=0.0, beta1=0.9, beta2=0.999, weight_decay=1e-05, momentum=0.9, clip_norm=1.0, resume='', aug='1st', exp='Pretrain', dataset='dgl', model='gin', num_layer=5, readout='avg', set2set_lstm_layer=3, set2set_iter=6, norm=True, nce_k=16384, nce_t=0.07, rw_hops=256, subgraph_size=128, restart_prob=0.8, hidden_size=64, positional_embedding_size=32, max_node_freq=16, max_edge_freq=16, max_degree=512, freq_embedding_size=16, degree_embedding_size=16, model_path='saved', tb_path='tensorboard', load_path=None, moco=True, finetune=False, alpha=0.999, gpu=3, seed=0, fold_idx=0, cv=False, cvrun=-1, positional_embedding_multi=1, model_name='Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_multi_1_momentum_0.999', model_folder='saved/Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_multi_1_momentum_0.999', tb_folder='tensorboard/Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_multi_1_momentum_0.999')
Pretrain_moco_True_dgl_gin_layer_5_lr_0.005_decay_1e-05_bsz_32_hid_64_samples_2000_nce_t_0.07_nce_k_16384_rw_hops_256_restart_prob_0.8_aug_1st_ft_False_deg_16_pos_32_multi_1_momentum_0.999
Use GPU: 3 for training
setting random seeds
before construct dataset 25.005939483642578
load graph done
before construct dataloader 25.005939483642578
before training 25.005939483642578
using queue shape: (16384,64)
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [0][1/750]	BT 29.711 (29.711)	DT 28.383 (28.383)	loss 6.036 (6.036)	gnorm 112.197 (112.197)	prob 5.234 (5.2337)	GS 33.891 (33.891)	mem 64.921
Train: [0][2/750]	BT 0.276 (14.993)	DT 0.165 (14.274)	loss 11.005 (11.005)	gnorm 5327.108 (5327.108)	prob 0.284 (0.2836)	GS 35.516 (35.516)	mem 65.027
Train: [0][3/750]	BT 0.060 (10.015)	DT 0.002 (9.517)	loss 10.290 (10.290)	gnorm 3567433.000 (3567433.000)	prob 0.059 (0.0591)	GS 32.812 (32.812)	mem 65.043
Train: [0][4/750]	BT 0.141 (7.547)	DT 0.021 (7.143)	loss 9.673 (9.673)	gnorm 3084371.000 (3084371.000)	prob 0.031 (0.0314)	GS 34.562 (34.562)	mem 65.049
Train: [0][5/750]	BT 0.150 (6.067)	DT 0.003 (5.715)	loss 9.646 (9.646)	gnorm 3161319.500 (3161319.500)	prob 0.058 (0.0584)	GS 29.391 (29.391)	mem 65.033
Train: [0][6/750]	BT 0.088 (5.071)	DT 0.029 (4.767)	loss 9.622 (9.622)	gnorm 3650993.500 (3650993.500)	prob 0.084 (0.0838)	GS 33.047 (33.047)	mem 65.059
Train: [0][7/750]	BT 0.034 (4.351)	DT 0.002 (4.086)	loss 9.603 (9.603)	gnorm 3173223.250 (3173223.250)	prob 0.103 (0.1026)	GS 27.406 (27.406)	mem 65.079
Train: [0][8/750]	BT 0.035 (3.812)	DT 0.001 (3.576)	loss 9.583 (9.583)	gnorm 3384656.000 (3384656.000)	prob 0.123 (0.1234)	GS 34.688 (34.688)	mem 65.110
Train: [0][9/750]	BT 0.037 (3.392)	DT 0.002 (3.179)	loss 9.539 (9.539)	gnorm 3350352.250 (3350352.250)	prob 0.169 (0.1688)	GS 27.188 (27.188)	mem 65.197
Train: [0][10/750]	BT 0.114 (3.065)	DT 0.005 (2.861)	loss 9.554 (9.554)	gnorm 3267582.000 (3267582.000)	prob 0.155 (0.1552)	GS 34.375 (34.375)	mem 65.358
Train: [0][11/750]	BT 0.109 (2.796)	DT 0.040 (2.605)	loss 9.491 (9.491)	gnorm 8143327.000 (8143327.000)	prob 0.222 (0.2219)	GS 38.734 (38.734)	mem 65.234
Train: [0][12/750]	BT 0.088 (2.570)	DT 0.007 (2.388)	loss 9.541 (9.541)	gnorm 3015807.250 (3015807.250)	prob 0.169 (0.1690)	GS 34.812 (34.812)	mem 65.260
Train: [0][13/750]	BT 9.046 (3.068)	DT 8.978 (2.895)	loss 9.496 (9.496)	gnorm 3313681.500 (3313681.500)	prob 0.216 (0.2157)	GS 31.359 (31.359)	mem 65.863
Train: [0][14/750]	BT 3.980 (3.133)	DT 3.889 (2.966)	loss 9.458 (9.458)	gnorm 3435715.750 (3435715.750)	prob 0.257 (0.2571)	GS 34.344 (34.344)	mem 65.938
Train: [0][15/750]	BT 0.066 (2.929)	DT 0.002 (2.769)	loss 9.455 (9.455)	gnorm 3360325.500 (3360325.500)	prob 0.260 (0.2600)	GS 32.312 (32.312)	mem 65.938
Train: [0][16/750]	BT 0.505 (2.777)	DT 0.473 (2.625)	loss 9.414 (9.414)	gnorm 3680696.500 (3680696.500)	prob 0.305 (0.3051)	GS 33.281 (33.281)	mem 65.951
Train: [0][17/750]	BT 0.035 (2.616)	DT 0.004 (2.471)	loss 9.408 (9.408)	gnorm 3189509.750 (3189509.750)	prob 0.312 (0.3118)	GS 31.766 (31.766)	mem 65.960
Train: [0][18/750]	BT 0.094 (2.476)	DT 0.012 (2.334)	loss 9.378 (9.378)	gnorm 4084906.500 (4084906.500)	prob 0.343 (0.3432)	GS 34.250 (34.250)	mem 65.985
Train: [0][19/750]	BT 0.072 (2.350)	DT 0.014 (2.212)	loss 9.398 (9.398)	gnorm 3342635.500 (3342635.500)	prob 0.324 (0.3243)	GS 32.875 (32.875)	mem 65.986
Train: [0][20/750]	BT 0.036 (2.234)	DT 0.002 (2.102)	loss 9.382 (9.382)	gnorm 3243505.500 (3243505.500)	prob 0.342 (0.3422)	GS 29.203 (29.203)	mem 65.950
Train: [0][21/750]	BT 0.047 (2.130)	DT 0.001 (2.002)	loss 9.354 (9.354)	gnorm 5687396.500 (5687396.500)	prob 0.378 (0.3782)	GS 38.594 (38.594)	mem 66.000
Train: [0][22/750]	BT 0.082 (2.037)	DT 0.005 (1.911)	loss 9.367 (9.367)	gnorm 3387830.500 (3387830.500)	prob 0.360 (0.3603)	GS 31.781 (31.781)	mem 66.001
Train: [0][23/750]	BT 0.060 (1.951)	DT 0.008 (1.828)	loss 9.300 (9.300)	gnorm 3122083.750 (3122083.750)	prob 0.436 (0.4356)	GS 30.672 (30.672)	mem 65.952
Train: [0][24/750]	BT 0.480 (1.889)	DT 0.403 (1.769)	loss 9.253 (9.253)	gnorm 2999440.000 (2999440.000)	prob 0.484 (0.4843)	GS 34.688 (34.688)	mem 66.041
Train: [0][25/750]	BT 4.735 (2.003)	DT 4.564 (1.881)	loss 9.274 (9.274)	gnorm 3107432.000 (3107432.000)	prob 0.467 (0.4668)	GS 32.938 (32.938)	mem 66.053
Train: [0][26/750]	BT 14.821 (2.496)	DT 14.782 (2.377)	loss 9.315 (9.315)	gnorm 3300408.750 (3300408.750)	prob 0.425 (0.4255)	GS 34.078 (34.078)	mem 60.477
Train: [0][27/750]	BT 0.099 (2.407)	DT 0.007 (2.289)	loss 9.255 (9.255)	gnorm 3212901.250 (3212901.250)	prob 0.490 (0.4895)	GS 28.859 (28.859)	mem 60.489
Train: [0][28/750]	BT 0.063 (2.324)	DT 0.027 (2.208)	loss 9.288 (9.288)	gnorm 2929423.500 (2929423.500)	prob 0.461 (0.4614)	GS 36.125 (36.125)	mem 60.499
Train: [0][29/750]	BT 0.039 (2.245)	DT 0.003 (2.132)	loss 9.218 (9.218)	gnorm 3218917.000 (3218917.000)	prob 0.539 (0.5388)	GS 31.406 (31.406)	mem 60.507
Train: [0][30/750]	BT 0.033 (2.171)	DT 0.003 (2.061)	loss 9.238 (9.238)	gnorm 3196484.750 (3196484.750)	prob 0.519 (0.5195)	GS 33.938 (33.938)	mem 60.514
Train: [0][31/750]	BT 0.037 (2.102)	DT 0.001 (1.995)	loss 9.136 (9.136)	gnorm 2965265.000 (2965265.000)	prob 0.627 (0.6271)	GS 31.234 (31.234)	mem 60.526
Train: [0][32/750]	BT 0.059 (2.039)	DT 0.004 (1.933)	loss 9.172 (9.172)	gnorm 3158072.000 (3158072.000)	prob 0.598 (0.5981)	GS 31.734 (31.734)	mem 60.534
Train: [0][33/750]	BT 0.057 (1.978)	DT 0.003 (1.874)	loss 9.103 (9.103)	gnorm 3480424.500 (3480424.500)	prob 0.675 (0.6749)	GS 32.453 (32.453)	mem 60.540
Train: [0][34/750]	BT 0.033 (1.921)	DT 0.004 (1.819)	loss 9.099 (9.099)	gnorm 3067456.500 (3067456.500)	prob 0.680 (0.6795)	GS 34.672 (34.672)	mem 60.550
Train: [0][35/750]	BT 0.039 (1.867)	DT 0.001 (1.767)	loss 9.097 (9.097)	gnorm 2940922.000 (2940922.000)	prob 0.686 (0.6863)	GS 28.734 (28.734)	mem 60.566
Train: [0][36/750]	BT 0.032 (1.816)	DT 0.002 (1.718)	loss 9.102 (9.102)	gnorm 2873510.500 (2873510.500)	prob 0.683 (0.6832)	GS 32.188 (32.188)	mem 60.575
Train: [0][37/750]	BT 0.032 (1.768)	DT 0.002 (1.672)	loss 9.025 (9.025)	gnorm 3042701.250 (3042701.250)	prob 0.772 (0.7722)	GS 33.359 (33.359)	mem 60.580
Train: [0][38/750]	BT 20.468 (2.260)	DT 20.422 (2.165)	loss 9.040 (9.040)	gnorm 3160760.250 (3160760.250)	prob 0.764 (0.7639)	GS 35.047 (35.047)	mem 64.602
Train: [0][39/750]	BT 0.039 (2.203)	DT 0.002 (2.110)	loss 9.171 (9.171)	gnorm 9924624.000 (9924624.000)	prob 0.681 (0.6809)	GS 43.594 (43.594)	mem 64.648
Train: [0][40/750]	BT 1.389 (2.183)	DT 1.351 (2.091)	loss 9.167 (9.167)	gnorm 3216946.000 (3216946.000)	prob 0.637 (0.6368)	GS 37.219 (37.219)	mem 64.848
Train: [0][41/750]	BT 0.046 (2.131)	DT 0.006 (2.040)	loss 9.062 (9.062)	gnorm 2893894.750 (2893894.750)	prob 0.747 (0.7466)	GS 32.359 (32.359)	mem 64.852
Train: [0][42/750]	BT 0.034 (2.081)	DT 0.002 (1.991)	loss 9.009 (9.009)	gnorm 3010155.500 (3010155.500)	prob 0.808 (0.8078)	GS 32.547 (32.547)	mem 64.852
Train: [0][43/750]	BT 0.075 (2.034)	DT 0.002 (1.945)	loss 9.030 (9.030)	gnorm 2667918.750 (2667918.750)	prob 0.793 (0.7929)	GS 28.781 (28.781)	mem 64.858
Train: [0][44/750]	BT 0.075 (1.990)	DT 0.016 (1.901)	loss 9.027 (9.027)	gnorm 3190202.000 (3190202.000)	prob 0.810 (0.8105)	GS 34.781 (34.781)	mem 64.862
Train: [0][45/750]	BT 0.140 (1.949)	DT 0.037 (1.860)	loss 8.989 (8.989)	gnorm 2764310.000 (2764310.000)	prob 0.853 (0.8535)	GS 33.109 (33.109)	mem 64.920
Train: [0][46/750]	BT 0.065 (1.908)	DT 0.013 (1.820)	loss 9.080 (9.080)	gnorm 3561812.500 (3561812.500)	prob 0.762 (0.7617)	GS 37.156 (37.156)	mem 64.878
Train: [0][47/750]	BT 0.060 (1.868)	DT 0.019 (1.781)	loss 8.977 (8.977)	gnorm 2970845.250 (2970845.250)	prob 0.886 (0.8862)	GS 35.328 (35.328)	mem 64.883
Train: [0][48/750]	BT 0.038 (1.830)	DT 0.002 (1.744)	loss 8.886 (8.886)	gnorm 2930163.500 (2930163.500)	prob 0.981 (0.9812)	GS 32.156 (32.156)	mem 64.895
Train: [0][49/750]	BT 0.045 (1.794)	DT 0.003 (1.709)	loss 8.906 (8.906)	gnorm 2678617.000 (2678617.000)	prob 0.962 (0.9619)	GS 30.328 (30.328)	mem 64.908
Train: [0][50/750]	BT 15.970 (2.077)	DT 15.897 (1.992)	loss 8.996 (8.996)	gnorm 2909453.500 (2909453.500)	prob 0.878 (0.8776)	GS 33.875 (33.875)	mem 61.425
Train: [0][51/750]	BT 0.120 (2.039)	DT 0.016 (1.954)	loss 8.979 (8.979)	gnorm 4753318.000 (4753318.000)	prob 0.976 (0.9763)	GS 37.938 (37.938)	mem 61.420
Train: [0][52/750]	BT 0.043 (2.001)	DT 0.013 (1.916)	loss 8.938 (8.938)	gnorm 2834674.750 (2834674.750)	prob 0.938 (0.9381)	GS 33.031 (33.031)	mem 61.430
Train: [0][53/750]	BT 0.037 (1.964)	DT 0.002 (1.880)	loss 8.880 (8.880)	gnorm 2607365.750 (2607365.750)	prob 1.017 (1.0173)	GS 27.266 (27.266)	mem 61.391
Train: [0][54/750]	BT 0.078 (1.929)	DT 0.002 (1.845)	loss 8.908 (8.908)	gnorm 3068837.250 (3068837.250)	prob 0.988 (0.9878)	GS 33.969 (33.969)	mem 61.425
Train: [0][55/750]	BT 0.034 (1.894)	DT 0.002 (1.812)	loss 8.814 (8.814)	gnorm 2868115.500 (2868115.500)	prob 1.103 (1.1033)	GS 30.953 (30.953)	mem 61.442
Train: [0][56/750]	BT 0.033 (1.861)	DT 0.002 (1.780)	loss 9.048 (9.048)	gnorm 2842911.500 (2842911.500)	prob 0.869 (0.8694)	GS 34.891 (34.891)	mem 61.459
Train: [0][57/750]	BT 0.073 (1.830)	DT 0.017 (1.749)	loss 8.841 (8.841)	gnorm 2664074.500 (2664074.500)	prob 1.074 (1.0739)	GS 30.203 (30.203)	mem 61.477
Train: [0][58/750]	BT 0.070 (1.799)	DT 0.010 (1.719)	loss 8.961 (8.961)	gnorm 2710132.500 (2710132.500)	prob 0.982 (0.9816)	GS 32.750 (32.750)	mem 61.490
Train: [0][59/750]	BT 0.036 (1.769)	DT 0.002 (1.690)	loss 8.836 (8.836)	gnorm 2554751.000 (2554751.000)	prob 1.127 (1.1273)	GS 27.953 (27.953)	mem 61.496
Train: [0][60/750]	BT 0.048 (1.741)	DT 0.014 (1.662)	loss 8.816 (8.816)	gnorm 2713861.500 (2713861.500)	prob 1.154 (1.1541)	GS 35.500 (35.500)	mem 61.545
Train: [0][61/750]	BT 0.056 (1.713)	DT 0.007 (1.635)	loss 8.774 (8.774)	gnorm 2648946.000 (2648946.000)	prob 1.200 (1.1999)	GS 32.328 (32.328)	mem 61.555
Train: [0][62/750]	BT 15.807 (1.940)	DT 15.761 (1.862)	loss 8.830 (8.830)	gnorm 2852696.750 (2852696.750)	prob 1.155 (1.1549)	GS 34.469 (34.469)	mem 63.877
Train: [0][63/750]	BT 0.032 (1.910)	DT 0.002 (1.833)	loss 8.830 (8.830)	gnorm 2547601.500 (2547601.500)	prob 1.139 (1.1394)	GS 33.969 (33.969)	mem 63.877
Train: [0][64/750]	BT 0.029 (1.881)	DT 0.002 (1.804)	loss 8.798 (8.798)	gnorm 2925018.750 (2925018.750)	prob 1.182 (1.1815)	GS 29.422 (29.422)	mem 63.885
Train: [0][65/750]	BT 0.025 (1.852)	DT 0.001 (1.777)	loss 8.641 (8.641)	gnorm 2498354.250 (2498354.250)	prob 1.346 (1.3462)	GS 28.500 (28.500)	mem 63.925
Train: [0][66/750]	BT 0.031 (1.825)	DT 0.001 (1.750)	loss 8.961 (8.961)	gnorm 2641920.250 (2641920.250)	prob 1.036 (1.0355)	GS 32.984 (32.984)	mem 63.993
Train: [0][67/750]	BT 0.045 (1.798)	DT 0.002 (1.724)	loss 8.690 (8.690)	gnorm 2525575.000 (2525575.000)	prob 1.354 (1.3541)	GS 26.453 (26.453)	mem 64.033
Train: [0][68/750]	BT 0.060 (1.772)	DT 0.008 (1.698)	loss 8.907 (8.907)	gnorm 2508571.500 (2508571.500)	prob 1.135 (1.1354)	GS 32.469 (32.469)	mem 63.948
Train: [0][69/750]	BT 0.046 (1.747)	DT 0.005 (1.674)	loss 8.769 (8.769)	gnorm 2515351.000 (2515351.000)	prob 1.287 (1.2874)	GS 30.531 (30.531)	mem 63.963
Train: [0][70/750]	BT 0.034 (1.723)	DT 0.002 (1.650)	loss 8.975 (8.975)	gnorm 2720645.000 (2720645.000)	prob 1.091 (1.0906)	GS 31.391 (31.391)	mem 63.971
Train: [0][71/750]	BT 0.049 (1.699)	DT 0.001 (1.627)	loss 8.689 (8.689)	gnorm 2706046.250 (2706046.250)	prob 1.372 (1.3722)	GS 34.250 (34.250)	mem 64.018
Train: [0][72/750]	BT 0.043 (1.676)	DT 0.002 (1.604)	loss 8.901 (8.901)	gnorm 2387038.250 (2387038.250)	prob 1.170 (1.1704)	GS 33.906 (33.906)	mem 64.053
Train: [0][73/750]	BT 0.057 (1.654)	DT 0.004 (1.582)	loss 8.664 (8.664)	gnorm 2539771.000 (2539771.000)	prob 1.442 (1.4418)	GS 33.078 (33.078)	mem 64.019
Train: [0][74/750]	BT 15.330 (1.839)	DT 15.253 (1.767)	loss 8.884 (8.884)	gnorm 2690553.500 (2690553.500)	prob 1.199 (1.1988)	GS 35.359 (35.359)	mem 65.824
Train: [0][75/750]	BT 0.034 (1.815)	DT 0.008 (1.743)	loss 8.664 (8.664)	gnorm 2361979.500 (2361979.500)	prob 1.450 (1.4502)	GS 30.984 (30.984)	mem 65.826
Train: [0][76/750]	BT 0.050 (1.792)	DT 0.001 (1.721)	loss 8.896 (8.896)	gnorm 2484118.250 (2484118.250)	prob 1.205 (1.2045)	GS 37.844 (37.844)	mem 65.830
Train: [0][77/750]	BT 0.048 (1.769)	DT 0.002 (1.698)	loss 8.770 (8.770)	gnorm 2303119.250 (2303119.250)	prob 1.367 (1.3671)	GS 28.750 (28.750)	mem 65.835
Train: [0][78/750]	BT 0.048 (1.747)	DT 0.002 (1.677)	loss 8.796 (8.796)	gnorm 3173880.500 (3173880.500)	prob 1.353 (1.3535)	GS 34.312 (34.312)	mem 65.838
Train: [0][79/750]	BT 0.082 (1.726)	DT 0.002 (1.655)	loss 8.801 (8.801)	gnorm 2238846.000 (2238846.000)	prob 1.303 (1.3026)	GS 31.172 (31.172)	mem 65.844
Train: [0][80/750]	BT 0.052 (1.705)	DT 0.003 (1.635)	loss 8.825 (8.825)	gnorm 2548159.750 (2548159.750)	prob 1.326 (1.3259)	GS 33.109 (33.109)	mem 65.847
Train: [0][81/750]	BT 0.047 (1.685)	DT 0.017 (1.615)	loss 9.039 (9.039)	gnorm 5570500.500 (5570500.500)	prob 1.110 (1.1100)	GS 35.672 (35.672)	mem 65.848
Train: [0][82/750]	BT 0.045 (1.665)	DT 0.001 (1.595)	loss 8.960 (8.960)	gnorm 2425164.750 (2425164.750)	prob 1.137 (1.1366)	GS 31.266 (31.266)	mem 65.849
Train: [0][83/750]	BT 0.035 (1.645)	DT 0.002 (1.576)	loss 8.713 (8.713)	gnorm 2490010.500 (2490010.500)	prob 1.401 (1.4012)	GS 35.250 (35.250)	mem 65.849
Train: [0][84/750]	BT 0.056 (1.626)	DT 0.003 (1.557)	loss 8.870 (8.870)	gnorm 2694243.250 (2694243.250)	prob 1.262 (1.2623)	GS 35.250 (35.250)	mem 65.885
Train: [0][85/750]	BT 0.064 (1.608)	DT 0.007 (1.539)	loss 8.721 (8.721)	gnorm 2588759.500 (2588759.500)	prob 1.433 (1.4331)	GS 29.797 (29.797)	mem 65.855
Train: [0][86/750]	BT 13.262 (1.743)	DT 13.217 (1.675)	loss 8.769 (8.769)	gnorm 2753770.000 (2753770.000)	prob 1.430 (1.4304)	GS 35.109 (35.109)	mem 66.262
Train: [0][87/750]	BT 0.040 (1.724)	DT 0.001 (1.655)	loss 8.898 (8.898)	gnorm 2778576.000 (2778576.000)	prob 1.240 (1.2399)	GS 34.078 (34.078)	mem 66.301
Train: [0][88/750]	BT 0.031 (1.704)	DT 0.001 (1.637)	loss 8.868 (8.868)	gnorm 2602957.750 (2602957.750)	prob 1.302 (1.3025)	GS 32.828 (32.828)	mem 66.384
Train: [0][89/750]	BT 0.098 (1.686)	DT 0.015 (1.618)	loss 8.800 (8.800)	gnorm 2313798.750 (2313798.750)	prob 1.373 (1.3735)	GS 28.156 (28.156)	mem 66.357
Train: [0][90/750]	BT 0.065 (1.668)	DT 0.003 (1.600)	loss 8.954 (8.954)	gnorm 2606200.000 (2606200.000)	prob 1.266 (1.2659)	GS 33.875 (33.875)	mem 66.276
Train: [0][91/750]	BT 0.029 (1.650)	DT 0.002 (1.583)	loss 8.833 (8.833)	gnorm 2298363.000 (2298363.000)	prob 1.370 (1.3701)	GS 28.047 (28.047)	mem 66.277
Train: [0][92/750]	BT 0.055 (1.633)	DT 0.002 (1.566)	loss 8.892 (8.892)	gnorm 2538143.500 (2538143.500)	prob 1.306 (1.3059)	GS 34.000 (34.000)	mem 66.225
Train: [0][93/750]	BT 0.069 (1.616)	DT 0.002 (1.549)	loss 8.852 (8.852)	gnorm 2197532.750 (2197532.750)	prob 1.353 (1.3535)	GS 31.766 (31.766)	mem 66.227
Train: [0][94/750]	BT 0.050 (1.599)	DT 0.005 (1.532)	loss 8.782 (8.782)	gnorm 2594299.000 (2594299.000)	prob 1.419 (1.4188)	GS 34.125 (34.125)	mem 66.228
Train: [0][95/750]	BT 0.053 (1.583)	DT 0.004 (1.516)	loss 8.793 (8.793)	gnorm 2322083.750 (2322083.750)	prob 1.424 (1.4235)	GS 28.562 (28.562)	mem 66.230
Train: [0][96/750]	BT 0.067 (1.567)	DT 0.004 (1.501)	loss 8.744 (8.744)	gnorm 2327244.750 (2327244.750)	prob 1.513 (1.5128)	GS 39.234 (39.234)	mem 66.231
Train: [0][97/750]	BT 0.049 (1.552)	DT 0.004 (1.485)	loss 8.713 (8.713)	gnorm 3598676.000 (3598676.000)	prob 1.613 (1.6130)	GS 30.438 (30.438)	mem 66.233
Train: [0][98/750]	BT 10.001 (1.638)	DT 9.944 (1.571)	loss 8.846 (8.846)	gnorm 2177100.750 (2177100.750)	prob 1.353 (1.3534)	GS 29.266 (29.266)	mem 66.643
Train: [0][99/750]	BT 0.115 (1.623)	DT 0.010 (1.556)	loss 8.739 (8.739)	gnorm 2176980.250 (2176980.250)	prob 1.543 (1.5426)	GS 32.172 (32.172)	mem 66.643
Train: [0][100/750]	BT 4.425 (1.651)	DT 4.366 (1.584)	loss 8.896 (8.896)	gnorm 2512833.000 (2512833.000)	prob 1.395 (1.3948)	GS 35.875 (35.875)	mem 66.698
Train: [0][101/750]	BT 0.088 (1.635)	DT 0.011 (1.568)	loss 8.825 (8.825)	gnorm 3820949.000 (3820949.000)	prob 1.505 (1.5052)	GS 34.891 (34.891)	mem 66.613
Train: [0][102/750]	BT 0.063 (1.620)	DT 0.004 (1.553)	loss 8.810 (8.810)	gnorm 2492634.750 (2492634.750)	prob 1.433 (1.4335)	GS 31.922 (31.922)	mem 66.656
Train: [0][103/750]	BT 0.093 (1.605)	DT 0.004 (1.538)	loss 8.713 (8.713)	gnorm 2135444.250 (2135444.250)	prob 1.607 (1.6069)	GS 30.422 (30.422)	mem 66.658
Train: [0][104/750]	BT 0.042 (1.590)	DT 0.003 (1.523)	loss 8.811 (8.811)	gnorm 2425648.500 (2425648.500)	prob 1.487 (1.4871)	GS 36.375 (36.375)	mem 66.661
Train: [0][105/750]	BT 0.035 (1.575)	DT 0.002 (1.509)	loss 8.682 (8.682)	gnorm 2291038.000 (2291038.000)	prob 1.687 (1.6868)	GS 31.625 (31.625)	mem 66.661
Train: [0][106/750]	BT 0.038 (1.561)	DT 0.002 (1.494)	loss 8.785 (8.785)	gnorm 2318725.000 (2318725.000)	prob 1.546 (1.5463)	GS 27.656 (27.656)	mem 66.589
Train: [0][107/750]	BT 0.052 (1.546)	DT 0.002 (1.480)	loss 8.647 (8.647)	gnorm 2321703.250 (2321703.250)	prob 1.754 (1.7539)	GS 35.047 (35.047)	mem 66.591
Train: [0][108/750]	BT 0.041 (1.532)	DT 0.004 (1.467)	loss 8.819 (8.819)	gnorm 2459392.250 (2459392.250)	prob 1.600 (1.5995)	GS 32.453 (32.453)	mem 66.589
Train: [0][109/750]	BT 0.062 (1.519)	DT 0.002 (1.453)	loss 8.878 (8.878)	gnorm 2137682.000 (2137682.000)	prob 1.495 (1.4945)	GS 31.969 (31.969)	mem 66.590
Train: [0][110/750]	BT 10.904 (1.604)	DT 10.857 (1.539)	loss 8.892 (8.892)	gnorm 2432579.500 (2432579.500)	prob 1.502 (1.5021)	GS 32.953 (32.953)	mem 60.465
Train: [0][111/750]	BT 0.068 (1.590)	DT 0.017 (1.525)	loss 8.717 (8.717)	gnorm 2356179.500 (2356179.500)	prob 1.659 (1.6587)	GS 29.797 (29.797)	mem 60.473
Train: [0][112/750]	BT 5.920 (1.629)	DT 5.875 (1.564)	loss 8.836 (8.836)	gnorm 2517776.500 (2517776.500)	prob 1.551 (1.5507)	GS 33.828 (33.828)	mem 60.827
Train: [0][113/750]	BT 0.043 (1.615)	DT 0.005 (1.550)	loss 8.798 (8.798)	gnorm 2467466.750 (2467466.750)	prob 1.555 (1.5551)	GS 30.625 (30.625)	mem 60.778
Train: [0][114/750]	BT 0.033 (1.601)	DT 0.002 (1.537)	loss 8.932 (8.932)	gnorm 2169558.000 (2169558.000)	prob 1.420 (1.4205)	GS 32.375 (32.375)	mem 60.788
Train: [0][115/750]	BT 0.034 (1.588)	DT 0.003 (1.523)	loss 8.694 (8.694)	gnorm 2304289.000 (2304289.000)	prob 1.725 (1.7254)	GS 34.375 (34.375)	mem 60.794
Train: [0][116/750]	BT 0.034 (1.574)	DT 0.003 (1.510)	loss 8.744 (8.744)	gnorm 2095261.875 (2095261.875)	prob 1.698 (1.6978)	GS 32.953 (32.953)	mem 60.803
Train: [0][117/750]	BT 0.034 (1.561)	DT 0.002 (1.497)	loss 8.763 (8.763)	gnorm 2113061.000 (2113061.000)	prob 1.648 (1.6477)	GS 30.516 (30.516)	mem 60.807
Train: [0][118/750]	BT 0.035 (1.548)	DT 0.002 (1.485)	loss 8.857 (8.857)	gnorm 2526597.000 (2526597.000)	prob 1.590 (1.5898)	GS 32.469 (32.469)	mem 60.821
Train: [0][119/750]	BT 0.031 (1.535)	DT 0.003 (1.472)	loss 8.861 (8.861)	gnorm 2376522.000 (2376522.000)	prob 1.652 (1.6518)	GS 36.844 (36.844)	mem 60.842
Train: [0][120/750]	BT 0.041 (1.523)	DT 0.007 (1.460)	loss 8.966 (8.966)	gnorm 2441627.750 (2441627.750)	prob 1.470 (1.4695)	GS 34.250 (34.250)	mem 60.862
Train: [0][121/750]	BT 0.036 (1.511)	DT 0.004 (1.448)	loss 8.712 (8.712)	gnorm 2205150.500 (2205150.500)	prob 1.769 (1.7691)	GS 32.453 (32.453)	mem 60.871
Train: [0][122/750]	BT 5.887 (1.546)	DT 5.824 (1.484)	loss 8.855 (8.855)	gnorm 2075443.375 (2075443.375)	prob 1.604 (1.6040)	GS 34.359 (34.359)	mem 61.976
Train: [0][123/750]	BT 0.131 (1.535)	DT 0.041 (1.472)	loss 8.976 (8.976)	gnorm 2466684.750 (2466684.750)	prob 1.462 (1.4620)	GS 32.188 (32.188)	mem 61.923
Train: [0][124/750]	BT 7.893 (1.586)	DT 7.835 (1.523)	loss 9.076 (9.076)	gnorm 2337441.250 (2337441.250)	prob 1.350 (1.3503)	GS 36.156 (36.156)	mem 63.535
Train: [0][125/750]	BT 0.094 (1.574)	DT 0.012 (1.511)	loss 8.855 (8.855)	gnorm 2254977.750 (2254977.750)	prob 1.532 (1.5319)	GS 29.469 (29.469)	mem 63.555
Train: [0][126/750]	BT 0.043 (1.562)	DT 0.015 (1.499)	loss 8.941 (8.941)	gnorm 2311564.250 (2311564.250)	prob 1.502 (1.5024)	GS 37.188 (37.188)	mem 63.570
Train: [0][127/750]	BT 0.030 (1.550)	DT 0.001 (1.488)	loss 8.904 (8.904)	gnorm 2190353.750 (2190353.750)	prob 1.502 (1.5024)	GS 29.016 (29.016)	mem 63.583
Train: [0][128/750]	BT 0.052 (1.538)	DT 0.002 (1.476)	loss 8.869 (8.869)	gnorm 2231988.250 (2231988.250)	prob 1.589 (1.5895)	GS 34.062 (34.062)	mem 63.602
Train: [0][129/750]	BT 0.050 (1.527)	DT 0.002 (1.465)	loss 8.774 (8.774)	gnorm 2147824.000 (2147824.000)	prob 1.671 (1.6708)	GS 29.016 (29.016)	mem 63.613
Train: [0][130/750]	BT 0.034 (1.515)	DT 0.003 (1.453)	loss 9.009 (9.009)	gnorm 2420860.250 (2420860.250)	prob 1.444 (1.4436)	GS 34.594 (34.594)	mem 63.620
Train: [0][131/750]	BT 0.035 (1.504)	DT 0.003 (1.442)	loss 8.839 (8.839)	gnorm 2363986.250 (2363986.250)	prob 1.644 (1.6442)	GS 31.734 (31.734)	mem 63.624
Train: [0][132/750]	BT 0.057 (1.493)	DT 0.001 (1.431)	loss 9.056 (9.056)	gnorm 2101161.250 (2101161.250)	prob 1.374 (1.3742)	GS 34.594 (34.594)	mem 63.672
Train: [0][133/750]	BT 0.120 (1.483)	DT 0.004 (1.421)	loss 8.695 (8.695)	gnorm 2118600.500 (2118600.500)	prob 1.796 (1.7964)	GS 29.328 (29.328)	mem 63.750
Train: [0][134/750]	BT 5.495 (1.513)	DT 5.397 (1.450)	loss 9.128 (9.128)	gnorm 2514640.250 (2514640.250)	prob 1.433 (1.4330)	GS 31.359 (31.359)	mem 64.736
Train: [0][135/750]	BT 0.060 (1.502)	DT 0.005 (1.440)	loss 9.033 (9.033)	gnorm 2248125.250 (2248125.250)	prob 1.481 (1.4811)	GS 27.250 (27.250)	mem 64.759
Train: [0][136/750]	BT 5.141 (1.529)	DT 5.105 (1.466)	loss 8.773 (8.773)	gnorm 2300672.500 (2300672.500)	prob 1.693 (1.6933)	GS 33.594 (33.594)	mem 65.442
Train: [0][137/750]	BT 0.067 (1.518)	DT 0.002 (1.456)	loss 8.897 (8.897)	gnorm 2100336.500 (2100336.500)	prob 1.536 (1.5355)	GS 32.188 (32.188)	mem 65.449
Train: [0][138/750]	BT 0.076 (1.508)	DT 0.013 (1.445)	loss 9.014 (9.014)	gnorm 2520788.750 (2520788.750)	prob 1.458 (1.4577)	GS 36.000 (36.000)	mem 65.455
Train: [0][139/750]	BT 0.036 (1.497)	DT 0.003 (1.435)	loss 8.861 (8.861)	gnorm 2090260.250 (2090260.250)	prob 1.732 (1.7323)	GS 24.391 (24.391)	mem 65.457
Train: [0][140/750]	BT 0.041 (1.487)	DT 0.006 (1.425)	loss 9.032 (9.032)	gnorm 2224352.000 (2224352.000)	prob 1.460 (1.4603)	GS 34.062 (34.062)	mem 65.488
Train: [0][141/750]	BT 0.069 (1.477)	DT 0.003 (1.415)	loss 8.924 (8.924)	gnorm 2239040.250 (2239040.250)	prob 1.644 (1.6442)	GS 32.656 (32.656)	mem 65.464
Train: [0][142/750]	BT 0.045 (1.466)	DT 0.003 (1.405)	loss 9.076 (9.076)	gnorm 2096404.125 (2096404.125)	prob 1.433 (1.4334)	GS 32.250 (32.250)	mem 65.498
Train: [0][143/750]	BT 0.056 (1.457)	DT 0.003 (1.395)	loss 8.884 (8.884)	gnorm 2035420.375 (2035420.375)	prob 1.699 (1.6991)	GS 29.516 (29.516)	mem 65.504
Train: [0][144/750]	BT 0.114 (1.447)	DT 0.007 (1.385)	loss 8.981 (8.981)	gnorm 2124311.000 (2124311.000)	prob 1.564 (1.5639)	GS 32.781 (32.781)	mem 65.503
Train: [0][145/750]	BT 0.063 (1.438)	DT 0.013 (1.376)	loss 8.805 (8.805)	gnorm 2280239.750 (2280239.750)	prob 1.758 (1.7578)	GS 33.609 (33.609)	mem 65.477
Train: [0][146/750]	BT 12.559 (1.514)	DT 12.519 (1.452)	loss 9.116 (9.116)	gnorm 2537021.250 (2537021.250)	prob 1.424 (1.4245)	GS 30.344 (30.344)	mem 61.060
Train: [0][147/750]	BT 0.043 (1.504)	DT 0.002 (1.442)	loss 8.913 (8.913)	gnorm 2779763.250 (2779763.250)	prob 1.617 (1.6169)	GS 34.188 (34.188)	mem 61.086
Train: [0][148/750]	BT 1.837 (1.506)	DT 1.798 (1.445)	loss 8.819 (8.819)	gnorm 2350486.750 (2350486.750)	prob 1.758 (1.7578)	GS 35.078 (35.078)	mem 61.301
Train: [0][149/750]	BT 0.042 (1.496)	DT 0.002 (1.435)	loss 8.835 (8.835)	gnorm 2003787.125 (2003787.125)	prob 1.743 (1.7435)	GS 32.266 (32.266)	mem 61.315
Train: [0][150/750]	BT 0.037 (1.487)	DT 0.006 (1.425)	loss 9.037 (9.037)	gnorm 2529462.500 (2529462.500)	prob 1.498 (1.4976)	GS 33.719 (33.719)	mem 61.325
Train: [0][151/750]	BT 0.034 (1.477)	DT 0.002 (1.416)	loss 8.955 (8.955)	gnorm 2024480.250 (2024480.250)	prob 1.644 (1.6436)	GS 32.062 (32.062)	mem 61.382
Train: [0][152/750]	BT 0.080 (1.468)	DT 0.002 (1.407)	loss 8.863 (8.863)	gnorm 2156511.750 (2156511.750)	prob 1.753 (1.7529)	GS 34.406 (34.406)	mem 61.529
Train: [0][153/750]	BT 0.092 (1.459)	DT 0.005 (1.398)	loss 8.944 (8.944)	gnorm 2045798.000 (2045798.000)	prob 1.799 (1.7994)	GS 28.453 (28.453)	mem 61.508
Train: [0][154/750]	BT 0.058 (1.450)	DT 0.009 (1.389)	loss 9.164 (9.164)	gnorm 2539030.000 (2539030.000)	prob 1.475 (1.4751)	GS 35.281 (35.281)	mem 61.523
Train: [0][155/750]	BT 0.060 (1.441)	DT 0.028 (1.380)	loss 9.104 (9.104)	gnorm 2287414.250 (2287414.250)	prob 1.488 (1.4878)	GS 30.938 (30.938)	mem 61.369
Train: [0][156/750]	BT 0.044 (1.432)	DT 0.002 (1.371)	loss 9.224 (9.224)	gnorm 3366947.750 (3366947.750)	prob 1.290 (1.2895)	GS 36.141 (36.141)	mem 61.377
Train: [0][157/750]	BT 0.054 (1.423)	DT 0.002 (1.362)	loss 8.969 (8.969)	gnorm 2015572.250 (2015572.250)	prob 1.571 (1.5712)	GS 29.000 (29.000)	mem 61.389
Train: [0][158/750]	BT 11.799 (1.489)	DT 11.750 (1.428)	loss 9.136 (9.136)	gnorm 2409580.250 (2409580.250)	prob 1.388 (1.3882)	GS 29.328 (29.328)	mem 63.662
Train: [0][159/750]	BT 0.094 (1.480)	DT 0.026 (1.419)	loss 8.970 (8.970)	gnorm 2254914.500 (2254914.500)	prob 1.525 (1.5251)	GS 31.625 (31.625)	mem 63.589
Train: [0][160/750]	BT 0.568 (1.474)	DT 0.525 (1.414)	loss 9.038 (9.038)	gnorm 2247605.000 (2247605.000)	prob 1.476 (1.4760)	GS 32.297 (32.297)	mem 63.842
Train: [0][161/750]	BT 0.076 (1.466)	DT 0.017 (1.405)	loss 9.052 (9.052)	gnorm 2163075.750 (2163075.750)	prob 1.633 (1.6328)	GS 26.391 (26.391)	mem 63.858
Train: [0][162/750]	BT 0.086 (1.457)	DT 0.019 (1.396)	loss 8.895 (8.895)	gnorm 2280837.250 (2280837.250)	prob 1.650 (1.6501)	GS 35.859 (35.859)	mem 63.743
Train: [0][163/750]	BT 0.041 (1.448)	DT 0.002 (1.388)	loss 8.888 (8.888)	gnorm 2249604.250 (2249604.250)	prob 1.765 (1.7648)	GS 32.688 (32.688)	mem 63.755
Train: [0][164/750]	BT 0.045 (1.440)	DT 0.003 (1.379)	loss 9.166 (9.166)	gnorm 2473788.000 (2473788.000)	prob 1.406 (1.4058)	GS 35.828 (35.828)	mem 63.767
Train: [0][165/750]	BT 0.074 (1.431)	DT 0.018 (1.371)	loss 8.984 (8.984)	gnorm 3306780.750 (3306780.750)	prob 1.674 (1.6738)	GS 42.828 (42.828)	mem 63.788
Train: [0][166/750]	BT 0.083 (1.423)	DT 0.005 (1.363)	loss 9.012 (9.012)	gnorm 2156005.250 (2156005.250)	prob 1.681 (1.6810)	GS 36.453 (36.453)	mem 63.806
Train: [0][167/750]	BT 0.072 (1.415)	DT 0.006 (1.355)	loss 9.011 (9.011)	gnorm 2766474.750 (2766474.750)	prob 1.632 (1.6320)	GS 31.516 (31.516)	mem 63.839
Train: [0][168/750]	BT 0.089 (1.407)	DT 0.011 (1.347)	loss 9.150 (9.150)	gnorm 2284037.750 (2284037.750)	prob 1.423 (1.4228)	GS 31.953 (31.953)	mem 63.888
Train: [0][169/750]	BT 0.077 (1.400)	DT 0.023 (1.339)	loss 9.023 (9.023)	gnorm 2213538.750 (2213538.750)	prob 1.621 (1.6210)	GS 35.250 (35.250)	mem 63.901
Train: [0][170/750]	BT 12.825 (1.467)	DT 12.781 (1.406)	loss 9.096 (9.096)	gnorm 2300798.000 (2300798.000)	prob 1.504 (1.5043)	GS 34.156 (34.156)	mem 66.313
Train: [0][171/750]	BT 0.056 (1.458)	DT 0.005 (1.398)	loss 9.047 (9.047)	gnorm 2222767.000 (2222767.000)	prob 1.566 (1.5659)	GS 33.000 (33.000)	mem 66.246
Train: [0][172/750]	BT 0.042 (1.450)	DT 0.012 (1.390)	loss 9.180 (9.180)	gnorm 2136580.000 (2136580.000)	prob 1.445 (1.4449)	GS 33.703 (33.703)	mem 66.262
Train: [0][173/750]	BT 0.035 (1.442)	DT 0.002 (1.382)	loss 8.975 (8.975)	gnorm 2212754.750 (2212754.750)	prob 1.687 (1.6870)	GS 30.031 (30.031)	mem 66.278
Train: [0][174/750]	BT 0.649 (1.437)	DT 0.617 (1.378)	loss 9.158 (9.158)	gnorm 2458110.250 (2458110.250)	prob 1.577 (1.5769)	GS 36.016 (36.016)	mem 66.317
Train: [0][175/750]	BT 0.076 (1.430)	DT 0.012 (1.370)	loss 8.955 (8.955)	gnorm 2172454.750 (2172454.750)	prob 1.694 (1.6936)	GS 31.766 (31.766)	mem 66.325
Train: [0][176/750]	BT 0.050 (1.422)	DT 0.002 (1.362)	loss 9.046 (9.046)	gnorm 2229820.500 (2229820.500)	prob 1.576 (1.5757)	GS 35.609 (35.609)	mem 66.330
Train: [0][177/750]	BT 0.053 (1.414)	DT 0.007 (1.354)	loss 8.999 (8.999)	gnorm 2058163.000 (2058163.000)	prob 1.715 (1.7146)	GS 31.016 (31.016)	mem 66.341
Train: [0][178/750]	BT 0.050 (1.406)	DT 0.012 (1.347)	loss 9.185 (9.185)	gnorm 2473343.000 (2473343.000)	prob 1.547 (1.5466)	GS 36.031 (36.031)	mem 66.350
Train: [0][179/750]	BT 0.035 (1.399)	DT 0.002 (1.339)	loss 8.965 (8.965)	gnorm 2074551.750 (2074551.750)	prob 1.711 (1.7114)	GS 30.516 (30.516)	mem 66.351
Train: [0][180/750]	BT 0.034 (1.391)	DT 0.002 (1.332)	loss 9.151 (9.151)	gnorm 2656131.250 (2656131.250)	prob 1.571 (1.5713)	GS 36.250 (36.250)	mem 66.352
Train: [0][181/750]	BT 0.037 (1.384)	DT 0.004 (1.324)	loss 9.115 (9.115)	gnorm 1930698.625 (1930698.625)	prob 1.567 (1.5675)	GS 31.406 (31.406)	mem 66.354
Train: [0][182/750]	BT 11.041 (1.437)	DT 10.995 (1.378)	loss 9.075 (9.075)	gnorm 2299070.750 (2299070.750)	prob 1.589 (1.5890)	GS 33.891 (33.891)	mem 65.685
Train: [0][183/750]	BT 0.055 (1.429)	DT 0.001 (1.370)	loss 9.047 (9.047)	gnorm 2269000.000 (2269000.000)	prob 1.727 (1.7270)	GS 32.438 (32.438)	mem 65.686
Train: [0][184/750]	BT 3.396 (1.440)	DT 3.363 (1.381)	loss 9.141 (9.141)	gnorm 2206404.250 (2206404.250)	prob 1.571 (1.5707)	GS 37.609 (37.609)	mem 65.700
Train: [0][185/750]	BT 0.050 (1.432)	DT 0.008 (1.373)	loss 8.906 (8.906)	gnorm 1874403.500 (1874403.500)	prob 1.753 (1.7526)	GS 25.281 (25.281)	mem 65.700
Train: [0][186/750]	BT 0.060 (1.425)	DT 0.001 (1.366)	loss 9.236 (9.236)	gnorm 2358441.000 (2358441.000)	prob 1.449 (1.4488)	GS 33.953 (33.953)	mem 65.701
Train: [0][187/750]	BT 0.063 (1.418)	DT 0.002 (1.359)	loss 9.209 (9.209)	gnorm 2139017.500 (2139017.500)	prob 1.550 (1.5496)	GS 28.203 (28.203)	mem 65.702
Train: [0][188/750]	BT 0.154 (1.411)	DT 0.007 (1.352)	loss 9.221 (9.221)	gnorm 2238460.250 (2238460.250)	prob 1.358 (1.3579)	GS 32.438 (32.438)	mem 65.703
Train: [0][189/750]	BT 0.142 (1.404)	DT 0.028 (1.345)	loss 8.793 (8.793)	gnorm 1773642.000 (1773642.000)	prob 1.934 (1.9340)	GS 28.453 (28.453)	mem 65.703
Train: [0][190/750]	BT 0.060 (1.397)	DT 0.004 (1.338)	loss 8.977 (8.977)	gnorm 2161613.750 (2161613.750)	prob 1.813 (1.8132)	GS 35.234 (35.234)	mem 65.728
Train: [0][191/750]	BT 0.098 (1.390)	DT 0.009 (1.331)	loss 9.242 (9.242)	gnorm 2418416.500 (2418416.500)	prob 1.568 (1.5682)	GS 30.953 (30.953)	mem 65.742
Train: [0][192/750]	BT 0.069 (1.384)	DT 0.003 (1.324)	loss 9.119 (9.119)	gnorm 2340155.000 (2340155.000)	prob 1.571 (1.5707)	GS 33.578 (33.578)	mem 65.845
Train: [0][193/750]	BT 0.075 (1.377)	DT 0.003 (1.317)	loss 8.939 (8.939)	gnorm 2042346.000 (2042346.000)	prob 1.901 (1.9015)	GS 34.859 (34.859)	mem 65.844
Train: [0][194/750]	BT 7.572 (1.409)	DT 7.530 (1.349)	loss 9.420 (9.420)	gnorm 2129293.000 (2129293.000)	prob 1.303 (1.3034)	GS 31.156 (31.156)	mem 65.752
Train: [0][195/750]	BT 0.040 (1.402)	DT 0.001 (1.342)	loss 9.032 (9.032)	gnorm 1959474.250 (1959474.250)	prob 1.648 (1.6477)	GS 30.844 (30.844)	mem 65.750
Train: [0][196/750]	BT 7.267 (1.432)	DT 7.126 (1.371)	loss 9.151 (9.151)	gnorm 2144090.000 (2144090.000)	prob 1.581 (1.5807)	GS 28.859 (28.859)	mem 65.820
Train: [0][197/750]	BT 0.185 (1.425)	DT 0.027 (1.365)	loss 9.209 (9.209)	gnorm 1853548.250 (1853548.250)	prob 1.474 (1.4739)	GS 28.219 (28.219)	mem 65.821
Train: [0][198/750]	BT 1.436 (1.425)	DT 1.369 (1.365)	loss 9.350 (9.350)	gnorm 3045747.000 (3045747.000)	prob 1.549 (1.5492)	GS 36.000 (36.000)	mem 65.804
Train: [0][199/750]	BT 0.112 (1.419)	DT 0.004 (1.358)	loss 8.966 (8.966)	gnorm 2417698.500 (2417698.500)	prob 1.912 (1.9121)	GS 29.203 (29.203)	mem 65.845
Train: [0][200/750]	BT 0.058 (1.412)	DT 0.010 (1.351)	loss 9.149 (9.149)	gnorm 2423678.250 (2423678.250)	prob 1.706 (1.7062)	GS 32.938 (32.938)	mem 65.846
Train: [0][201/750]	BT 0.073 (1.405)	DT 0.004 (1.344)	loss 9.130 (9.130)	gnorm 2133189.250 (2133189.250)	prob 1.640 (1.6405)	GS 31.047 (31.047)	mem 65.808
Train: [0][202/750]	BT 0.180 (1.399)	DT 0.029 (1.338)	loss 9.107 (9.107)	gnorm 1874604.250 (1874604.250)	prob 1.700 (1.6997)	GS 32.625 (32.625)	mem 65.810
Train: [0][203/750]	BT 0.105 (1.393)	DT 0.002 (1.331)	loss 9.227 (9.227)	gnorm 2259148.250 (2259148.250)	prob 1.621 (1.6207)	GS 29.844 (29.844)	mem 65.812
Train: [0][204/750]	BT 0.162 (1.387)	DT 0.002 (1.325)	loss 9.260 (9.260)	gnorm 2865834.000 (2865834.000)	prob 1.457 (1.4574)	GS 34.641 (34.641)	mem 65.813
Train: [0][205/750]	BT 0.079 (1.380)	DT 0.004 (1.318)	loss 9.126 (9.126)	gnorm 2077161.250 (2077161.250)	prob 1.626 (1.6259)	GS 29.609 (29.609)	mem 65.830
Train: [0][206/750]	BT 7.175 (1.409)	DT 7.060 (1.346)	loss 9.155 (9.155)	gnorm 2467165.500 (2467165.500)	prob 1.625 (1.6254)	GS 32.734 (32.734)	mem 65.936
Train: [0][207/750]	BT 0.096 (1.402)	DT 0.024 (1.340)	loss 9.382 (9.382)	gnorm 2343652.250 (2343652.250)	prob 1.396 (1.3964)	GS 30.688 (30.688)	mem 65.938
Train: [0][208/750]	BT 9.734 (1.442)	DT 9.667 (1.380)	loss 9.162 (9.162)	gnorm 2350285.500 (2350285.500)	prob 1.574 (1.5735)	GS 35.875 (35.875)	mem 59.680
Train: [0][209/750]	BT 0.097 (1.436)	DT 0.028 (1.373)	loss 9.001 (9.001)	gnorm 2234492.500 (2234492.500)	prob 1.710 (1.7098)	GS 29.188 (29.188)	mem 59.704
Train: [0][210/750]	BT 0.630 (1.432)	DT 0.520 (1.369)	loss 9.208 (9.208)	gnorm 2198602.250 (2198602.250)	prob 1.562 (1.5621)	GS 35.984 (35.984)	mem 59.805
Train: [0][211/750]	BT 0.094 (1.426)	DT 0.011 (1.363)	loss 9.293 (9.293)	gnorm 2038259.250 (2038259.250)	prob 1.505 (1.5050)	GS 32.641 (32.641)	mem 59.813
Train: [0][212/750]	BT 0.082 (1.419)	DT 0.013 (1.357)	loss 9.288 (9.288)	gnorm 2402051.250 (2402051.250)	prob 1.366 (1.3663)	GS 35.078 (35.078)	mem 59.825
Train: [0][213/750]	BT 0.080 (1.413)	DT 0.014 (1.350)	loss 9.289 (9.289)	gnorm 2051829.625 (2051829.625)	prob 1.458 (1.4577)	GS 30.047 (30.047)	mem 59.839
Train: [0][214/750]	BT 0.086 (1.407)	DT 0.007 (1.344)	loss 9.203 (9.203)	gnorm 2291938.250 (2291938.250)	prob 1.424 (1.4237)	GS 37.797 (37.797)	mem 59.842
Train: [0][215/750]	BT 0.080 (1.401)	DT 0.005 (1.338)	loss 9.175 (9.175)	gnorm 3007459.500 (3007459.500)	prob 1.749 (1.7485)	GS 46.469 (46.469)	mem 59.855
Train: [0][216/750]	BT 0.088 (1.395)	DT 0.010 (1.332)	loss 9.204 (9.204)	gnorm 2278061.250 (2278061.250)	prob 1.567 (1.5673)	GS 29.938 (29.938)	mem 59.877
Train: [0][217/750]	BT 0.157 (1.389)	DT 0.044 (1.326)	loss 9.279 (9.279)	gnorm 2787377.500 (2787377.500)	prob 1.597 (1.5973)	GS 39.578 (39.578)	mem 59.887
Train: [0][218/750]	BT 3.163 (1.397)	DT 3.083 (1.334)	loss 9.344 (9.344)	gnorm 2222913.000 (2222913.000)	prob 1.295 (1.2952)	GS 31.781 (31.781)	mem 60.548
Train: [0][219/750]	BT 0.100 (1.391)	DT 0.010 (1.328)	loss 9.312 (9.312)	gnorm 2198949.000 (2198949.000)	prob 1.537 (1.5366)	GS 30.875 (30.875)	mem 60.558
Train: [0][220/750]	BT 9.837 (1.429)	DT 9.799 (1.366)	loss 9.265 (9.265)	gnorm 2006561.500 (2006561.500)	prob 1.498 (1.4975)	GS 32.266 (32.266)	mem 62.182
Train: [0][221/750]	BT 0.054 (1.423)	DT 0.002 (1.360)	loss 9.053 (9.053)	gnorm 1909429.375 (1909429.375)	prob 1.785 (1.7853)	GS 26.531 (26.531)	mem 62.196
Train: [0][222/750]	BT 0.154 (1.418)	DT 0.120 (1.354)	loss 9.236 (9.236)	gnorm 2374520.750 (2374520.750)	prob 1.559 (1.5588)	GS 32.922 (32.922)	mem 62.150
Train: [0][223/750]	BT 0.035 (1.411)	DT 0.002 (1.348)	loss 9.279 (9.279)	gnorm 2426129.250 (2426129.250)	prob 1.554 (1.5536)	GS 35.016 (35.016)	mem 62.146
Train: [0][224/750]	BT 0.058 (1.405)	DT 0.002 (1.342)	loss 9.400 (9.400)	gnorm 2237231.000 (2237231.000)	prob 1.313 (1.3130)	GS 30.359 (30.359)	mem 62.156
Train: [0][225/750]	BT 0.058 (1.399)	DT 0.013 (1.336)	loss 9.136 (9.136)	gnorm 2155389.250 (2155389.250)	prob 1.700 (1.6996)	GS 31.312 (31.312)	mem 62.172
Train: [0][226/750]	BT 0.049 (1.393)	DT 0.008 (1.331)	loss 9.088 (9.088)	gnorm 2483648.000 (2483648.000)	prob 1.630 (1.6301)	GS 35.875 (35.875)	mem 62.186
Train: [0][227/750]	BT 0.129 (1.388)	DT 0.006 (1.325)	loss 9.410 (9.410)	gnorm 3094197.750 (3094197.750)	prob 1.478 (1.4783)	GS 46.125 (46.125)	mem 62.211
Train: [0][228/750]	BT 0.062 (1.382)	DT 0.012 (1.319)	loss 9.383 (9.383)	gnorm 3116047.500 (3116047.500)	prob 1.426 (1.4264)	GS 36.734 (36.734)	mem 62.264
Train: [0][229/750]	BT 0.036 (1.376)	DT 0.004 (1.313)	loss 9.164 (9.164)	gnorm 2074386.500 (2074386.500)	prob 1.493 (1.4927)	GS 26.719 (26.719)	mem 62.266
Train: [0][230/750]	BT 4.699 (1.391)	DT 4.575 (1.327)	loss 9.502 (9.502)	gnorm 2519976.000 (2519976.000)	prob 1.219 (1.2194)	GS 36.406 (36.406)	mem 63.310
Train: [0][231/750]	BT 0.087 (1.385)	DT 0.003 (1.322)	loss 9.091 (9.091)	gnorm 1894730.750 (1894730.750)	prob 1.663 (1.6634)	GS 28.188 (28.188)	mem 63.317
Train: [0][232/750]	BT 9.733 (1.421)	DT 9.704 (1.358)	loss 9.371 (9.371)	gnorm 2035651.625 (2035651.625)	prob 1.488 (1.4883)	GS 34.781 (34.781)	mem 64.890
Train: [0][233/750]	BT 0.045 (1.415)	DT 0.002 (1.352)	loss 9.152 (9.152)	gnorm 2360568.500 (2360568.500)	prob 1.727 (1.7267)	GS 34.547 (34.547)	mem 64.909
Train: [0][234/750]	BT 0.048 (1.409)	DT 0.002 (1.346)	loss 9.281 (9.281)	gnorm 2532691.750 (2532691.750)	prob 1.485 (1.4852)	GS 35.156 (35.156)	mem 64.987
Train: [0][235/750]	BT 0.035 (1.403)	DT 0.003 (1.340)	loss 9.366 (9.366)	gnorm 2679869.750 (2679869.750)	prob 1.573 (1.5726)	GS 38.047 (38.047)	mem 65.002
Train: [0][236/750]	BT 0.034 (1.397)	DT 0.003 (1.335)	loss 9.360 (9.360)	gnorm 2078691.875 (2078691.875)	prob 1.502 (1.5016)	GS 31.047 (31.047)	mem 65.067
Train: [0][237/750]	BT 0.059 (1.392)	DT 0.002 (1.329)	loss 9.183 (9.183)	gnorm 2082554.000 (2082554.000)	prob 1.665 (1.6654)	GS 30.172 (30.172)	mem 65.010
Train: [0][238/750]	BT 0.085 (1.386)	DT 0.016 (1.324)	loss 9.304 (9.304)	gnorm 2142406.250 (2142406.250)	prob 1.548 (1.5481)	GS 31.047 (31.047)	mem 65.028
Train: [0][239/750]	BT 0.046 (1.381)	DT 0.004 (1.318)	loss 9.129 (9.129)	gnorm 2353075.250 (2353075.250)	prob 1.891 (1.8906)	GS 34.516 (34.516)	mem 64.972
Train: [0][240/750]	BT 0.044 (1.375)	DT 0.012 (1.313)	loss 9.281 (9.281)	gnorm 2143563.750 (2143563.750)	prob 1.481 (1.4805)	GS 32.422 (32.422)	mem 64.975
Train: [0][241/750]	BT 0.063 (1.370)	DT 0.001 (1.307)	loss 9.556 (9.556)	gnorm 2318466.750 (2318466.750)	prob 1.356 (1.3564)	GS 34.906 (34.906)	mem 64.984
Train: [0][242/750]	BT 3.186 (1.377)	DT 3.052 (1.314)	loss 9.568 (9.568)	gnorm 2170467.250 (2170467.250)	prob 1.251 (1.2505)	GS 31.844 (31.844)	mem 65.486
Train: [0][243/750]	BT 0.054 (1.372)	DT 0.022 (1.309)	loss 9.348 (9.348)	gnorm 1924768.500 (1924768.500)	prob 1.387 (1.3874)	GS 29.391 (29.391)	mem 65.487
Train: [0][244/750]	BT 11.430 (1.413)	DT 11.395 (1.350)	loss 9.480 (9.480)	gnorm 2218405.750 (2218405.750)	prob 1.259 (1.2585)	GS 32.344 (32.344)	mem 60.836
Train: [0][245/750]	BT 0.059 (1.407)	DT 0.027 (1.345)	loss 9.349 (9.349)	gnorm 2157544.500 (2157544.500)	prob 1.497 (1.4970)	GS 30.297 (30.297)	mem 60.800
Train: [0][246/750]	BT 0.034 (1.402)	DT 0.002 (1.340)	loss 9.178 (9.178)	gnorm 2388315.750 (2388315.750)	prob 1.629 (1.6288)	GS 30.312 (30.312)	mem 60.807
Train: [0][247/750]	BT 0.030 (1.396)	DT 0.002 (1.334)	loss 9.170 (9.170)	gnorm 2065252.625 (2065252.625)	prob 1.742 (1.7425)	GS 30.125 (30.125)	mem 60.818
Train: [0][248/750]	BT 0.032 (1.391)	DT 0.001 (1.329)	loss 9.060 (9.060)	gnorm 2277650.500 (2277650.500)	prob 1.768 (1.7683)	GS 35.672 (35.672)	mem 60.865
Train: [0][249/750]	BT 0.038 (1.385)	DT 0.002 (1.324)	loss 9.133 (9.133)	gnorm 2353143.750 (2353143.750)	prob 1.757 (1.7568)	GS 34.438 (34.438)	mem 60.831
Train: [0][250/750]	BT 0.048 (1.380)	DT 0.004 (1.318)	loss 9.582 (9.582)	gnorm 2064301.125 (2064301.125)	prob 1.350 (1.3497)	GS 34.875 (34.875)	mem 60.834
Train: [0][251/750]	BT 0.034 (1.375)	DT 0.002 (1.313)	loss 9.346 (9.346)	gnorm 2599626.750 (2599626.750)	prob 1.517 (1.5174)	GS 33.141 (33.141)	mem 60.839
Train: [0][252/750]	BT 0.034 (1.369)	DT 0.003 (1.308)	loss 9.375 (9.375)	gnorm 2212321.250 (2212321.250)	prob 1.520 (1.5204)	GS 30.672 (30.672)	mem 60.845
Train: [0][253/750]	BT 0.034 (1.364)	DT 0.003 (1.303)	loss 9.328 (9.328)	gnorm 1970915.125 (1970915.125)	prob 1.521 (1.5214)	GS 30.016 (30.016)	mem 60.849
Train: [0][254/750]	BT 0.042 (1.359)	DT 0.004 (1.298)	loss 9.278 (9.278)	gnorm 2022560.875 (2022560.875)	prob 1.544 (1.5438)	GS 31.406 (31.406)	mem 60.856
Train: [0][255/750]	BT 0.048 (1.354)	DT 0.003 (1.292)	loss 9.023 (9.023)	gnorm 2209466.250 (2209466.250)	prob 1.972 (1.9717)	GS 32.422 (32.422)	mem 60.848
Train: [0][256/750]	BT 13.231 (1.400)	DT 13.118 (1.339)	loss 9.491 (9.491)	gnorm 2104478.500 (2104478.500)	prob 1.446 (1.4455)	GS 32.797 (32.797)	mem 63.144
Train: [0][257/750]	BT 0.080 (1.395)	DT 0.004 (1.333)	loss 9.546 (9.546)	gnorm 2133055.500 (2133055.500)	prob 1.310 (1.3100)	GS 30.750 (30.750)	mem 63.063
Train: [0][258/750]	BT 0.053 (1.390)	DT 0.002 (1.328)	loss 9.540 (9.540)	gnorm 2579426.500 (2579426.500)	prob 1.272 (1.2722)	GS 34.406 (34.406)	mem 63.033
Train: [0][259/750]	BT 0.066 (1.385)	DT 0.011 (1.323)	loss 9.376 (9.376)	gnorm 2118750.250 (2118750.250)	prob 1.449 (1.4494)	GS 32.500 (32.500)	mem 63.045
Train: [0][260/750]	BT 0.035 (1.379)	DT 0.004 (1.318)	loss 9.420 (9.420)	gnorm 2106256.000 (2106256.000)	prob 1.400 (1.4005)	GS 32.953 (32.953)	mem 63.053
Train: [0][261/750]	BT 0.035 (1.374)	DT 0.004 (1.313)	loss 9.250 (9.250)	gnorm 2005588.625 (2005588.625)	prob 1.523 (1.5234)	GS 32.484 (32.484)	mem 63.061
Train: [0][262/750]	BT 0.075 (1.369)	DT 0.004 (1.308)	loss 9.567 (9.567)	gnorm 2440414.500 (2440414.500)	prob 1.235 (1.2354)	GS 32.922 (32.922)	mem 63.076
Train: [0][263/750]	BT 0.134 (1.365)	DT 0.017 (1.303)	loss 9.467 (9.467)	gnorm 2158078.750 (2158078.750)	prob 1.350 (1.3503)	GS 27.578 (27.578)	mem 63.115
Train: [0][264/750]	BT 0.113 (1.360)	DT 0.009 (1.298)	loss 9.605 (9.605)	gnorm 3008624.750 (3008624.750)	prob 1.195 (1.1949)	GS 39.047 (39.047)	mem 63.152
Train: [0][265/750]	BT 0.113 (1.355)	DT 0.061 (1.294)	loss 9.290 (9.290)	gnorm 2137819.000 (2137819.000)	prob 1.524 (1.5241)	GS 30.328 (30.328)	mem 63.185
Train: [0][266/750]	BT 2.124 (1.358)	DT 2.079 (1.297)	loss 9.469 (9.469)	gnorm 2387511.750 (2387511.750)	prob 1.257 (1.2573)	GS 35.094 (35.094)	mem 63.399
Train: [0][267/750]	BT 0.060 (1.353)	DT 0.014 (1.292)	loss 9.494 (9.494)	gnorm 1966434.750 (1966434.750)	prob 1.222 (1.2221)	GS 28.094 (28.094)	mem 63.338
Train: [0][268/750]	BT 14.525 (1.402)	DT 14.495 (1.341)	loss 9.482 (9.482)	gnorm 2326327.250 (2326327.250)	prob 1.268 (1.2680)	GS 35.375 (35.375)	mem 65.352
Train: [0][269/750]	BT 0.045 (1.397)	DT 0.001 (1.336)	loss 9.223 (9.223)	gnorm 2068330.875 (2068330.875)	prob 1.496 (1.4957)	GS 28.859 (28.859)	mem 65.353
Train: [0][270/750]	BT 0.041 (1.392)	DT 0.002 (1.331)	loss 9.482 (9.482)	gnorm 2418040.750 (2418040.750)	prob 1.292 (1.2923)	GS 33.000 (33.000)	mem 65.355
Train: [0][271/750]	BT 0.031 (1.387)	DT 0.002 (1.326)	loss 9.247 (9.247)	gnorm 2012748.875 (2012748.875)	prob 1.703 (1.7027)	GS 27.969 (27.969)	mem 65.357
Train: [0][272/750]	BT 0.113 (1.383)	DT 0.025 (1.321)	loss 9.451 (9.451)	gnorm 2512673.750 (2512673.750)	prob 1.262 (1.2619)	GS 34.172 (34.172)	mem 65.361
Train: [0][273/750]	BT 0.035 (1.378)	DT 0.002 (1.317)	loss 9.552 (9.552)	gnorm 2353665.000 (2353665.000)	prob 1.283 (1.2830)	GS 29.609 (29.609)	mem 65.361
Train: [0][274/750]	BT 0.059 (1.373)	DT 0.003 (1.312)	loss 9.387 (9.387)	gnorm 2319110.250 (2319110.250)	prob 1.529 (1.5293)	GS 35.234 (35.234)	mem 65.361
Train: [0][275/750]	BT 0.065 (1.368)	DT 0.003 (1.307)	loss 9.313 (9.313)	gnorm 2042433.125 (2042433.125)	prob 1.585 (1.5854)	GS 28.266 (28.266)	mem 65.362
Train: [0][276/750]	BT 0.040 (1.363)	DT 0.004 (1.302)	loss 9.590 (9.590)	gnorm 2287114.250 (2287114.250)	prob 1.217 (1.2168)	GS 35.500 (35.500)	mem 65.382
Train: [0][277/750]	BT 0.092 (1.359)	DT 0.002 (1.298)	loss 9.313 (9.313)	gnorm 1964421.375 (1964421.375)	prob 1.518 (1.5176)	GS 34.172 (34.172)	mem 65.439
Train: [0][278/750]	BT 0.089 (1.354)	DT 0.003 (1.293)	loss 9.647 (9.647)	gnorm 2441149.500 (2441149.500)	prob 1.194 (1.1945)	GS 33.375 (33.375)	mem 65.483
Train: [0][279/750]	BT 0.062 (1.350)	DT 0.009 (1.288)	loss 9.283 (9.283)	gnorm 2163173.250 (2163173.250)	prob 1.765 (1.7650)	GS 30.516 (30.516)	mem 65.416
Train: [0][280/750]	BT 17.752 (1.408)	DT 17.709 (1.347)	loss 9.704 (9.704)	gnorm 2709338.500 (2709338.500)	prob 1.250 (1.2498)	GS 33.219 (33.219)	mem 65.566
Train: [0][281/750]	BT 0.037 (1.403)	DT 0.002 (1.342)	loss 9.251 (9.251)	gnorm 2002801.000 (2002801.000)	prob 1.532 (1.5324)	GS 29.797 (29.797)	mem 65.566
Train: [0][282/750]	BT 0.062 (1.398)	DT 0.002 (1.337)	loss 9.621 (9.621)	gnorm 2808916.000 (2808916.000)	prob 1.358 (1.3581)	GS 36.109 (36.109)	mem 65.485
Train: [0][283/750]	BT 0.038 (1.394)	DT 0.002 (1.333)	loss 9.421 (9.421)	gnorm 2648947.000 (2648947.000)	prob 1.596 (1.5961)	GS 31.797 (31.797)	mem 65.486
Train: [0][284/750]	BT 0.105 (1.389)	DT 0.024 (1.328)	loss 9.449 (9.449)	gnorm 2109950.250 (2109950.250)	prob 1.284 (1.2843)	GS 33.719 (33.719)	mem 65.547
Train: [0][285/750]	BT 0.037 (1.384)	DT 0.007 (1.323)	loss 9.370 (9.370)	gnorm 1926410.375 (1926410.375)	prob 1.503 (1.5034)	GS 29.031 (29.031)	mem 65.611
Train: [0][286/750]	BT 0.168 (1.380)	DT 0.025 (1.319)	loss 9.414 (9.414)	gnorm 2252249.500 (2252249.500)	prob 1.454 (1.4536)	GS 36.219 (36.219)	mem 65.663
Train: [0][287/750]	BT 0.108 (1.376)	DT 0.002 (1.314)	loss 9.694 (9.694)	gnorm 2058925.000 (2058925.000)	prob 1.213 (1.2133)	GS 30.797 (30.797)	mem 65.659
Train: [0][288/750]	BT 0.077 (1.371)	DT 0.003 (1.310)	loss 9.461 (9.461)	gnorm 2456316.250 (2456316.250)	prob 1.458 (1.4578)	GS 36.688 (36.688)	mem 65.659
Train: [0][289/750]	BT 0.069 (1.367)	DT 0.009 (1.305)	loss 9.331 (9.331)	gnorm 2436373.250 (2436373.250)	prob 1.549 (1.5489)	GS 30.734 (30.734)	mem 65.540
Train: [0][290/750]	BT 0.053 (1.362)	DT 0.006 (1.301)	loss 9.543 (9.543)	gnorm 2213605.500 (2213605.500)	prob 1.353 (1.3530)	GS 32.359 (32.359)	mem 65.556
Train: [0][291/750]	BT 0.138 (1.358)	DT 0.020 (1.296)	loss 9.432 (9.432)	gnorm 2124842.000 (2124842.000)	prob 1.607 (1.6070)	GS 33.172 (33.172)	mem 65.545
Train: [0][292/750]	BT 13.951 (1.401)	DT 13.908 (1.340)	loss 9.583 (9.583)	gnorm 2746966.500 (2746966.500)	prob 1.314 (1.3142)	GS 34.297 (34.297)	mem 65.432
Train: [0][293/750]	BT 0.059 (1.396)	DT 0.002 (1.335)	loss 9.635 (9.635)	gnorm 2253881.000 (2253881.000)	prob 1.333 (1.3334)	GS 36.609 (36.609)	mem 65.433
Train: [0][294/750]	BT 0.095 (1.392)	DT 0.006 (1.331)	loss 9.486 (9.486)	gnorm 2603050.750 (2603050.750)	prob 1.223 (1.2226)	GS 36.078 (36.078)	mem 65.434
Train: [0][295/750]	BT 0.065 (1.388)	DT 0.003 (1.326)	loss 9.569 (9.569)	gnorm 2067594.250 (2067594.250)	prob 1.356 (1.3558)	GS 30.141 (30.141)	mem 65.435
Train: [0][296/750]	BT 0.167 (1.383)	DT 0.004 (1.322)	loss 9.582 (9.582)	gnorm 2325036.750 (2325036.750)	prob 1.209 (1.2092)	GS 37.578 (37.578)	mem 65.435
Train: [0][297/750]	BT 0.120 (1.379)	DT 0.018 (1.317)	loss 9.520 (9.520)	gnorm 2051070.625 (2051070.625)	prob 1.173 (1.1733)	GS 30.156 (30.156)	mem 65.435
Train: [0][298/750]	BT 0.058 (1.375)	DT 0.003 (1.313)	loss 9.441 (9.441)	gnorm 2497537.500 (2497537.500)	prob 1.427 (1.4274)	GS 34.500 (34.500)	mem 65.435
Train: [0][299/750]	BT 0.124 (1.371)	DT 0.009 (1.308)	loss 9.605 (9.605)	gnorm 2750018.750 (2750018.750)	prob 1.161 (1.1612)	GS 31.938 (31.938)	mem 65.465
Train: [0][300/750]	BT 0.044 (1.366)	DT 0.004 (1.304)	loss 9.675 (9.675)	gnorm 2566023.000 (2566023.000)	prob 1.226 (1.2259)	GS 33.375 (33.375)	mem 65.465
Train: [0][301/750]	BT 0.085 (1.362)	DT 0.015 (1.300)	loss 9.556 (9.556)	gnorm 2486354.750 (2486354.750)	prob 1.296 (1.2961)	GS 43.688 (43.688)	mem 65.435
Train: [0][302/750]	BT 0.076 (1.358)	DT 0.021 (1.296)	loss 9.421 (9.421)	gnorm 2515364.250 (2515364.250)	prob 1.362 (1.3618)	GS 35.422 (35.422)	mem 65.436
Train: [0][303/750]	BT 0.061 (1.353)	DT 0.011 (1.291)	loss 9.581 (9.581)	gnorm 2148310.000 (2148310.000)	prob 1.273 (1.2732)	GS 28.125 (28.125)	mem 65.437
Train: [0][304/750]	BT 14.154 (1.395)	DT 14.063 (1.333)	loss 9.722 (9.722)	gnorm 2285004.250 (2285004.250)	prob 1.072 (1.0722)	GS 32.562 (32.562)	mem 59.326
Train: [0][305/750]	BT 0.075 (1.391)	DT 0.010 (1.329)	loss 9.481 (9.481)	gnorm 2060214.250 (2060214.250)	prob 1.342 (1.3419)	GS 31.125 (31.125)	mem 59.364
Train: [0][306/750]	BT 0.034 (1.387)	DT 0.002 (1.325)	loss 9.577 (9.577)	gnorm 3073073.250 (3073073.250)	prob 1.309 (1.3095)	GS 34.688 (34.688)	mem 59.391
Train: [0][307/750]	BT 0.028 (1.382)	DT 0.002 (1.320)	loss 9.217 (9.217)	gnorm 1992216.000 (1992216.000)	prob 1.645 (1.6447)	GS 31.438 (31.438)	mem 59.461
Train: [0][308/750]	BT 0.039 (1.378)	DT 0.001 (1.316)	loss 9.640 (9.640)	gnorm 2115903.250 (2115903.250)	prob 1.389 (1.3889)	GS 30.188 (30.188)	mem 59.485
Train: [0][309/750]	BT 0.067 (1.374)	DT 0.013 (1.312)	loss 9.569 (9.569)	gnorm 2957216.500 (2957216.500)	prob 1.393 (1.3927)	GS 37.688 (37.688)	mem 59.502
Train: [0][310/750]	BT 0.044 (1.369)	DT 0.004 (1.308)	loss 9.361 (9.361)	gnorm 2077786.250 (2077786.250)	prob 1.556 (1.5557)	GS 34.172 (34.172)	mem 59.406
Train: [0][311/750]	BT 0.033 (1.365)	DT 0.001 (1.303)	loss 9.282 (9.282)	gnorm 1838408.250 (1838408.250)	prob 1.645 (1.6453)	GS 31.109 (31.109)	mem 59.411
Train: [0][312/750]	BT 0.055 (1.361)	DT 0.003 (1.299)	loss 9.867 (9.867)	gnorm 2823997.250 (2823997.250)	prob 1.176 (1.1761)	GS 32.797 (32.797)	mem 59.421
Train: [0][313/750]	BT 0.065 (1.357)	DT 0.006 (1.295)	loss 9.502 (9.502)	gnorm 2231748.250 (2231748.250)	prob 1.351 (1.3506)	GS 28.156 (28.156)	mem 59.439
Train: [0][314/750]	BT 0.067 (1.353)	DT 0.018 (1.291)	loss 9.611 (9.611)	gnorm 2951759.250 (2951759.250)	prob 1.188 (1.1875)	GS 33.344 (33.344)	mem 59.458
Train: [0][315/750]	BT 0.052 (1.349)	DT 0.015 (1.287)	loss 9.619 (9.619)	gnorm 3049858.250 (3049858.250)	prob 1.246 (1.2464)	GS 38.578 (38.578)	mem 59.462
Train: [0][316/750]	BT 12.068 (1.382)	DT 12.041 (1.321)	loss 9.809 (9.809)	gnorm 2549655.750 (2549655.750)	prob 0.947 (0.9473)	GS 34.391 (34.391)	mem 61.617
Train: [0][317/750]	BT 0.041 (1.378)	DT 0.001 (1.317)	loss 9.508 (9.508)	gnorm 2198983.000 (2198983.000)	prob 1.427 (1.4273)	GS 35.375 (35.375)	mem 61.625
Train: [0][318/750]	BT 0.063 (1.374)	DT 0.015 (1.313)	loss 9.819 (9.819)	gnorm 2468474.500 (2468474.500)	prob 1.031 (1.0315)	GS 33.812 (33.812)	mem 61.637
Train: [0][319/750]	BT 0.042 (1.370)	DT 0.016 (1.309)	loss 9.385 (9.385)	gnorm 2159840.750 (2159840.750)	prob 1.546 (1.5462)	GS 30.906 (30.906)	mem 61.650
Train: [0][320/750]	BT 0.031 (1.366)	DT 0.001 (1.305)	loss 9.769 (9.769)	gnorm 2260432.750 (2260432.750)	prob 1.077 (1.0767)	GS 35.641 (35.641)	mem 61.717
Train: [0][321/750]	BT 0.039 (1.362)	DT 0.002 (1.301)	loss 9.445 (9.445)	gnorm 1944455.875 (1944455.875)	prob 1.527 (1.5274)	GS 32.766 (32.766)	mem 61.758
Train: [0][322/750]	BT 0.044 (1.357)	DT 0.002 (1.297)	loss 9.541 (9.541)	gnorm 2227623.750 (2227623.750)	prob 1.354 (1.3542)	GS 34.688 (34.688)	mem 61.749
Train: [0][323/750]	BT 0.084 (1.354)	DT 0.002 (1.293)	loss 9.635 (9.635)	gnorm 2026797.250 (2026797.250)	prob 1.540 (1.5396)	GS 26.766 (26.766)	mem 61.829
Train: [0][324/750]	BT 0.037 (1.349)	DT 0.002 (1.289)	loss 9.797 (9.797)	gnorm 2771830.000 (2771830.000)	prob 1.131 (1.1315)	GS 35.922 (35.922)	mem 61.935
Train: [0][325/750]	BT 0.093 (1.346)	DT 0.012 (1.285)	loss 9.556 (9.556)	gnorm 2186552.500 (2186552.500)	prob 1.452 (1.4515)	GS 34.062 (34.062)	mem 62.012
Train: [0][326/750]	BT 0.127 (1.342)	DT 0.002 (1.281)	loss 9.614 (9.614)	gnorm 2346521.000 (2346521.000)	prob 1.260 (1.2597)	GS 34.328 (34.328)	mem 62.060
Train: [0][327/750]	BT 0.081 (1.338)	DT 0.006 (1.277)	loss 9.652 (9.652)	gnorm 1988815.250 (1988815.250)	prob 1.228 (1.2283)	GS 30.047 (30.047)	mem 62.053
Train: [0][328/750]	BT 15.063 (1.380)	DT 15.031 (1.319)	loss 9.925 (9.925)	gnorm 2457124.000 (2457124.000)	prob 0.932 (0.9321)	GS 33.859 (33.859)	mem 64.580
Train: [0][329/750]	BT 0.062 (1.376)	DT 0.003 (1.315)	loss 9.605 (9.605)	gnorm 2147699.250 (2147699.250)	prob 1.396 (1.3960)	GS 26.453 (26.453)	mem 64.589
Train: [0][330/750]	BT 0.054 (1.372)	DT 0.009 (1.311)	loss 9.709 (9.709)	gnorm 2193316.000 (2193316.000)	prob 1.132 (1.1318)	GS 29.656 (29.656)	mem 64.604
Train: [0][331/750]	BT 0.038 (1.368)	DT 0.001 (1.307)	loss 9.579 (9.579)	gnorm 2133777.500 (2133777.500)	prob 1.366 (1.3662)	GS 32.750 (32.750)	mem 64.613
Train: [0][332/750]	BT 0.091 (1.364)	DT 0.026 (1.303)	loss 9.661 (9.661)	gnorm 2266692.500 (2266692.500)	prob 1.170 (1.1704)	GS 36.641 (36.641)	mem 64.628
Train: [0][333/750]	BT 0.185 (1.360)	DT 0.018 (1.299)	loss 9.384 (9.384)	gnorm 1942552.500 (1942552.500)	prob 1.664 (1.6643)	GS 28.703 (28.703)	mem 64.644
Train: [0][334/750]	BT 0.041 (1.356)	DT 0.002 (1.295)	loss 9.465 (9.465)	gnorm 1998740.125 (1998740.125)	prob 1.467 (1.4665)	GS 29.500 (29.500)	mem 64.651
Train: [0][335/750]	BT 0.144 (1.353)	DT 0.006 (1.291)	loss 9.510 (9.510)	gnorm 1883381.750 (1883381.750)	prob 1.598 (1.5982)	GS 29.062 (29.062)	mem 64.661
Train: [0][336/750]	BT 0.044 (1.349)	DT 0.013 (1.288)	loss 9.697 (9.697)	gnorm 2614084.000 (2614084.000)	prob 1.418 (1.4178)	GS 33.922 (33.922)	mem 64.669
Train: [0][337/750]	BT 0.040 (1.345)	DT 0.004 (1.284)	loss 9.568 (9.568)	gnorm 2201460.750 (2201460.750)	prob 1.563 (1.5633)	GS 31.953 (31.953)	mem 64.679
Train: [0][338/750]	BT 0.046 (1.341)	DT 0.002 (1.280)	loss 9.766 (9.766)	gnorm 2028590.375 (2028590.375)	prob 1.114 (1.1137)	GS 30.359 (30.359)	mem 64.689
Train: [0][339/750]	BT 0.040 (1.337)	DT 0.001 (1.276)	loss 9.591 (9.591)	gnorm 2153908.500 (2153908.500)	prob 1.221 (1.2215)	GS 30.266 (30.266)	mem 64.695
Train: [0][340/750]	BT 17.676 (1.385)	DT 17.570 (1.324)	loss 9.670 (9.670)	gnorm 2204532.750 (2204532.750)	prob 1.320 (1.3204)	GS 35.641 (35.641)	mem 61.339
Train: [0][341/750]	BT 0.055 (1.382)	DT 0.007 (1.320)	loss 9.660 (9.660)	gnorm 2602014.000 (2602014.000)	prob 1.348 (1.3481)	GS 37.062 (37.062)	mem 61.291
Train: [0][342/750]	BT 0.035 (1.378)	DT 0.004 (1.316)	loss 9.642 (9.642)	gnorm 2262312.500 (2262312.500)	prob 1.307 (1.3067)	GS 34.812 (34.812)	mem 61.297
Train: [0][343/750]	BT 0.067 (1.374)	DT 0.016 (1.313)	loss 9.806 (9.806)	gnorm 2374263.500 (2374263.500)	prob 1.085 (1.0850)	GS 38.000 (38.000)	mem 61.312
Train: [0][344/750]	BT 0.035 (1.370)	DT 0.002 (1.309)	loss 9.708 (9.708)	gnorm 2103565.500 (2103565.500)	prob 1.074 (1.0740)	GS 28.047 (28.047)	mem 61.318
Train: [0][345/750]	BT 0.033 (1.366)	DT 0.002 (1.305)	loss 9.466 (9.466)	gnorm 1989058.000 (1989058.000)	prob 1.523 (1.5227)	GS 27.250 (27.250)	mem 61.363
Train: [0][346/750]	BT 0.040 (1.362)	DT 0.002 (1.301)	loss 9.517 (9.517)	gnorm 2160350.000 (2160350.000)	prob 1.370 (1.3695)	GS 30.250 (30.250)	mem 61.433
Train: [0][347/750]	BT 0.071 (1.358)	DT 0.002 (1.297)	loss 9.240 (9.240)	gnorm 2214987.750 (2214987.750)	prob 2.014 (2.0136)	GS 33.625 (33.625)	mem 61.457
Train: [0][348/750]	BT 0.093 (1.355)	DT 0.017 (1.294)	loss 9.608 (9.608)	gnorm 2097052.250 (2097052.250)	prob 1.539 (1.5394)	GS 31.859 (31.859)	mem 61.445
Train: [0][349/750]	BT 0.103 (1.351)	DT 0.033 (1.290)	loss 9.808 (9.808)	gnorm 2140833.500 (2140833.500)	prob 1.155 (1.1547)	GS 27.562 (27.562)	mem 61.478
Train: [0][350/750]	BT 0.142 (1.348)	DT 0.021 (1.287)	loss 9.713 (9.713)	gnorm 2336421.750 (2336421.750)	prob 1.324 (1.3235)	GS 34.797 (34.797)	mem 61.431
Train: [0][351/750]	BT 0.059 (1.344)	DT 0.016 (1.283)	loss 9.406 (9.406)	gnorm 1889942.625 (1889942.625)	prob 1.509 (1.5095)	GS 29.344 (29.344)	mem 61.475
Train: [0][352/750]	BT 11.930 (1.374)	DT 11.896 (1.313)	loss 9.717 (9.717)	gnorm 2161985.500 (2161985.500)	prob 1.265 (1.2647)	GS 31.203 (31.203)	mem 63.386
Train: [0][353/750]	BT 0.038 (1.370)	DT 0.004 (1.309)	loss 9.627 (9.627)	gnorm 1957582.625 (1957582.625)	prob 1.339 (1.3392)	GS 30.344 (30.344)	mem 63.393
Train: [0][354/750]	BT 0.088 (1.367)	DT 0.004 (1.306)	loss 9.797 (9.797)	gnorm 2732784.250 (2732784.250)	prob 1.272 (1.2724)	GS 34.844 (34.844)	mem 63.406
Train: [0][355/750]	BT 0.072 (1.363)	DT 0.014 (1.302)	loss 9.625 (9.625)	gnorm 2015336.875 (2015336.875)	prob 1.520 (1.5195)	GS 35.672 (35.672)	mem 63.420
Train: [0][356/750]	BT 0.064 (1.359)	DT 0.022 (1.298)	loss 9.768 (9.768)	gnorm 2436687.500 (2436687.500)	prob 1.160 (1.1595)	GS 32.688 (32.688)	mem 63.437
Train: [0][357/750]	BT 0.057 (1.356)	DT 0.007 (1.295)	loss 9.665 (9.665)	gnorm 2268752.250 (2268752.250)	prob 1.315 (1.3150)	GS 36.547 (36.547)	mem 63.451
Train: [0][358/750]	BT 0.068 (1.352)	DT 0.018 (1.291)	loss 9.631 (9.631)	gnorm 2205519.500 (2205519.500)	prob 1.537 (1.5373)	GS 35.750 (35.750)	mem 63.465
Train: [0][359/750]	BT 0.058 (1.349)	DT 0.013 (1.288)	loss 9.446 (9.446)	gnorm 1935214.500 (1935214.500)	prob 1.329 (1.3295)	GS 30.312 (30.312)	mem 63.530
Train: [0][360/750]	BT 0.071 (1.345)	DT 0.007 (1.284)	loss 9.718 (9.718)	gnorm 2110349.750 (2110349.750)	prob 1.292 (1.2919)	GS 34.391 (34.391)	mem 63.540
Train: [0][361/750]	BT 0.174 (1.342)	DT 0.036 (1.281)	loss 9.567 (9.567)	gnorm 2198812.000 (2198812.000)	prob 1.415 (1.4147)	GS 32.344 (32.344)	mem 63.489
Train: [0][362/750]	BT 0.098 (1.338)	DT 0.003 (1.277)	loss 9.624 (9.624)	gnorm 2260188.500 (2260188.500)	prob 1.458 (1.4577)	GS 35.266 (35.266)	mem 63.492
Train: [0][363/750]	BT 0.115 (1.335)	DT 0.008 (1.274)	loss 9.526 (9.526)	gnorm 2178425.250 (2178425.250)	prob 1.569 (1.5688)	GS 31.281 (31.281)	mem 63.494
Train: [0][364/750]	BT 10.689 (1.361)	DT 10.652 (1.299)	loss 9.605 (9.605)	gnorm 2102927.000 (2102927.000)	prob 1.416 (1.4155)	GS 31.609 (31.609)	mem 65.592
Train: [0][365/750]	BT 0.047 (1.357)	DT 0.003 (1.296)	loss 9.683 (9.683)	gnorm 2722679.750 (2722679.750)	prob 1.371 (1.3713)	GS 44.500 (44.500)	mem 65.579
Train: [0][366/750]	BT 0.047 (1.354)	DT 0.006 (1.292)	loss 9.735 (9.735)	gnorm 2008315.125 (2008315.125)	prob 1.465 (1.4647)	GS 30.266 (30.266)	mem 65.602
Train: [0][367/750]	BT 0.035 (1.350)	DT 0.003 (1.289)	loss 9.566 (9.566)	gnorm 2004973.250 (2004973.250)	prob 1.547 (1.5467)	GS 28.594 (28.594)	mem 65.652
Train: [0][368/750]	BT 0.044 (1.346)	DT 0.010 (1.285)	loss 9.894 (9.894)	gnorm 2437543.500 (2437543.500)	prob 1.139 (1.1391)	GS 32.109 (32.109)	mem 65.635
Train: [0][369/750]	BT 0.038 (1.343)	DT 0.001 (1.282)	loss 9.461 (9.461)	gnorm 2194615.750 (2194615.750)	prob 1.573 (1.5728)	GS 31.750 (31.750)	mem 65.638
Train: [0][370/750]	BT 0.034 (1.339)	DT 0.002 (1.278)	loss 9.998 (9.998)	gnorm 2257082.500 (2257082.500)	prob 0.902 (0.9024)	GS 32.953 (32.953)	mem 65.536
Train: [0][371/750]	BT 0.038 (1.336)	DT 0.003 (1.275)	loss 9.500 (9.500)	gnorm 2158552.500 (2158552.500)	prob 1.359 (1.3585)	GS 36.438 (36.438)	mem 65.494
Train: [0][372/750]	BT 0.033 (1.332)	DT 0.002 (1.272)	loss 9.743 (9.743)	gnorm 2175461.000 (2175461.000)	prob 1.317 (1.3174)	GS 33.750 (33.750)	mem 65.498
Train: [0][373/750]	BT 0.047 (1.329)	DT 0.003 (1.268)	loss 9.605 (9.605)	gnorm 2063788.125 (2063788.125)	prob 1.573 (1.5727)	GS 31.453 (31.453)	mem 65.502
Train: [0][374/750]	BT 10.237 (1.353)	DT 10.186 (1.292)	loss 10.114 (10.114)	gnorm 2652645.250 (2652645.250)	prob 0.779 (0.7789)	GS 32.156 (32.156)	mem 65.641
Train: [0][375/750]	BT 0.054 (1.349)	DT 0.001 (1.289)	loss 9.411 (9.411)	gnorm 2650927.500 (2650927.500)	prob 1.365 (1.3646)	GS 34.828 (34.828)	mem 65.665
Train: [0][376/750]	BT 2.472 (1.352)	DT 2.393 (1.292)	loss 9.908 (9.908)	gnorm 2436642.250 (2436642.250)	prob 0.935 (0.9351)	GS 37.359 (37.359)	mem 65.691
Train: [0][377/750]	BT 0.062 (1.349)	DT 0.002 (1.288)	loss 9.586 (9.586)	gnorm 2266486.250 (2266486.250)	prob 1.330 (1.3300)	GS 33.672 (33.672)	mem 65.739
Train: [0][378/750]	BT 0.087 (1.345)	DT 0.002 (1.285)	loss 9.861 (9.861)	gnorm 2435601.000 (2435601.000)	prob 0.955 (0.9545)	GS 35.078 (35.078)	mem 65.758
Train: [0][379/750]	BT 0.130 (1.342)	DT 0.023 (1.281)	loss 9.925 (9.925)	gnorm 2360431.500 (2360431.500)	prob 1.092 (1.0924)	GS 27.172 (27.172)	mem 65.675
Train: [0][380/750]	BT 0.063 (1.339)	DT 0.004 (1.278)	loss 9.802 (9.802)	gnorm 2227989.000 (2227989.000)	prob 0.851 (0.8509)	GS 31.984 (31.984)	mem 65.676
Train: [0][381/750]	BT 0.029 (1.335)	DT 0.002 (1.275)	loss 9.841 (9.841)	gnorm 2089709.875 (2089709.875)	prob 1.064 (1.0645)	GS 29.984 (29.984)	mem 65.676
Train: [0][382/750]	BT 0.149 (1.332)	DT 0.002 (1.271)	loss 9.973 (9.973)	gnorm 2436429.000 (2436429.000)	prob 0.754 (0.7539)	GS 34.297 (34.297)	mem 65.676
Train: [0][383/750]	BT 0.080 (1.329)	DT 0.004 (1.268)	loss 9.615 (9.615)	gnorm 1944136.000 (1944136.000)	prob 1.248 (1.2485)	GS 30.297 (30.297)	mem 65.676
Train: [0][384/750]	BT 0.075 (1.326)	DT 0.003 (1.265)	loss 9.903 (9.903)	gnorm 2443367.250 (2443367.250)	prob 0.848 (0.8476)	GS 35.953 (35.953)	mem 65.676
Train: [0][385/750]	BT 0.092 (1.323)	DT 0.005 (1.261)	loss 9.853 (9.853)	gnorm 2184539.250 (2184539.250)	prob 1.159 (1.1595)	GS 30.500 (30.500)	mem 65.676
Train: [0][386/750]	BT 13.120 (1.353)	DT 13.069 (1.292)	loss 9.854 (9.854)	gnorm 1843628.000 (1843628.000)	prob 0.852 (0.8521)	GS 28.750 (28.750)	mem 65.538
Train: [0][387/750]	BT 0.082 (1.350)	DT 0.002 (1.289)	loss 9.842 (9.842)	gnorm 2331840.750 (2331840.750)	prob 1.045 (1.0447)	GS 32.484 (32.484)	mem 65.481
Train: [0][388/750]	BT 1.504 (1.350)	DT 1.456 (1.289)	loss 9.903 (9.903)	gnorm 2253650.500 (2253650.500)	prob 0.813 (0.8135)	GS 34.047 (34.047)	mem 65.482
Train: [0][389/750]	BT 0.053 (1.347)	DT 0.004 (1.286)	loss 9.941 (9.941)	gnorm 2173746.250 (2173746.250)	prob 0.903 (0.9033)	GS 30.031 (30.031)	mem 65.482
Train: [0][390/750]	BT 0.088 (1.344)	DT 0.006 (1.283)	loss 9.720 (9.720)	gnorm 2282020.750 (2282020.750)	prob 1.135 (1.1346)	GS 34.672 (34.672)	mem 65.482
Train: [0][391/750]	BT 0.073 (1.340)	DT 0.003 (1.279)	loss 9.638 (9.638)	gnorm 2067582.875 (2067582.875)	prob 1.378 (1.3783)	GS 30.938 (30.938)	mem 65.515
Train: [0][392/750]	BT 0.207 (1.338)	DT 0.004 (1.276)	loss 9.698 (9.698)	gnorm 1899722.875 (1899722.875)	prob 1.496 (1.4958)	GS 33.312 (33.312)	mem 65.527
Train: [0][393/750]	BT 0.174 (1.335)	DT 0.122 (1.273)	loss 9.683 (9.683)	gnorm 2655500.750 (2655500.750)	prob 1.428 (1.4276)	GS 47.984 (47.984)	mem 65.416
Train: [0][394/750]	BT 0.063 (1.331)	DT 0.011 (1.270)	loss 9.619 (9.619)	gnorm 2481369.250 (2481369.250)	prob 1.362 (1.3624)	GS 32.328 (32.328)	mem 65.415
Train: [0][395/750]	BT 0.050 (1.328)	DT 0.004 (1.267)	loss 9.874 (9.874)	gnorm 2966823.250 (2966823.250)	prob 1.595 (1.5952)	GS 35.297 (35.297)	mem 65.416
Train: [0][396/750]	BT 0.038 (1.325)	DT 0.003 (1.263)	loss 9.987 (9.987)	gnorm 2050681.750 (2050681.750)	prob 0.982 (0.9824)	GS 31.438 (31.438)	mem 65.416
Train: [0][397/750]	BT 0.066 (1.322)	DT 0.003 (1.260)	loss 9.792 (9.792)	gnorm 1955304.875 (1955304.875)	prob 1.226 (1.2260)	GS 27.703 (27.703)	mem 65.461
Train: [0][398/750]	BT 17.344 (1.362)	DT 17.288 (1.301)	loss 9.685 (9.685)	gnorm 2368820.500 (2368820.500)	prob 1.236 (1.2361)	GS 37.016 (37.016)	mem 65.585
Train: [0][399/750]	BT 0.052 (1.359)	DT 0.016 (1.297)	loss 9.574 (9.574)	gnorm 2086770.125 (2086770.125)	prob 1.310 (1.3099)	GS 33.938 (33.938)	mem 65.538
Train: [0][400/750]	BT 0.037 (1.355)	DT 0.003 (1.294)	loss 9.646 (9.646)	gnorm 2000243.875 (2000243.875)	prob 1.348 (1.3481)	GS 33.750 (33.750)	mem 65.537
Train: [0][401/750]	BT 0.036 (1.352)	DT 0.003 (1.291)	loss 10.165 (10.165)	gnorm 2203595.750 (2203595.750)	prob 1.087 (1.0873)	GS 28.875 (28.875)	mem 65.442
Train: [0][402/750]	BT 0.031 (1.349)	DT 0.002 (1.288)	loss 9.925 (9.925)	gnorm 2409870.500 (2409870.500)	prob 0.881 (0.8812)	GS 32.062 (32.062)	mem 65.439
Train: [0][403/750]	BT 0.036 (1.346)	DT 0.002 (1.284)	loss 9.774 (9.774)	gnorm 2054400.875 (2054400.875)	prob 1.031 (1.0310)	GS 29.031 (29.031)	mem 65.439
Train: [0][404/750]	BT 0.064 (1.342)	DT 0.015 (1.281)	loss 10.068 (10.068)	gnorm 2949596.750 (2949596.750)	prob 0.794 (0.7936)	GS 33.375 (33.375)	mem 65.435
Train: [0][405/750]	BT 0.055 (1.339)	DT 0.007 (1.278)	loss 9.654 (9.654)	gnorm 1998034.250 (1998034.250)	prob 1.302 (1.3025)	GS 28.984 (28.984)	mem 65.436
Train: [0][406/750]	BT 0.036 (1.336)	DT 0.002 (1.275)	loss 9.800 (9.800)	gnorm 2386450.500 (2386450.500)	prob 1.331 (1.3309)	GS 34.453 (34.453)	mem 65.434
Train: [0][407/750]	BT 0.036 (1.333)	DT 0.003 (1.272)	loss 9.803 (9.803)	gnorm 2543317.000 (2543317.000)	prob 1.017 (1.0170)	GS 32.297 (32.297)	mem 65.434
Train: [0][408/750]	BT 0.037 (1.330)	DT 0.004 (1.269)	loss 9.898 (9.898)	gnorm 2169576.750 (2169576.750)	prob 0.859 (0.8585)	GS 31.141 (31.141)	mem 65.434
Train: [0][409/750]	BT 0.092 (1.327)	DT 0.028 (1.266)	loss 9.276 (9.276)	gnorm 2165546.250 (2165546.250)	prob 1.827 (1.8267)	GS 32.422 (32.422)	mem 65.438
Train: [0][410/750]	BT 17.229 (1.365)	DT 17.140 (1.305)	loss 9.919 (9.919)	gnorm 2364116.750 (2364116.750)	prob 1.077 (1.0766)	GS 31.969 (31.969)	mem 61.497
Train: [0][411/750]	BT 0.042 (1.362)	DT 0.003 (1.301)	loss 9.684 (9.684)	gnorm 1908075.875 (1908075.875)	prob 1.438 (1.4384)	GS 28.297 (28.297)	mem 61.474
Train: [0][412/750]	BT 0.035 (1.359)	DT 0.003 (1.298)	loss 10.367 (10.367)	gnorm 2565809.000 (2565809.000)	prob 0.641 (0.6408)	GS 30.125 (30.125)	mem 61.478
Train: [0][413/750]	BT 0.051 (1.356)	DT 0.005 (1.295)	loss 9.822 (9.822)	gnorm 2346108.500 (2346108.500)	prob 0.856 (0.8563)	GS 30.391 (30.391)	mem 61.488
Train: [0][414/750]	BT 0.031 (1.353)	DT 0.002 (1.292)	loss 10.044 (10.044)	gnorm 2118265.500 (2118265.500)	prob 0.734 (0.7340)	GS 32.078 (32.078)	mem 61.495
Train: [0][415/750]	BT 0.039 (1.349)	DT 0.002 (1.289)	loss 9.840 (9.840)	gnorm 2153946.500 (2153946.500)	prob 1.070 (1.0700)	GS 27.984 (27.984)	mem 61.501
Train: [0][416/750]	BT 0.059 (1.346)	DT 0.002 (1.286)	loss 9.931 (9.931)	gnorm 2449560.750 (2449560.750)	prob 0.908 (0.9080)	GS 31.500 (31.500)	mem 61.505
Train: [0][417/750]	BT 0.083 (1.343)	DT 0.003 (1.283)	loss 9.675 (9.675)	gnorm 2257978.250 (2257978.250)	prob 1.128 (1.1280)	GS 35.578 (35.578)	mem 61.509
Train: [0][418/750]	BT 0.035 (1.340)	DT 0.004 (1.280)	loss 9.873 (9.873)	gnorm 2572519.000 (2572519.000)	prob 0.972 (0.9719)	GS 38.844 (38.844)	mem 61.515
Train: [0][419/750]	BT 0.034 (1.337)	DT 0.004 (1.277)	loss 9.724 (9.724)	gnorm 2130004.750 (2130004.750)	prob 1.204 (1.2044)	GS 29.375 (29.375)	mem 61.526
Train: [0][420/750]	BT 0.048 (1.334)	DT 0.001 (1.274)	loss 9.777 (9.777)	gnorm 1906145.000 (1906145.000)	prob 1.098 (1.0984)	GS 29.359 (29.359)	mem 61.567
Train: [0][421/750]	BT 0.122 (1.331)	DT 0.002 (1.271)	loss 9.823 (9.823)	gnorm 2198871.250 (2198871.250)	prob 1.053 (1.0529)	GS 33.203 (33.203)	mem 61.632
Train: [0][422/750]	BT 15.798 (1.365)	DT 15.765 (1.305)	loss 9.653 (9.653)	gnorm 2039816.125 (2039816.125)	prob 1.244 (1.2441)	GS 33.906 (33.906)	mem 64.640
Train: [0][423/750]	BT 0.035 (1.362)	DT 0.002 (1.302)	loss 10.027 (10.027)	gnorm 2363403.750 (2363403.750)	prob 1.330 (1.3302)	GS 31.875 (31.875)	mem 64.659
Train: [0][424/750]	BT 0.067 (1.359)	DT 0.026 (1.299)	loss 9.676 (9.676)	gnorm 2183851.000 (2183851.000)	prob 1.359 (1.3591)	GS 33.672 (33.672)	mem 64.654
Train: [0][425/750]	BT 0.037 (1.356)	DT 0.003 (1.296)	loss 9.838 (9.838)	gnorm 2163470.250 (2163470.250)	prob 1.316 (1.3157)	GS 32.281 (32.281)	mem 64.659
Train: [0][426/750]	BT 0.027 (1.353)	DT 0.002 (1.293)	loss 9.770 (9.770)	gnorm 3287528.250 (3287528.250)	prob 1.061 (1.0608)	GS 36.547 (36.547)	mem 64.665
Train: [0][427/750]	BT 0.065 (1.350)	DT 0.002 (1.290)	loss 9.702 (9.702)	gnorm 2152004.000 (2152004.000)	prob 1.254 (1.2536)	GS 33.922 (33.922)	mem 64.672
Train: [0][428/750]	BT 0.116 (1.347)	DT 0.030 (1.287)	loss 9.878 (9.878)	gnorm 3128370.500 (3128370.500)	prob 1.036 (1.0358)	GS 34.234 (34.234)	mem 64.684
Train: [0][429/750]	BT 0.072 (1.344)	DT 0.006 (1.284)	loss 9.762 (9.762)	gnorm 2438892.000 (2438892.000)	prob 1.105 (1.1053)	GS 35.938 (35.938)	mem 64.693
Train: [0][430/750]	BT 0.057 (1.341)	DT 0.017 (1.281)	loss 9.942 (9.942)	gnorm 2399469.000 (2399469.000)	prob 0.920 (0.9197)	GS 28.953 (28.953)	mem 64.722
Train: [0][431/750]	BT 0.031 (1.338)	DT 0.001 (1.278)	loss 9.557 (9.557)	gnorm 2191254.750 (2191254.750)	prob 1.334 (1.3341)	GS 31.734 (31.734)	mem 64.713
Train: [0][432/750]	BT 0.033 (1.335)	DT 0.002 (1.275)	loss 9.900 (9.900)	gnorm 2562314.250 (2562314.250)	prob 0.990 (0.9902)	GS 42.844 (42.844)	mem 64.722
Train: [0][433/750]	BT 0.068 (1.332)	DT 0.011 (1.272)	loss 10.160 (10.160)	gnorm 2477078.500 (2477078.500)	prob 0.446 (0.4462)	GS 28.406 (28.406)	mem 64.732
Train: [0][434/750]	BT 11.930 (1.356)	DT 11.884 (1.296)	loss 10.030 (10.030)	gnorm 2712753.500 (2712753.500)	prob 0.777 (0.7773)	GS 37.094 (37.094)	mem 60.184
Train: [0][435/750]	BT 0.052 (1.353)	DT 0.008 (1.293)	loss 10.070 (10.070)	gnorm 2723100.250 (2723100.250)	prob 0.495 (0.4948)	GS 33.594 (33.594)	mem 60.190
Train: [0][436/750]	BT 0.030 (1.350)	DT 0.001 (1.290)	loss 9.967 (9.967)	gnorm 2285236.500 (2285236.500)	prob 0.664 (0.6641)	GS 32.594 (32.594)	mem 60.203
Train: [0][437/750]	BT 0.062 (1.348)	DT 0.010 (1.288)	loss 9.870 (9.870)	gnorm 2499749.750 (2499749.750)	prob 1.025 (1.0250)	GS 33.031 (33.031)	mem 60.219
Train: [0][438/750]	BT 0.045 (1.345)	DT 0.005 (1.285)	loss 9.842 (9.842)	gnorm 2376571.500 (2376571.500)	prob 0.975 (0.9752)	GS 35.688 (35.688)	mem 60.232
Train: [0][439/750]	BT 0.029 (1.342)	DT 0.001 (1.282)	loss 9.622 (9.622)	gnorm 1876348.625 (1876348.625)	prob 1.148 (1.1477)	GS 32.484 (32.484)	mem 60.239
Train: [0][440/750]	BT 0.098 (1.339)	DT 0.002 (1.279)	loss 10.204 (10.204)	gnorm 2637628.750 (2637628.750)	prob 0.611 (0.6107)	GS 37.297 (37.297)	mem 60.255
Train: [0][441/750]	BT 0.069 (1.336)	DT 0.002 (1.276)	loss 9.598 (9.598)	gnorm 1972020.875 (1972020.875)	prob 1.278 (1.2783)	GS 29.172 (29.172)	mem 60.267
Train: [0][442/750]	BT 0.066 (1.333)	DT 0.008 (1.273)	loss 10.342 (10.342)	gnorm 2559666.250 (2559666.250)	prob 0.666 (0.6661)	GS 38.078 (38.078)	mem 60.277
Train: [0][443/750]	BT 0.107 (1.330)	DT 0.010 (1.270)	loss 9.550 (9.550)	gnorm 2166398.000 (2166398.000)	prob 1.295 (1.2945)	GS 30.172 (30.172)	mem 60.333
Train: [0][444/750]	BT 0.084 (1.327)	DT 0.012 (1.267)	loss 9.935 (9.935)	gnorm 2590826.250 (2590826.250)	prob 0.921 (0.9214)	GS 35.047 (35.047)	mem 60.330
Train: [0][445/750]	BT 0.045 (1.325)	DT 0.005 (1.264)	loss 9.830 (9.830)	gnorm 2190045.500 (2190045.500)	prob 1.225 (1.2255)	GS 29.656 (29.656)	mem 60.303
Train: [0][446/750]	BT 9.665 (1.343)	DT 9.617 (1.283)	loss 9.682 (9.682)	gnorm 2006539.500 (2006539.500)	prob 1.219 (1.2189)	GS 30.031 (30.031)	mem 62.511
Train: [0][447/750]	BT 0.055 (1.340)	DT 0.005 (1.280)	loss 9.775 (9.775)	gnorm 2066749.000 (2066749.000)	prob 1.081 (1.0811)	GS 30.312 (30.312)	mem 62.515
Train: [0][448/750]	BT 0.036 (1.337)	DT 0.002 (1.277)	loss 9.528 (9.528)	gnorm 2102881.250 (2102881.250)	prob 1.506 (1.5057)	GS 31.953 (31.953)	mem 62.522
Train: [0][449/750]	BT 0.037 (1.335)	DT 0.002 (1.275)	loss 9.858 (9.858)	gnorm 1859345.625 (1859345.625)	prob 1.427 (1.4265)	GS 29.812 (29.812)	mem 62.509
Train: [0][450/750]	BT 0.063 (1.332)	DT 0.009 (1.272)	loss 10.117 (10.117)	gnorm 2217693.500 (2217693.500)	prob 0.706 (0.7059)	GS 34.672 (34.672)	mem 62.504
Train: [0][451/750]	BT 0.055 (1.329)	DT 0.004 (1.269)	loss 9.605 (9.605)	gnorm 2116212.250 (2116212.250)	prob 1.175 (1.1750)	GS 31.422 (31.422)	mem 62.558
Train: [0][452/750]	BT 0.038 (1.326)	DT 0.003 (1.266)	loss 9.639 (9.639)	gnorm 1900975.625 (1900975.625)	prob 1.226 (1.2256)	GS 30.781 (30.781)	mem 62.677
Train: [0][453/750]	BT 0.039 (1.323)	DT 0.003 (1.263)	loss 10.073 (10.073)	gnorm 1942886.750 (1942886.750)	prob 0.659 (0.6591)	GS 26.844 (26.844)	mem 62.597
Train: [0][454/750]	BT 0.037 (1.320)	DT 0.003 (1.261)	loss 9.839 (9.839)	gnorm 2427693.750 (2427693.750)	prob 0.821 (0.8215)	GS 31.828 (31.828)	mem 62.618
Train: [0][455/750]	BT 0.037 (1.318)	DT 0.003 (1.258)	loss 9.960 (9.960)	gnorm 2241861.000 (2241861.000)	prob 0.866 (0.8663)	GS 30.047 (30.047)	mem 62.634
Train: [0][456/750]	BT 0.036 (1.315)	DT 0.002 (1.255)	loss 10.436 (10.436)	gnorm 2769554.500 (2769554.500)	prob 0.303 (0.3032)	GS 31.516 (31.516)	mem 62.653
Train: [0][457/750]	BT 0.143 (1.312)	DT 0.006 (1.252)	loss 10.196 (10.196)	gnorm 2365505.250 (2365505.250)	prob 0.592 (0.5917)	GS 29.875 (29.875)	mem 62.759
Train: [0][458/750]	BT 14.736 (1.341)	DT 14.708 (1.282)	loss 9.643 (9.643)	gnorm 2520313.250 (2520313.250)	prob 0.762 (0.7618)	GS 36.125 (36.125)	mem 65.046
Train: [0][459/750]	BT 0.029 (1.339)	DT 0.001 (1.279)	loss 9.684 (9.684)	gnorm 2567269.250 (2567269.250)	prob 1.081 (1.0814)	GS 28.641 (28.641)	mem 65.078
Train: [0][460/750]	BT 0.034 (1.336)	DT 0.001 (1.276)	loss 10.050 (10.050)	gnorm 3258243.500 (3258243.500)	prob 0.611 (0.6107)	GS 35.562 (35.562)	mem 65.117
Train: [0][461/750]	BT 0.084 (1.333)	DT 0.002 (1.273)	loss 10.033 (10.033)	gnorm 2304767.000 (2304767.000)	prob 0.757 (0.7568)	GS 29.719 (29.719)	mem 65.130
Train: [0][462/750]	BT 0.067 (1.330)	DT 0.012 (1.271)	loss 9.944 (9.944)	gnorm 2669937.250 (2669937.250)	prob 0.783 (0.7835)	GS 34.812 (34.812)	mem 65.143
Train: [0][463/750]	BT 0.037 (1.327)	DT 0.002 (1.268)	loss 9.636 (9.636)	gnorm 2017234.750 (2017234.750)	prob 1.000 (1.0004)	GS 27.984 (27.984)	mem 65.097
Train: [0][464/750]	BT 0.062 (1.325)	DT 0.013 (1.265)	loss 10.238 (10.238)	gnorm 2497803.750 (2497803.750)	prob 0.554 (0.5538)	GS 32.172 (32.172)	mem 65.112
Train: [0][465/750]	BT 0.080 (1.322)	DT 0.017 (1.263)	loss 9.862 (9.862)	gnorm 2267359.500 (2267359.500)	prob 1.022 (1.0218)	GS 34.750 (34.750)	mem 65.152
Train: [0][466/750]	BT 0.887 (1.321)	DT 0.820 (1.262)	loss 9.876 (9.876)	gnorm 2688757.000 (2688757.000)	prob 0.865 (0.8652)	GS 35.938 (35.938)	mem 65.460
Train: [0][467/750]	BT 0.147 (1.319)	DT 0.007 (1.259)	loss 9.688 (9.688)	gnorm 1969390.125 (1969390.125)	prob 0.875 (0.8752)	GS 32.562 (32.562)	mem 65.411
Train: [0][468/750]	BT 0.043 (1.316)	DT 0.007 (1.256)	loss 9.946 (9.946)	gnorm 2947246.500 (2947246.500)	prob 0.832 (0.8316)	GS 36.203 (36.203)	mem 65.380
Train: [0][469/750]	BT 0.046 (1.313)	DT 0.002 (1.254)	loss 9.803 (9.803)	gnorm 2011969.375 (2011969.375)	prob 1.049 (1.0492)	GS 30.594 (30.594)	mem 65.324
Train: [0][470/750]	BT 12.582 (1.337)	DT 12.479 (1.277)	loss 9.886 (9.886)	gnorm 2197294.750 (2197294.750)	prob 0.748 (0.7483)	GS 31.047 (31.047)	mem 65.711
Train: [0][471/750]	BT 0.048 (1.334)	DT 0.006 (1.275)	loss 9.813 (9.813)	gnorm 1997146.250 (1997146.250)	prob 1.042 (1.0417)	GS 33.828 (33.828)	mem 65.710
Train: [0][472/750]	BT 0.079 (1.332)	DT 0.015 (1.272)	loss 9.625 (9.625)	gnorm 2235270.750 (2235270.750)	prob 1.177 (1.1774)	GS 32.703 (32.703)	mem 65.710
Train: [0][473/750]	BT 0.095 (1.329)	DT 0.001 (1.269)	loss 10.010 (10.010)	gnorm 2120211.250 (2120211.250)	prob 0.918 (0.9183)	GS 35.406 (35.406)	mem 65.736
Train: [0][474/750]	BT 1.489 (1.330)	DT 1.437 (1.270)	loss 9.885 (9.885)	gnorm 2172804.750 (2172804.750)	prob 0.593 (0.5935)	GS 31.469 (31.469)	mem 65.858
Train: [0][475/750]	BT 0.050 (1.327)	DT 0.005 (1.267)	loss 10.140 (10.140)	gnorm 2050484.000 (2050484.000)	prob 0.977 (0.9772)	GS 29.531 (29.531)	mem 65.858
Train: [0][476/750]	BT 0.095 (1.324)	DT 0.017 (1.265)	loss 9.702 (9.702)	gnorm 2877618.250 (2877618.250)	prob 0.996 (0.9963)	GS 29.688 (29.688)	mem 65.871
Train: [0][477/750]	BT 0.029 (1.322)	DT 0.001 (1.262)	loss 9.928 (9.928)	gnorm 2156730.250 (2156730.250)	prob 0.916 (0.9162)	GS 31.672 (31.672)	mem 65.912
Train: [0][478/750]	BT 0.140 (1.319)	DT 0.106 (1.259)	loss 10.176 (10.176)	gnorm 2290689.250 (2290689.250)	prob 0.458 (0.4577)	GS 31.453 (31.453)	mem 65.674
Train: [0][479/750]	BT 0.059 (1.316)	DT 0.013 (1.257)	loss 9.887 (9.887)	gnorm 2252173.000 (2252173.000)	prob 0.772 (0.7723)	GS 27.969 (27.969)	mem 65.639
Train: [0][480/750]	BT 0.039 (1.314)	DT 0.011 (1.254)	loss 10.058 (10.058)	gnorm 2672073.500 (2672073.500)	prob 0.511 (0.5112)	GS 34.625 (34.625)	mem 65.681
Train: [0][481/750]	BT 0.111 (1.311)	DT 0.001 (1.252)	loss 9.859 (9.859)	gnorm 2204255.500 (2204255.500)	prob 0.369 (0.3686)	GS 33.109 (33.109)	mem 65.690
Train: [0][482/750]	BT 13.277 (1.336)	DT 13.229 (1.276)	loss 10.081 (10.081)	gnorm 2643490.500 (2643490.500)	prob 0.262 (0.2620)	GS 33.219 (33.219)	mem 65.674
Train: [0][483/750]	BT 0.083 (1.333)	DT 0.013 (1.274)	loss 10.177 (10.177)	gnorm 2407119.000 (2407119.000)	prob 0.288 (0.2876)	GS 36.531 (36.531)	mem 65.673
Train: [0][484/750]	BT 0.064 (1.331)	DT 0.011 (1.271)	loss 9.821 (9.821)	gnorm 2523931.000 (2523931.000)	prob 0.640 (0.6401)	GS 34.828 (34.828)	mem 65.673
Train: [0][485/750]	BT 0.038 (1.328)	DT 0.003 (1.269)	loss 9.903 (9.903)	gnorm 2163764.750 (2163764.750)	prob 0.646 (0.6457)	GS 26.219 (26.219)	mem 65.673
Train: [0][486/750]	BT 0.034 (1.326)	DT 0.003 (1.266)	loss 10.218 (10.218)	gnorm 1947875.750 (1947875.750)	prob 0.302 (0.3022)	GS 29.734 (29.734)	mem 65.673
Train: [0][487/750]	BT 0.036 (1.323)	DT 0.002 (1.263)	loss 9.627 (9.627)	gnorm 2499533.500 (2499533.500)	prob 1.250 (1.2504)	GS 34.922 (34.922)	mem 65.673
Train: [0][488/750]	BT 0.050 (1.320)	DT 0.003 (1.261)	loss 9.949 (9.949)	gnorm 2425514.250 (2425514.250)	prob 0.875 (0.8748)	GS 34.031 (34.031)	mem 65.672
Train: [0][489/750]	BT 0.035 (1.318)	DT 0.001 (1.258)	loss 9.793 (9.793)	gnorm 1998746.000 (1998746.000)	prob 1.043 (1.0427)	GS 30.859 (30.859)	mem 65.672
Train: [0][490/750]	BT 4.661 (1.324)	DT 4.585 (1.265)	loss 10.262 (10.262)	gnorm 2608423.250 (2608423.250)	prob 0.408 (0.4075)	GS 37.062 (37.062)	mem 65.820
Train: [0][491/750]	BT 0.083 (1.322)	DT 0.016 (1.263)	loss 9.741 (9.741)	gnorm 2061672.000 (2061672.000)	prob 1.019 (1.0190)	GS 32.797 (32.797)	mem 65.821
Train: [0][492/750]	BT 0.073 (1.319)	DT 0.002 (1.260)	loss 10.015 (10.015)	gnorm 2079077.625 (2079077.625)	prob 0.607 (0.6065)	GS 33.234 (33.234)	mem 65.821
Train: [0][493/750]	BT 0.046 (1.317)	DT 0.001 (1.257)	loss 10.092 (10.092)	gnorm 2392183.000 (2392183.000)	prob 0.738 (0.7380)	GS 33.125 (33.125)	mem 65.632
Train: [0][494/750]	BT 14.785 (1.344)	DT 14.752 (1.285)	loss 10.197 (10.197)	gnorm 2886215.500 (2886215.500)	prob 0.647 (0.6466)	GS 36.859 (36.859)	mem 59.633
Train: [0][495/750]	BT 0.057 (1.341)	DT 0.008 (1.282)	loss 10.103 (10.103)	gnorm 2134867.750 (2134867.750)	prob 0.531 (0.5309)	GS 29.047 (29.047)	mem 59.683
Train: [0][496/750]	BT 0.036 (1.339)	DT 0.009 (1.280)	loss 9.978 (9.978)	gnorm 2386522.500 (2386522.500)	prob 0.378 (0.3777)	GS 35.844 (35.844)	mem 59.718
Train: [0][497/750]	BT 0.031 (1.336)	DT 0.001 (1.277)	loss 10.130 (10.130)	gnorm 2140576.500 (2140576.500)	prob 0.504 (0.5037)	GS 36.172 (36.172)	mem 59.753
Train: [0][498/750]	BT 0.107 (1.334)	DT 0.006 (1.274)	loss 9.928 (9.928)	gnorm 2111260.750 (2111260.750)	prob 0.564 (0.5638)	GS 33.266 (33.266)	mem 59.763
Train: [0][499/750]	BT 0.038 (1.331)	DT 0.006 (1.272)	loss 9.710 (9.710)	gnorm 2443456.500 (2443456.500)	prob 0.766 (0.7664)	GS 33.578 (33.578)	mem 59.780
Train: [0][500/750]	BT 0.030 (1.329)	DT 0.001 (1.269)	loss 9.876 (9.876)	gnorm 2344991.500 (2344991.500)	prob 0.601 (0.6009)	GS 35.297 (35.297)	mem 59.716
Train: [0][501/750]	BT 0.095 (1.326)	DT 0.002 (1.267)	loss 9.672 (9.672)	gnorm 1852827.750 (1852827.750)	prob 1.072 (1.0724)	GS 34.609 (34.609)	mem 59.742
Train: [0][502/750]	BT 0.027 (1.324)	DT 0.002 (1.264)	loss 9.867 (9.867)	gnorm 2709377.500 (2709377.500)	prob 0.774 (0.7736)	GS 35.234 (35.234)	mem 59.751
Train: [0][503/750]	BT 0.060 (1.321)	DT 0.002 (1.262)	loss 10.389 (10.389)	gnorm 2395926.500 (2395926.500)	prob 0.517 (0.5168)	GS 32.328 (32.328)	mem 59.765
Train: [0][504/750]	BT 0.093 (1.319)	DT 0.002 (1.259)	loss 10.129 (10.129)	gnorm 2861491.000 (2861491.000)	prob 0.476 (0.4757)	GS 37.438 (37.438)	mem 59.778
Train: [0][505/750]	BT 0.072 (1.316)	DT 0.004 (1.257)	loss 10.015 (10.015)	gnorm 1960961.625 (1960961.625)	prob 0.215 (0.2146)	GS 32.938 (32.938)	mem 59.794
Train: [0][506/750]	BT 12.083 (1.337)	DT 12.059 (1.278)	loss 9.898 (9.898)	gnorm 2201099.750 (2201099.750)	prob 0.359 (0.3588)	GS 32.016 (32.016)	mem 61.865
Train: [0][507/750]	BT 0.026 (1.335)	DT 0.001 (1.276)	loss 10.015 (10.015)	gnorm 1791831.375 (1791831.375)	prob 0.080 (0.0796)	GS 25.797 (25.797)	mem 61.880
Train: [0][508/750]	BT 0.032 (1.332)	DT 0.001 (1.273)	loss 9.958 (9.958)	gnorm 2462893.250 (2462893.250)	prob 0.438 (0.4380)	GS 29.547 (29.547)	mem 61.886
Train: [0][509/750]	BT 0.040 (1.330)	DT 0.002 (1.271)	loss 10.054 (10.054)	gnorm 2363981.500 (2363981.500)	prob 0.618 (0.6181)	GS 33.938 (33.938)	mem 61.938
Train: [0][510/750]	BT 0.032 (1.327)	DT 0.001 (1.268)	loss 10.258 (10.258)	gnorm 2236423.000 (2236423.000)	prob 0.365 (0.3650)	GS 34.078 (34.078)	mem 61.902
Train: [0][511/750]	BT 0.063 (1.325)	DT 0.003 (1.266)	loss 10.006 (10.006)	gnorm 1935710.875 (1935710.875)	prob 0.269 (0.2686)	GS 27.422 (27.422)	mem 61.922
Train: [0][512/750]	BT 0.037 (1.322)	DT 0.002 (1.263)	loss 10.467 (10.467)	gnorm 2863301.500 (2863301.500)	prob -0.129 (-0.1295)	GS 28.922 (28.922)	mem 61.941
Train: [0][513/750]	BT 0.073 (1.320)	DT 0.002 (1.261)	loss 9.904 (9.904)	gnorm 1790825.875 (1790825.875)	prob 0.385 (0.3854)	GS 28.953 (28.953)	mem 62.015
Train: [0][514/750]	BT 0.056 (1.317)	DT 0.004 (1.258)	loss 9.741 (9.741)	gnorm 2323146.000 (2323146.000)	prob 0.536 (0.5358)	GS 32.656 (32.656)	mem 62.105
Train: [0][515/750]	BT 0.082 (1.315)	DT 0.019 (1.256)	loss 10.354 (10.354)	gnorm 2485882.000 (2485882.000)	prob -0.071 (-0.0710)	GS 35.516 (35.516)	mem 62.126
Train: [0][516/750]	BT 0.037 (1.312)	DT 0.004 (1.253)	loss 9.956 (9.956)	gnorm 2229356.000 (2229356.000)	prob 0.165 (0.1652)	GS 34.594 (34.594)	mem 62.096
Train: [0][517/750]	BT 0.044 (1.310)	DT 0.004 (1.251)	loss 10.022 (10.022)	gnorm 2331297.750 (2331297.750)	prob 0.285 (0.2849)	GS 31.156 (31.156)	mem 62.150
Train: [0][518/750]	BT 15.673 (1.338)	DT 15.637 (1.279)	loss 10.311 (10.311)	gnorm 2969545.750 (2969545.750)	prob -0.061 (-0.0612)	GS 36.078 (36.078)	mem 64.427
Train: [0][519/750]	BT 0.054 (1.335)	DT 0.002 (1.276)	loss 10.120 (10.120)	gnorm 2329112.250 (2329112.250)	prob 0.268 (0.2677)	GS 30.281 (30.281)	mem 64.445
Train: [0][520/750]	BT 0.044 (1.333)	DT 0.003 (1.274)	loss 10.080 (10.080)	gnorm 2866108.500 (2866108.500)	prob 0.207 (0.2069)	GS 34.562 (34.562)	mem 64.470
Train: [0][521/750]	BT 0.037 (1.330)	DT 0.001 (1.271)	loss 10.196 (10.196)	gnorm 2168664.750 (2168664.750)	prob 0.213 (0.2135)	GS 34.234 (34.234)	mem 64.466
Train: [0][522/750]	BT 0.079 (1.328)	DT 0.012 (1.269)	loss 10.123 (10.123)	gnorm 2424489.250 (2424489.250)	prob 0.275 (0.2745)	GS 35.391 (35.391)	mem 64.481
Train: [0][523/750]	BT 0.040 (1.325)	DT 0.002 (1.267)	loss 10.084 (10.084)	gnorm 2366188.000 (2366188.000)	prob -0.020 (-0.0200)	GS 31.391 (31.391)	mem 64.494
Train: [0][524/750]	BT 0.038 (1.323)	DT 0.004 (1.264)	loss 10.023 (10.023)	gnorm 2174705.250 (2174705.250)	prob 0.050 (0.0498)	GS 32.172 (32.172)	mem 64.500
Train: [0][525/750]	BT 0.027 (1.320)	DT 0.001 (1.262)	loss 9.986 (9.986)	gnorm 2392203.500 (2392203.500)	prob 0.192 (0.1916)	GS 30.734 (30.734)	mem 64.509
Train: [0][526/750]	BT 0.032 (1.318)	DT 0.002 (1.259)	loss 10.412 (10.412)	gnorm 2526966.000 (2526966.000)	prob -0.592 (-0.5921)	GS 33.531 (33.531)	mem 64.516
Train: [0][527/750]	BT 0.062 (1.316)	DT 0.002 (1.257)	loss 10.445 (10.445)	gnorm 2203673.500 (2203673.500)	prob -0.216 (-0.2159)	GS 29.219 (29.219)	mem 64.550
Train: [0][528/750]	BT 0.122 (1.313)	DT 0.023 (1.255)	loss 10.103 (10.103)	gnorm 2160147.500 (2160147.500)	prob 0.125 (0.1252)	GS 30.812 (30.812)	mem 64.611
Train: [0][529/750]	BT 0.066 (1.311)	DT 0.017 (1.252)	loss 9.980 (9.980)	gnorm 2757230.250 (2757230.250)	prob 0.369 (0.3690)	GS 37.016 (37.016)	mem 64.625
Train: [0][530/750]	BT 15.520 (1.338)	DT 15.469 (1.279)	loss 10.120 (10.120)	gnorm 2599168.500 (2599168.500)	prob 0.111 (0.1112)	GS 34.078 (34.078)	mem 60.638
Train: [0][531/750]	BT 0.040 (1.335)	DT 0.005 (1.277)	loss 10.325 (10.325)	gnorm 2334048.250 (2334048.250)	prob 0.072 (0.0718)	GS 29.453 (29.453)	mem 60.687
Train: [0][532/750]	BT 0.065 (1.333)	DT 0.016 (1.274)	loss 10.372 (10.372)	gnorm 2982750.250 (2982750.250)	prob -0.213 (-0.2129)	GS 38.312 (38.312)	mem 60.681
Train: [0][533/750]	BT 0.064 (1.331)	DT 0.004 (1.272)	loss 9.960 (9.960)	gnorm 1972830.625 (1972830.625)	prob 0.138 (0.1381)	GS 29.484 (29.484)	mem 60.769
Train: [0][534/750]	BT 0.082 (1.328)	DT 0.005 (1.270)	loss 10.090 (10.090)	gnorm 2260388.250 (2260388.250)	prob 0.191 (0.1908)	GS 36.484 (36.484)	mem 60.756
Train: [0][535/750]	BT 0.072 (1.326)	DT 0.013 (1.267)	loss 10.280 (10.280)	gnorm 2456036.250 (2456036.250)	prob -0.360 (-0.3597)	GS 49.438 (49.438)	mem 60.764
Train: [0][536/750]	BT 0.037 (1.323)	DT 0.006 (1.265)	loss 10.336 (10.336)	gnorm 2505297.000 (2505297.000)	prob -0.440 (-0.4397)	GS 38.562 (38.562)	mem 60.767
Train: [0][537/750]	BT 0.031 (1.321)	DT 0.001 (1.263)	loss 10.134 (10.134)	gnorm 2033922.125 (2033922.125)	prob -0.098 (-0.0984)	GS 28.625 (28.625)	mem 60.780
Train: [0][538/750]	BT 0.033 (1.319)	DT 0.002 (1.260)	loss 10.294 (10.294)	gnorm 2240057.000 (2240057.000)	prob -0.554 (-0.5543)	GS 33.422 (33.422)	mem 60.808
Train: [0][539/750]	BT 0.034 (1.316)	DT 0.002 (1.258)	loss 9.879 (9.879)	gnorm 1837030.375 (1837030.375)	prob -0.321 (-0.3207)	GS 27.406 (27.406)	mem 60.856
Train: [0][540/750]	BT 0.036 (1.314)	DT 0.002 (1.256)	loss 9.407 (9.407)	gnorm 1958063.250 (1958063.250)	prob 0.545 (0.5449)	GS 31.016 (31.016)	mem 60.820
Train: [0][541/750]	BT 0.060 (1.312)	DT 0.011 (1.253)	loss 10.268 (10.268)	gnorm 2808855.500 (2808855.500)	prob -0.541 (-0.5410)	GS 34.812 (34.812)	mem 60.861
Train: [0][542/750]	BT 15.269 (1.337)	DT 15.226 (1.279)	loss 10.250 (10.250)	gnorm 2220666.500 (2220666.500)	prob -0.605 (-0.6046)	GS 30.875 (30.875)	mem 64.071
Train: [0][543/750]	BT 0.039 (1.335)	DT 0.005 (1.277)	loss 10.158 (10.158)	gnorm 2138756.750 (2138756.750)	prob -0.518 (-0.5182)	GS 28.062 (28.062)	mem 64.004
Train: [0][544/750]	BT 0.036 (1.333)	DT 0.003 (1.274)	loss 10.312 (10.312)	gnorm 2558834.000 (2558834.000)	prob -0.713 (-0.7131)	GS 30.828 (30.828)	mem 64.032
Train: [0][545/750]	BT 0.091 (1.330)	DT 0.006 (1.272)	loss 9.950 (9.950)	gnorm 2014474.125 (2014474.125)	prob -0.124 (-0.1243)	GS 27.031 (27.031)	mem 64.160
Train: [0][546/750]	BT 0.125 (1.328)	DT 0.004 (1.270)	loss 9.921 (9.921)	gnorm 2431166.750 (2431166.750)	prob -0.043 (-0.0428)	GS 30.406 (30.406)	mem 64.142
Train: [0][547/750]	BT 0.064 (1.326)	DT 0.024 (1.267)	loss 10.012 (10.012)	gnorm 2004384.625 (2004384.625)	prob 0.591 (0.5908)	GS 31.188 (31.188)	mem 64.055
Train: [0][548/750]	BT 0.036 (1.323)	DT 0.002 (1.265)	loss 10.058 (10.058)	gnorm 2654694.250 (2654694.250)	prob 0.031 (0.0306)	GS 34.766 (34.766)	mem 64.112
Train: [0][549/750]	BT 0.103 (1.321)	DT 0.008 (1.263)	loss 10.231 (10.231)	gnorm 2009597.750 (2009597.750)	prob -0.002 (-0.0015)	GS 29.641 (29.641)	mem 64.077
Train: [0][550/750]	BT 0.056 (1.319)	DT 0.014 (1.261)	loss 10.332 (10.332)	gnorm 2494784.500 (2494784.500)	prob -0.045 (-0.0447)	GS 31.328 (31.328)	mem 64.098
Train: [0][551/750]	BT 0.031 (1.317)	DT 0.002 (1.258)	loss 10.046 (10.046)	gnorm 2043466.000 (2043466.000)	prob 0.199 (0.1987)	GS 31.250 (31.250)	mem 64.109
Train: [0][552/750]	BT 0.031 (1.314)	DT 0.001 (1.256)	loss 9.764 (9.764)	gnorm 1914679.625 (1914679.625)	prob 0.228 (0.2281)	GS 31.469 (31.469)	mem 64.130
Train: [0][553/750]	BT 0.066 (1.312)	DT 0.015 (1.254)	loss 10.435 (10.435)	gnorm 2280051.750 (2280051.750)	prob -0.046 (-0.0461)	GS 34.406 (34.406)	mem 64.266
Train: [0][554/750]	BT 11.861 (1.331)	DT 11.810 (1.273)	loss 9.953 (9.953)	gnorm 2329871.750 (2329871.750)	prob 0.192 (0.1923)	GS 34.844 (34.844)	mem 66.372
Train: [0][555/750]	BT 0.036 (1.329)	DT 0.002 (1.271)	loss 10.404 (10.404)	gnorm 2286595.500 (2286595.500)	prob 0.047 (0.0472)	GS 32.109 (32.109)	mem 66.423
Train: [0][556/750]	BT 0.100 (1.326)	DT 0.017 (1.268)	loss 10.119 (10.119)	gnorm 2373514.500 (2373514.500)	prob 0.162 (0.1621)	GS 34.312 (34.312)	mem 66.417
Train: [0][557/750]	BT 0.068 (1.324)	DT 0.007 (1.266)	loss 9.908 (9.908)	gnorm 1871232.125 (1871232.125)	prob 0.325 (0.3253)	GS 34.828 (34.828)	mem 66.504
Train: [0][558/750]	BT 0.163 (1.322)	DT 0.007 (1.264)	loss 10.107 (10.107)	gnorm 2388650.000 (2388650.000)	prob 0.572 (0.5725)	GS 31.750 (31.750)	mem 66.453
Train: [0][559/750]	BT 0.051 (1.320)	DT 0.014 (1.262)	loss 10.042 (10.042)	gnorm 2116124.250 (2116124.250)	prob 0.274 (0.2739)	GS 33.969 (33.969)	mem 66.381
Train: [0][560/750]	BT 0.240 (1.318)	DT 0.196 (1.260)	loss 10.356 (10.356)	gnorm 2843418.750 (2843418.750)	prob -0.131 (-0.1314)	GS 35.953 (35.953)	mem 66.556
Train: [0][561/750]	BT 0.083 (1.316)	DT 0.002 (1.257)	loss 9.930 (9.930)	gnorm 2074681.125 (2074681.125)	prob 0.159 (0.1586)	GS 28.328 (28.328)	mem 66.607
Train: [0][562/750]	BT 0.111 (1.314)	DT 0.011 (1.255)	loss 10.168 (10.168)	gnorm 2354142.500 (2354142.500)	prob -0.325 (-0.3254)	GS 33.906 (33.906)	mem 66.592
Train: [0][563/750]	BT 0.052 (1.311)	DT 0.008 (1.253)	loss 9.965 (9.965)	gnorm 2054428.875 (2054428.875)	prob 0.136 (0.1359)	GS 32.828 (32.828)	mem 66.431
Train: [0][564/750]	BT 0.052 (1.309)	DT 0.002 (1.251)	loss 10.034 (10.034)	gnorm 2197501.500 (2197501.500)	prob 0.453 (0.4529)	GS 31.016 (31.016)	mem 66.391
Train: [0][565/750]	BT 0.050 (1.307)	DT 0.001 (1.249)	loss 9.674 (9.674)	gnorm 2101789.750 (2101789.750)	prob 0.902 (0.9018)	GS 33.297 (33.297)	mem 66.392
Train: [0][566/750]	BT 10.956 (1.324)	DT 10.891 (1.266)	loss 10.036 (10.036)	gnorm 2631832.500 (2631832.500)	prob 0.470 (0.4703)	GS 32.375 (32.375)	mem 66.448
Train: [0][567/750]	BT 0.084 (1.322)	DT 0.003 (1.263)	loss 9.998 (9.998)	gnorm 2173308.250 (2173308.250)	prob 0.624 (0.6239)	GS 27.469 (27.469)	mem 66.447
Train: [0][568/750]	BT 0.065 (1.320)	DT 0.004 (1.261)	loss 10.140 (10.140)	gnorm 2578742.000 (2578742.000)	prob 0.334 (0.3337)	GS 36.047 (36.047)	mem 66.445
Train: [0][569/750]	BT 0.060 (1.317)	DT 0.002 (1.259)	loss 10.066 (10.066)	gnorm 2100126.000 (2100126.000)	prob 0.619 (0.6186)	GS 30.484 (30.484)	mem 66.446
Train: [0][570/750]	BT 0.046 (1.315)	DT 0.007 (1.257)	loss 10.242 (10.242)	gnorm 2514970.750 (2514970.750)	prob 0.336 (0.3355)	GS 32.250 (32.250)	mem 66.446
Train: [0][571/750]	BT 0.057 (1.313)	DT 0.001 (1.255)	loss 9.941 (9.941)	gnorm 2079652.875 (2079652.875)	prob 0.470 (0.4696)	GS 30.719 (30.719)	mem 66.445
Train: [0][572/750]	BT 0.057 (1.311)	DT 0.002 (1.252)	loss 9.983 (9.983)	gnorm 2179235.250 (2179235.250)	prob 0.136 (0.1359)	GS 33.078 (33.078)	mem 66.445
Train: [0][573/750]	BT 0.078 (1.309)	DT 0.006 (1.250)	loss 10.296 (10.296)	gnorm 2354076.750 (2354076.750)	prob 0.211 (0.2110)	GS 29.781 (29.781)	mem 66.445
Train: [0][574/750]	BT 1.231 (1.308)	DT 1.184 (1.250)	loss 10.007 (10.007)	gnorm 2215795.250 (2215795.250)	prob 0.329 (0.3287)	GS 36.125 (36.125)	mem 66.559
Train: [0][575/750]	BT 0.061 (1.306)	DT 0.003 (1.248)	loss 10.198 (10.198)	gnorm 2143832.000 (2143832.000)	prob 0.390 (0.3896)	GS 32.250 (32.250)	mem 66.510
Train: [0][576/750]	BT 0.049 (1.304)	DT 0.006 (1.246)	loss 9.982 (9.982)	gnorm 3220923.000 (3220923.000)	prob 0.326 (0.3259)	GS 37.062 (37.062)	mem 66.522
Train: [0][577/750]	BT 0.040 (1.302)	DT 0.003 (1.244)	loss 9.971 (9.971)	gnorm 2250061.500 (2250061.500)	prob 0.353 (0.3526)	GS 31.453 (31.453)	mem 66.515
Train: [0][578/750]	BT 10.303 (1.317)	DT 10.254 (1.259)	loss 9.998 (9.998)	gnorm 2402958.500 (2402958.500)	prob 0.464 (0.4643)	GS 32.859 (32.859)	mem 66.471
Train: [0][579/750]	BT 0.056 (1.315)	DT 0.003 (1.257)	loss 10.006 (10.006)	gnorm 2536528.750 (2536528.750)	prob 0.640 (0.6397)	GS 34.859 (34.859)	mem 66.472
Train: [0][580/750]	BT 0.048 (1.313)	DT 0.003 (1.255)	loss 10.487 (10.487)	gnorm 2363322.000 (2363322.000)	prob -0.159 (-0.1585)	GS 29.391 (29.391)	mem 66.472
Train: [0][581/750]	BT 0.034 (1.311)	DT 0.003 (1.253)	loss 10.105 (10.105)	gnorm 2236940.750 (2236940.750)	prob 0.697 (0.6971)	GS 31.031 (31.031)	mem 66.472
Train: [0][582/750]	BT 0.030 (1.309)	DT 0.002 (1.251)	loss 9.990 (9.990)	gnorm 2243673.250 (2243673.250)	prob 0.498 (0.4983)	GS 34.516 (34.516)	mem 66.470
Train: [0][583/750]	BT 0.031 (1.306)	DT 0.001 (1.248)	loss 10.269 (10.269)	gnorm 2303407.500 (2303407.500)	prob 0.573 (0.5726)	GS 34.109 (34.109)	mem 66.470
Train: [0][584/750]	BT 0.965 (1.306)	DT 0.863 (1.248)	loss 9.468 (9.468)	gnorm 2726185.750 (2726185.750)	prob 1.174 (1.1741)	GS 28.516 (28.516)	mem 66.413
Train: [0][585/750]	BT 0.134 (1.304)	DT 0.011 (1.246)	loss 10.210 (10.210)	gnorm 2320270.750 (2320270.750)	prob 0.448 (0.4482)	GS 30.766 (30.766)	mem 66.446
Train: [0][586/750]	BT 1.193 (1.304)	DT 1.089 (1.245)	loss 9.671 (9.671)	gnorm 2250304.250 (2250304.250)	prob 0.805 (0.8053)	GS 30.188 (30.188)	mem 66.449
Train: [0][587/750]	BT 0.059 (1.302)	DT 0.003 (1.243)	loss 10.232 (10.232)	gnorm 2487807.500 (2487807.500)	prob 0.158 (0.1576)	GS 34.422 (34.422)	mem 66.445
Train: [0][588/750]	BT 2.501 (1.304)	DT 2.464 (1.245)	loss 9.963 (9.963)	gnorm 2432071.750 (2432071.750)	prob 0.714 (0.7138)	GS 33.938 (33.938)	mem 66.447
Train: [0][589/750]	BT 0.053 (1.301)	DT 0.009 (1.243)	loss 9.820 (9.820)	gnorm 2238842.250 (2238842.250)	prob 0.605 (0.6048)	GS 31.125 (31.125)	mem 66.447
Train: [0][590/750]	BT 6.134 (1.310)	DT 6.058 (1.251)	loss 10.190 (10.190)	gnorm 2324576.750 (2324576.750)	prob 0.482 (0.4823)	GS 35.406 (35.406)	mem 66.451
Train: [0][591/750]	BT 0.045 (1.308)	DT 0.002 (1.249)	loss 9.860 (9.860)	gnorm 1950538.625 (1950538.625)	prob 0.557 (0.5568)	GS 29.016 (29.016)	mem 66.421
Train: [0][592/750]	BT 0.037 (1.305)	DT 0.001 (1.247)	loss 10.363 (10.363)	gnorm 2348496.250 (2348496.250)	prob 0.442 (0.4421)	GS 36.031 (36.031)	mem 66.421
Train: [0][593/750]	BT 0.067 (1.303)	DT 0.002 (1.245)	loss 10.283 (10.283)	gnorm 2462156.000 (2462156.000)	prob 0.426 (0.4257)	GS 31.875 (31.875)	mem 66.421
Train: [0][594/750]	BT 0.834 (1.303)	DT 0.787 (1.244)	loss 9.700 (9.700)	gnorm 2225976.750 (2225976.750)	prob 1.181 (1.1814)	GS 35.688 (35.688)	mem 66.710
Train: [0][595/750]	BT 0.029 (1.300)	DT 0.001 (1.242)	loss 10.111 (10.111)	gnorm 2022211.500 (2022211.500)	prob 0.926 (0.9261)	GS 28.219 (28.219)	mem 66.419
Train: [0][596/750]	BT 4.573 (1.306)	DT 4.501 (1.248)	loss 10.106 (10.106)	gnorm 2239244.750 (2239244.750)	prob 0.202 (0.2018)	GS 31.016 (31.016)	mem 66.416
Train: [0][597/750]	BT 0.130 (1.304)	DT 0.054 (1.246)	loss 10.183 (10.183)	gnorm 2308289.750 (2308289.750)	prob 0.198 (0.1984)	GS 30.594 (30.594)	mem 66.417
Train: [0][598/750]	BT 2.017 (1.305)	DT 1.910 (1.247)	loss 10.167 (10.167)	gnorm 2402461.000 (2402461.000)	prob 0.097 (0.0968)	GS 35.938 (35.938)	mem 66.175
Train: [0][599/750]	BT 0.130 (1.303)	DT 0.007 (1.245)	loss 9.940 (9.940)	gnorm 2022301.375 (2022301.375)	prob 0.509 (0.5087)	GS 30.016 (30.016)	mem 64.992
Train: [0][600/750]	BT 0.120 (1.301)	DT 0.087 (1.243)	loss 10.318 (10.318)	gnorm 2466523.000 (2466523.000)	prob 0.095 (0.0946)	GS 30.297 (30.297)	mem 62.997
Train: [0][601/750]	BT 0.034 (1.299)	DT 0.002 (1.241)	loss 10.195 (10.195)	gnorm 2318707.750 (2318707.750)	prob 0.463 (0.4627)	GS 28.203 (28.203)	mem 62.426
Train: [0][602/750]	BT 7.415 (1.309)	DT 7.343 (1.251)	loss 10.248 (10.248)	gnorm 3025843.000 (3025843.000)	prob 0.176 (0.1764)	GS 35.250 (35.250)	mem 60.904
Train: [0][603/750]	BT 0.038 (1.307)	DT 0.002 (1.249)	loss 10.111 (10.111)	gnorm 2582737.500 (2582737.500)	prob 0.310 (0.3099)	GS 29.594 (29.594)	mem 60.826
Train: [0][604/750]	BT 0.037 (1.305)	DT 0.002 (1.247)	loss 10.085 (10.085)	gnorm 2441529.000 (2441529.000)	prob 0.255 (0.2546)	GS 34.922 (34.922)	mem 60.835
Train: [0][605/750]	BT 0.035 (1.303)	DT 0.002 (1.245)	loss 10.229 (10.229)	gnorm 2102611.500 (2102611.500)	prob 0.543 (0.5426)	GS 29.844 (29.844)	mem 60.817
Train: [0][606/750]	BT 4.094 (1.308)	DT 4.040 (1.249)	loss 10.238 (10.238)	gnorm 3168570.500 (3168570.500)	prob 0.306 (0.3059)	GS 37.266 (37.266)	mem 61.674
Train: [0][607/750]	BT 0.086 (1.305)	DT 0.028 (1.247)	loss 9.855 (9.855)	gnorm 2038843.000 (2038843.000)	prob 0.804 (0.8037)	GS 33.031 (33.031)	mem 61.691
Train: [0][608/750]	BT 6.663 (1.314)	DT 6.567 (1.256)	loss 10.338 (10.338)	gnorm 2652617.000 (2652617.000)	prob 0.274 (0.2735)	GS 37.141 (37.141)	mem 62.052
Train: [0][609/750]	BT 0.059 (1.312)	DT 0.026 (1.254)	loss 9.867 (9.867)	gnorm 2192880.500 (2192880.500)	prob 0.556 (0.5563)	GS 32.828 (32.828)	mem 62.072
Train: [0][610/750]	BT 0.029 (1.310)	DT 0.002 (1.252)	loss 10.345 (10.345)	gnorm 2138458.500 (2138458.500)	prob 0.312 (0.3119)	GS 31.797 (31.797)	mem 62.084
Train: [0][611/750]	BT 0.039 (1.308)	DT 0.001 (1.250)	loss 10.040 (10.040)	gnorm 2372609.000 (2372609.000)	prob 0.911 (0.9106)	GS 35.453 (35.453)	mem 62.097
Train: [0][612/750]	BT 0.062 (1.306)	DT 0.004 (1.248)	loss 10.148 (10.148)	gnorm 3033781.000 (3033781.000)	prob 0.494 (0.4942)	GS 37.328 (37.328)	mem 62.110
Train: [0][613/750]	BT 0.047 (1.304)	DT 0.006 (1.246)	loss 9.803 (9.803)	gnorm 2072346.250 (2072346.250)	prob 0.491 (0.4908)	GS 28.953 (28.953)	mem 62.122
Train: [0][614/750]	BT 5.342 (1.311)	DT 5.309 (1.252)	loss 10.037 (10.037)	gnorm 2252444.750 (2252444.750)	prob 0.658 (0.6578)	GS 32.875 (32.875)	mem 63.068
Train: [0][615/750]	BT 0.034 (1.308)	DT 0.003 (1.250)	loss 10.045 (10.045)	gnorm 2022674.125 (2022674.125)	prob 0.776 (0.7762)	GS 29.266 (29.266)	mem 63.095
Train: [0][616/750]	BT 0.029 (1.306)	DT 0.001 (1.248)	loss 10.203 (10.203)	gnorm 2343872.250 (2343872.250)	prob 0.303 (0.3028)	GS 34.062 (34.062)	mem 63.138
Train: [0][617/750]	BT 0.116 (1.304)	DT 0.037 (1.246)	loss 10.160 (10.160)	gnorm 2265332.500 (2265332.500)	prob -0.162 (-0.1618)	GS 32.547 (32.547)	mem 63.087
Train: [0][618/750]	BT 0.042 (1.302)	DT 0.002 (1.244)	loss 9.905 (9.905)	gnorm 2051458.750 (2051458.750)	prob 0.310 (0.3105)	GS 32.938 (32.938)	mem 63.093
Train: [0][619/750]	BT 0.093 (1.300)	DT 0.002 (1.242)	loss 10.143 (10.143)	gnorm 2326880.250 (2326880.250)	prob 0.323 (0.3233)	GS 32.312 (32.312)	mem 63.100
Train: [0][620/750]	BT 8.926 (1.313)	DT 8.878 (1.255)	loss 10.291 (10.291)	gnorm 3197958.500 (3197958.500)	prob -0.017 (-0.0171)	GS 36.172 (36.172)	mem 64.696
Train: [0][621/750]	BT 0.079 (1.311)	DT 0.016 (1.253)	loss 10.232 (10.232)	gnorm 2400551.500 (2400551.500)	prob 0.259 (0.2588)	GS 31.734 (31.734)	mem 64.732
Train: [0][622/750]	BT 0.079 (1.309)	DT 0.010 (1.251)	loss 10.356 (10.356)	gnorm 2976724.250 (2976724.250)	prob -0.163 (-0.1629)	GS 35.266 (35.266)	mem 64.759
Train: [0][623/750]	BT 0.106 (1.307)	DT 0.030 (1.249)	loss 10.472 (10.472)	gnorm 3216975.000 (3216975.000)	prob 0.731 (0.7307)	GS 31.203 (31.203)	mem 64.768
Train: [0][624/750]	BT 0.057 (1.305)	DT 0.012 (1.247)	loss 9.991 (9.991)	gnorm 2559443.750 (2559443.750)	prob 0.331 (0.3307)	GS 35.297 (35.297)	mem 64.727
Train: [0][625/750]	BT 0.068 (1.303)	DT 0.002 (1.245)	loss 10.206 (10.206)	gnorm 2217754.250 (2217754.250)	prob 0.431 (0.4306)	GS 33.781 (33.781)	mem 64.735
Train: [0][626/750]	BT 6.138 (1.311)	DT 6.014 (1.252)	loss 10.301 (10.301)	gnorm 2268240.250 (2268240.250)	prob 0.121 (0.1205)	GS 31.734 (31.734)	mem 59.137
Train: [0][627/750]	BT 0.079 (1.309)	DT 0.017 (1.250)	loss 10.023 (10.023)	gnorm 2166083.250 (2166083.250)	prob 0.163 (0.1628)	GS 32.438 (32.438)	mem 59.155
Train: [0][628/750]	BT 0.035 (1.307)	DT 0.001 (1.248)	loss 10.015 (10.015)	gnorm 2574338.750 (2574338.750)	prob 0.310 (0.3099)	GS 34.281 (34.281)	mem 59.175
Train: [0][629/750]	BT 0.031 (1.305)	DT 0.001 (1.246)	loss 10.196 (10.196)	gnorm 2154071.750 (2154071.750)	prob -0.023 (-0.0231)	GS 32.344 (32.344)	mem 59.185
Train: [0][630/750]	BT 0.026 (1.303)	DT 0.001 (1.244)	loss 10.360 (10.360)	gnorm 2184280.500 (2184280.500)	prob -0.058 (-0.0584)	GS 32.891 (32.891)	mem 59.197
Train: [0][631/750]	BT 0.052 (1.301)	DT 0.002 (1.243)	loss 10.289 (10.289)	gnorm 2357413.500 (2357413.500)	prob -0.086 (-0.0857)	GS 30.781 (30.781)	mem 59.236
Train: [0][632/750]	BT 11.825 (1.317)	DT 11.783 (1.259)	loss 10.050 (10.050)	gnorm 2684183.750 (2684183.750)	prob 0.456 (0.4562)	GS 34.312 (34.312)	mem 61.872
Train: [0][633/750]	BT 0.083 (1.315)	DT 0.029 (1.257)	loss 10.089 (10.089)	gnorm 2231699.750 (2231699.750)	prob -0.227 (-0.2266)	GS 30.891 (30.891)	mem 61.884
Train: [0][634/750]	BT 0.040 (1.313)	DT 0.003 (1.255)	loss 10.272 (10.272)	gnorm 2473267.500 (2473267.500)	prob -0.280 (-0.2798)	GS 31.328 (31.328)	mem 61.895
Train: [0][635/750]	BT 0.039 (1.311)	DT 0.002 (1.253)	loss 10.144 (10.144)	gnorm 2293249.000 (2293249.000)	prob 0.137 (0.1368)	GS 33.781 (33.781)	mem 61.898
Train: [0][636/750]	BT 0.040 (1.309)	DT 0.003 (1.251)	loss 10.090 (10.090)	gnorm 2812607.750 (2812607.750)	prob 0.138 (0.1380)	GS 33.453 (33.453)	mem 61.906
Train: [0][637/750]	BT 0.043 (1.307)	DT 0.003 (1.249)	loss 10.093 (10.093)	gnorm 2060396.625 (2060396.625)	prob 0.164 (0.1639)	GS 32.172 (32.172)	mem 61.920
Train: [0][638/750]	BT 0.047 (1.305)	DT 0.014 (1.247)	loss 10.344 (10.344)	gnorm 2327013.000 (2327013.000)	prob 0.050 (0.0496)	GS 33.016 (33.016)	mem 61.584
Train: [0][639/750]	BT 0.037 (1.303)	DT 0.001 (1.245)	loss 10.105 (10.105)	gnorm 2310129.250 (2310129.250)	prob 0.437 (0.4365)	GS 31.531 (31.531)	mem 61.592
Train: [0][640/750]	BT 0.039 (1.301)	DT 0.003 (1.244)	loss 10.602 (10.602)	gnorm 3315907.500 (3315907.500)	prob 0.010 (0.0097)	GS 30.938 (30.938)	mem 61.601
Train: [0][641/750]	BT 0.036 (1.299)	DT 0.003 (1.242)	loss 10.375 (10.375)	gnorm 2431747.000 (2431747.000)	prob 0.372 (0.3724)	GS 32.594 (32.594)	mem 61.607
Train: [0][642/750]	BT 0.620 (1.298)	DT 0.519 (1.240)	loss 10.041 (10.041)	gnorm 2693540.750 (2693540.750)	prob 0.584 (0.5842)	GS 34.844 (34.844)	mem 61.783
Train: [0][643/750]	BT 0.070 (1.296)	DT 0.004 (1.239)	loss 10.293 (10.293)	gnorm 2582646.250 (2582646.250)	prob -0.070 (-0.0697)	GS 33.781 (33.781)	mem 61.820
Train: [0][644/750]	BT 11.091 (1.312)	DT 11.018 (1.254)	loss 10.148 (10.148)	gnorm 2304383.750 (2304383.750)	prob -0.248 (-0.2477)	GS 36.969 (36.969)	mem 63.688
Train: [0][645/750]	BT 0.058 (1.310)	DT 0.021 (1.252)	loss 10.401 (10.401)	gnorm 2414828.500 (2414828.500)	prob 0.181 (0.1808)	GS 30.078 (30.078)	mem 63.694
Train: [0][646/750]	BT 0.065 (1.308)	DT 0.004 (1.250)	loss 10.578 (10.578)	gnorm 3857151.750 (3857151.750)	prob -0.037 (-0.0369)	GS 35.641 (35.641)	mem 63.699
Train: [0][647/750]	BT 0.034 (1.306)	DT 0.002 (1.248)	loss 9.822 (9.822)	gnorm 1841341.875 (1841341.875)	prob 0.597 (0.5973)	GS 28.016 (28.016)	mem 63.711
Train: [0][648/750]	BT 0.042 (1.304)	DT 0.011 (1.246)	loss 10.307 (10.307)	gnorm 2847738.000 (2847738.000)	prob 0.649 (0.6492)	GS 37.625 (37.625)	mem 63.771
Train: [0][649/750]	BT 0.091 (1.302)	DT 0.001 (1.244)	loss 10.212 (10.212)	gnorm 2766771.500 (2766771.500)	prob 0.554 (0.5544)	GS 33.750 (33.750)	mem 63.953
Train: [0][650/750]	BT 3.276 (1.305)	DT 3.229 (1.247)	loss 10.060 (10.060)	gnorm 2342847.000 (2342847.000)	prob 0.452 (0.4516)	GS 36.016 (36.016)	mem 64.583
Train: [0][651/750]	BT 0.079 (1.303)	DT 0.008 (1.245)	loss 9.721 (9.721)	gnorm 1965001.250 (1965001.250)	prob 0.421 (0.4212)	GS 29.375 (29.375)	mem 64.634
Train: [0][652/750]	BT 0.113 (1.301)	DT 0.019 (1.243)	loss 9.927 (9.927)	gnorm 3354928.000 (3354928.000)	prob 0.292 (0.2917)	GS 34.172 (34.172)	mem 64.642
Train: [0][653/750]	BT 0.059 (1.299)	DT 0.009 (1.242)	loss 10.748 (10.748)	gnorm 3460329.250 (3460329.250)	prob -0.583 (-0.5833)	GS 30.734 (30.734)	mem 64.614
Train: [0][654/750]	BT 2.021 (1.300)	DT 1.978 (1.243)	loss 9.885 (9.885)	gnorm 3282865.500 (3282865.500)	prob 0.008 (0.0083)	GS 35.453 (35.453)	mem 64.910
Train: [0][655/750]	BT 0.152 (1.299)	DT 0.051 (1.241)	loss 10.337 (10.337)	gnorm 2979563.500 (2979563.500)	prob -0.125 (-0.1246)	GS 31.344 (31.344)	mem 64.878
Train: [0][656/750]	BT 8.624 (1.310)	DT 8.582 (1.252)	loss 9.921 (9.921)	gnorm 2418325.750 (2418325.750)	prob 0.093 (0.0929)	GS 33.844 (33.844)	mem 65.634
Train: [0][657/750]	BT 0.031 (1.308)	DT 0.002 (1.250)	loss 10.442 (10.442)	gnorm 2191356.000 (2191356.000)	prob -0.117 (-0.1169)	GS 29.484 (29.484)	mem 65.634
Train: [0][658/750]	BT 0.029 (1.306)	DT 0.001 (1.248)	loss 9.685 (9.685)	gnorm 2096383.250 (2096383.250)	prob 0.207 (0.2068)	GS 31.516 (31.516)	mem 65.634
Train: [0][659/750]	BT 0.050 (1.304)	DT 0.001 (1.246)	loss 10.293 (10.293)	gnorm 2113056.750 (2113056.750)	prob 0.061 (0.0612)	GS 32.703 (32.703)	mem 65.708
Train: [0][660/750]	BT 1.310 (1.304)	DT 1.244 (1.246)	loss 10.003 (10.003)	gnorm 2430299.250 (2430299.250)	prob 0.299 (0.2986)	GS 34.016 (34.016)	mem 65.632
Train: [0][661/750]	BT 0.036 (1.302)	DT 0.001 (1.244)	loss 10.080 (10.080)	gnorm 2219702.000 (2219702.000)	prob 0.188 (0.1884)	GS 31.156 (31.156)	mem 65.633
Train: [0][662/750]	BT 5.686 (1.309)	DT 5.623 (1.251)	loss 10.079 (10.079)	gnorm 2768819.250 (2768819.250)	prob 0.039 (0.0391)	GS 37.406 (37.406)	mem 65.871
Train: [0][663/750]	BT 0.065 (1.307)	DT 0.009 (1.249)	loss 10.446 (10.446)	gnorm 2445309.000 (2445309.000)	prob -0.126 (-0.1265)	GS 30.875 (30.875)	mem 65.873
Train: [0][664/750]	BT 0.042 (1.305)	DT 0.004 (1.247)	loss 10.305 (10.305)	gnorm 2757152.750 (2757152.750)	prob -0.186 (-0.1863)	GS 35.641 (35.641)	mem 65.869
Train: [0][665/750]	BT 0.062 (1.303)	DT 0.003 (1.245)	loss 10.401 (10.401)	gnorm 2335093.000 (2335093.000)	prob 0.394 (0.3945)	GS 33.094 (33.094)	mem 65.868
Train: [0][666/750]	BT 0.040 (1.301)	DT 0.007 (1.244)	loss 9.973 (9.973)	gnorm 2830215.750 (2830215.750)	prob 0.348 (0.3478)	GS 30.781 (30.781)	mem 65.868
Train: [0][667/750]	BT 0.054 (1.299)	DT 0.005 (1.242)	loss 10.023 (10.023)	gnorm 2024912.000 (2024912.000)	prob 0.541 (0.5412)	GS 32.312 (32.312)	mem 65.882
Train: [0][668/750]	BT 6.299 (1.307)	DT 6.197 (1.249)	loss 10.559 (10.559)	gnorm 3021346.750 (3021346.750)	prob 0.012 (0.0123)	GS 32.234 (32.234)	mem 65.630
Train: [0][669/750]	BT 0.034 (1.305)	DT 0.002 (1.247)	loss 10.344 (10.344)	gnorm 2341172.750 (2341172.750)	prob 0.541 (0.5414)	GS 33.969 (33.969)	mem 65.629
Train: [0][670/750]	BT 0.044 (1.303)	DT 0.002 (1.245)	loss 10.761 (10.761)	gnorm 3165701.500 (3165701.500)	prob -0.510 (-0.5097)	GS 30.359 (30.359)	mem 65.630
Train: [0][671/750]	BT 0.110 (1.301)	DT 0.002 (1.244)	loss 10.208 (10.208)	gnorm 2377485.750 (2377485.750)	prob 0.765 (0.7645)	GS 33.531 (33.531)	mem 65.631
Train: [0][672/750]	BT 2.191 (1.303)	DT 2.141 (1.245)	loss 10.562 (10.562)	gnorm 2715159.750 (2715159.750)	prob -0.208 (-0.2076)	GS 36.172 (36.172)	mem 65.633
Train: [0][673/750]	BT 0.038 (1.301)	DT 0.001 (1.243)	loss 9.966 (9.966)	gnorm 2242294.750 (2242294.750)	prob 0.528 (0.5277)	GS 30.094 (30.094)	mem 65.634
Train: [0][674/750]	BT 0.137 (1.299)	DT 0.098 (1.241)	loss 10.245 (10.245)	gnorm 2229674.000 (2229674.000)	prob 0.098 (0.0982)	GS 27.219 (27.219)	mem 65.636
Train: [0][675/750]	BT 0.064 (1.297)	DT 0.004 (1.240)	loss 10.033 (10.033)	gnorm 2178368.750 (2178368.750)	prob 1.274 (1.2744)	GS 30.234 (30.234)	mem 65.636
Train: [0][676/750]	BT 0.115 (1.295)	DT 0.012 (1.238)	loss 9.970 (9.970)	gnorm 2131284.000 (2131284.000)	prob 1.255 (1.2548)	GS 34.234 (34.234)	mem 65.569
Train: [0][677/750]	BT 0.075 (1.294)	DT 0.015 (1.236)	loss 10.140 (10.140)	gnorm 1831451.125 (1831451.125)	prob 0.930 (0.9295)	GS 27.141 (27.141)	mem 65.570
Train: [0][678/750]	BT 0.049 (1.292)	DT 0.013 (1.234)	loss 10.427 (10.427)	gnorm 2335705.500 (2335705.500)	prob 0.185 (0.1847)	GS 35.781 (35.781)	mem 65.570
Train: [0][679/750]	BT 0.065 (1.290)	DT 0.004 (1.232)	loss 10.316 (10.316)	gnorm 2234766.750 (2234766.750)	prob 0.763 (0.7632)	GS 28.391 (28.391)	mem 65.598
Train: [0][680/750]	BT 10.652 (1.304)	DT 10.619 (1.246)	loss 10.591 (10.591)	gnorm 3061424.750 (3061424.750)	prob 0.382 (0.3819)	GS 36.203 (36.203)	mem 65.617
Train: [0][681/750]	BT 0.032 (1.302)	DT 0.004 (1.244)	loss 10.087 (10.087)	gnorm 2160982.750 (2160982.750)	prob 1.081 (1.0809)	GS 28.062 (28.062)	mem 65.617
Train: [0][682/750]	BT 0.055 (1.300)	DT 0.001 (1.242)	loss 10.193 (10.193)	gnorm 2214538.250 (2214538.250)	prob 0.504 (0.5045)	GS 30.141 (30.141)	mem 65.619
Train: [0][683/750]	BT 0.100 (1.298)	DT 0.006 (1.241)	loss 10.183 (10.183)	gnorm 2148326.500 (2148326.500)	prob 0.919 (0.9192)	GS 30.562 (30.562)	mem 65.619
Train: [0][684/750]	BT 6.625 (1.306)	DT 6.577 (1.248)	loss 10.763 (10.763)	gnorm 2363716.250 (2363716.250)	prob -0.599 (-0.5994)	GS 32.891 (32.891)	mem 65.636
Train: [0][685/750]	BT 0.039 (1.304)	DT 0.004 (1.247)	loss 10.371 (10.371)	gnorm 2298128.750 (2298128.750)	prob 0.114 (0.1142)	GS 34.453 (34.453)	mem 65.637
Train: [0][686/750]	BT 0.114 (1.303)	DT 0.020 (1.245)	loss 10.104 (10.104)	gnorm 2774663.250 (2774663.250)	prob 0.466 (0.4655)	GS 36.453 (36.453)	mem 65.637
Train: [0][687/750]	BT 0.087 (1.301)	DT 0.002 (1.243)	loss 10.115 (10.115)	gnorm 2119948.250 (2119948.250)	prob 0.472 (0.4723)	GS 33.328 (33.328)	mem 65.637
Train: [0][688/750]	BT 0.068 (1.299)	DT 0.003 (1.241)	loss 10.122 (10.122)	gnorm 2334122.500 (2334122.500)	prob 0.543 (0.5435)	GS 31.547 (31.547)	mem 65.636
Train: [0][689/750]	BT 0.039 (1.297)	DT 0.001 (1.239)	loss 10.563 (10.563)	gnorm 2412485.000 (2412485.000)	prob 0.242 (0.2417)	GS 30.125 (30.125)	mem 65.636
Train: [0][690/750]	BT 0.048 (1.295)	DT 0.002 (1.238)	loss 10.051 (10.051)	gnorm 2691217.750 (2691217.750)	prob 0.035 (0.0352)	GS 35.156 (35.156)	mem 65.637
Train: [0][691/750]	BT 0.088 (1.294)	DT 0.024 (1.236)	loss 10.610 (10.610)	gnorm 2291866.500 (2291866.500)	prob 0.073 (0.0730)	GS 30.391 (30.391)	mem 65.638
Train: [0][692/750]	BT 8.979 (1.305)	DT 8.932 (1.247)	loss 10.220 (10.220)	gnorm 2128966.750 (2128966.750)	prob -0.034 (-0.0336)	GS 29.469 (29.469)	mem 59.505
Train: [0][693/750]	BT 0.055 (1.303)	DT 0.002 (1.245)	loss 10.377 (10.377)	gnorm 2498654.500 (2498654.500)	prob -0.030 (-0.0299)	GS 40.906 (40.906)	mem 59.511
Train: [0][694/750]	BT 0.070 (1.301)	DT 0.002 (1.243)	loss 10.045 (10.045)	gnorm 2718937.250 (2718937.250)	prob 0.029 (0.0294)	GS 34.766 (34.766)	mem 59.454
Train: [0][695/750]	BT 0.119 (1.299)	DT 0.024 (1.242)	loss 10.013 (10.013)	gnorm 2363336.250 (2363336.250)	prob 0.073 (0.0731)	GS 35.281 (35.281)	mem 59.491
Train: [0][696/750]	BT 5.045 (1.305)	DT 5.012 (1.247)	loss 10.655 (10.655)	gnorm 2780568.250 (2780568.250)	prob -0.329 (-0.3293)	GS 35.000 (35.000)	mem 60.221
Train: [0][697/750]	BT 0.040 (1.303)	DT 0.004 (1.245)	loss 10.243 (10.243)	gnorm 2640627.500 (2640627.500)	prob 0.763 (0.7631)	GS 31.750 (31.750)	mem 60.233
Train: [0][698/750]	BT 1.801 (1.304)	DT 1.730 (1.246)	loss 9.947 (9.947)	gnorm 2688792.750 (2688792.750)	prob 0.230 (0.2296)	GS 32.938 (32.938)	mem 60.642
Train: [0][699/750]	BT 1.936 (1.305)	DT 1.850 (1.247)	loss 10.122 (10.122)	gnorm 2961290.500 (2961290.500)	prob 0.158 (0.1580)	GS 55.719 (55.719)	mem 61.038
Train: [0][700/750]	BT 0.077 (1.303)	DT 0.017 (1.245)	loss 9.598 (9.598)	gnorm 1855259.875 (1855259.875)	prob 0.821 (0.8209)	GS 29.234 (29.234)	mem 61.054
Train: [0][701/750]	BT 0.038 (1.301)	DT 0.002 (1.243)	loss 10.415 (10.415)	gnorm 2227429.500 (2227429.500)	prob 0.094 (0.0935)	GS 27.859 (27.859)	mem 61.003
Train: [0][702/750]	BT 0.035 (1.299)	DT 0.002 (1.242)	loss 10.046 (10.046)	gnorm 2522728.000 (2522728.000)	prob 0.362 (0.3624)	GS 35.266 (35.266)	mem 61.013
Train: [0][703/750]	BT 0.055 (1.297)	DT 0.008 (1.240)	loss 10.075 (10.075)	gnorm 2319774.500 (2319774.500)	prob 0.212 (0.2119)	GS 28.734 (28.734)	mem 61.023
Train: [0][704/750]	BT 8.306 (1.307)	DT 8.270 (1.250)	loss 9.614 (9.614)	gnorm 2349737.750 (2349737.750)	prob 0.641 (0.6407)	GS 35.578 (35.578)	mem 62.550
Train: [0][705/750]	BT 0.035 (1.306)	DT 0.003 (1.248)	loss 10.507 (10.507)	gnorm 2206193.500 (2206193.500)	prob -0.186 (-0.1859)	GS 30.266 (30.266)	mem 62.609
Train: [0][706/750]	BT 0.080 (1.304)	DT 0.001 (1.246)	loss 10.546 (10.546)	gnorm 2376148.250 (2376148.250)	prob -0.581 (-0.5813)	GS 30.547 (30.547)	mem 62.602
Train: [0][707/750]	BT 0.109 (1.302)	DT 0.017 (1.244)	loss 10.521 (10.521)	gnorm 2231344.500 (2231344.500)	prob -0.159 (-0.1593)	GS 34.828 (34.828)	mem 62.633
Train: [0][708/750]	BT 1.529 (1.302)	DT 1.497 (1.245)	loss 9.578 (9.578)	gnorm 2313066.250 (2313066.250)	prob 0.604 (0.6043)	GS 31.000 (31.000)	mem 62.815
Train: [0][709/750]	BT 0.038 (1.301)	DT 0.002 (1.243)	loss 10.348 (10.348)	gnorm 2073754.625 (2073754.625)	prob -0.066 (-0.0662)	GS 30.594 (30.594)	mem 62.895
Train: [0][710/750]	BT 4.786 (1.306)	DT 4.729 (1.248)	loss 9.468 (9.468)	gnorm 2202136.500 (2202136.500)	prob 0.708 (0.7079)	GS 35.969 (35.969)	mem 63.648
Train: [0][711/750]	BT 0.064 (1.304)	DT 0.003 (1.246)	loss 10.702 (10.702)	gnorm 2372972.750 (2372972.750)	prob -0.273 (-0.2735)	GS 31.188 (31.188)	mem 63.614
Train: [0][712/750]	BT 0.070 (1.302)	DT 0.010 (1.245)	loss 10.439 (10.439)	gnorm 2220393.500 (2220393.500)	prob -0.400 (-0.4004)	GS 34.594 (34.594)	mem 63.638
Train: [0][713/750]	BT 0.062 (1.300)	DT 0.011 (1.243)	loss 10.348 (10.348)	gnorm 2363285.500 (2363285.500)	prob -0.010 (-0.0104)	GS 30.594 (30.594)	mem 63.689
Train: [0][714/750]	BT 0.069 (1.299)	DT 0.013 (1.241)	loss 10.245 (10.245)	gnorm 3420056.500 (3420056.500)	prob -0.057 (-0.0573)	GS 35.234 (35.234)	mem 63.820
Train: [0][715/750]	BT 0.053 (1.297)	DT 0.007 (1.239)	loss 10.338 (10.338)	gnorm 3097219.750 (3097219.750)	prob 0.406 (0.4060)	GS 33.141 (33.141)	mem 63.765
Train: [0][716/750]	BT 7.927 (1.306)	DT 7.895 (1.249)	loss 10.107 (10.107)	gnorm 2301309.750 (2301309.750)	prob 0.473 (0.4725)	GS 35.547 (35.547)	mem 65.080
Train: [0][717/750]	BT 0.032 (1.304)	DT 0.001 (1.247)	loss 10.030 (10.030)	gnorm 2553421.250 (2553421.250)	prob 0.377 (0.3772)	GS 31.547 (31.547)	mem 65.090
Train: [0][718/750]	BT 0.056 (1.303)	DT 0.002 (1.245)	loss 9.822 (9.822)	gnorm 2367688.750 (2367688.750)	prob 0.389 (0.3890)	GS 35.656 (35.656)	mem 65.101
Train: [0][719/750]	BT 0.030 (1.301)	DT 0.003 (1.243)	loss 10.350 (10.350)	gnorm 2361448.500 (2361448.500)	prob -0.020 (-0.0197)	GS 36.656 (36.656)	mem 65.107
Train: [0][720/750]	BT 3.085 (1.303)	DT 3.045 (1.246)	loss 10.219 (10.219)	gnorm 2597899.500 (2597899.500)	prob 0.047 (0.0468)	GS 35.031 (35.031)	mem 65.426
Train: [0][721/750]	BT 0.080 (1.302)	DT 0.012 (1.244)	loss 10.321 (10.321)	gnorm 2454622.500 (2454622.500)	prob 0.221 (0.2212)	GS 33.203 (33.203)	mem 65.415
Train: [0][722/750]	BT 1.591 (1.302)	DT 1.539 (1.245)	loss 10.459 (10.459)	gnorm 2411041.750 (2411041.750)	prob -0.303 (-0.3027)	GS 32.406 (32.406)	mem 65.394
Train: [0][723/750]	BT 0.082 (1.300)	DT 0.013 (1.243)	loss 10.520 (10.520)	gnorm 2308223.750 (2308223.750)	prob 0.184 (0.1837)	GS 31.875 (31.875)	mem 65.422
Train: [0][724/750]	BT 0.052 (1.299)	DT 0.005 (1.241)	loss 10.064 (10.064)	gnorm 2181717.500 (2181717.500)	prob -0.233 (-0.2333)	GS 32.016 (32.016)	mem 65.420
Train: [0][725/750]	BT 0.111 (1.297)	DT 0.003 (1.240)	loss 10.553 (10.553)	gnorm 2193003.500 (2193003.500)	prob -0.051 (-0.0509)	GS 26.953 (26.953)	mem 65.439
Train: [0][726/750]	BT 0.084 (1.295)	DT 0.005 (1.238)	loss 10.547 (10.547)	gnorm 2529516.500 (2529516.500)	prob 0.125 (0.1250)	GS 33.562 (33.562)	mem 65.390
Train: [0][727/750]	BT 0.109 (1.294)	DT 0.001 (1.236)	loss 10.387 (10.387)	gnorm 2183910.250 (2183910.250)	prob 0.035 (0.0350)	GS 28.141 (28.141)	mem 65.387
Train: [0][728/750]	BT 10.709 (1.307)	DT 10.617 (1.249)	loss 10.590 (10.590)	gnorm 2519088.750 (2519088.750)	prob -0.040 (-0.0404)	GS 30.609 (30.609)	mem 61.305
Train: [0][729/750]	BT 0.065 (1.305)	DT 0.013 (1.247)	loss 10.530 (10.530)	gnorm 2047737.250 (2047737.250)	prob 0.001 (0.0007)	GS 28.766 (28.766)	mem 61.321
Train: [0][730/750]	BT 0.036 (1.303)	DT 0.002 (1.246)	loss 10.320 (10.320)	gnorm 2432600.500 (2432600.500)	prob 0.049 (0.0485)	GS 34.750 (34.750)	mem 61.323
Train: [0][731/750]	BT 0.033 (1.301)	DT 0.002 (1.244)	loss 10.120 (10.120)	gnorm 1884837.500 (1884837.500)	prob 0.619 (0.6187)	GS 28.688 (28.688)	mem 61.329
Train: [0][732/750]	BT 4.744 (1.306)	DT 4.704 (1.249)	loss 10.370 (10.370)	gnorm 2427080.250 (2427080.250)	prob 0.029 (0.0286)	GS 36.391 (36.391)	mem 62.175
Train: [0][733/750]	BT 0.058 (1.304)	DT 0.003 (1.247)	loss 10.168 (10.168)	gnorm 2112734.250 (2112734.250)	prob -0.219 (-0.2195)	GS 27.625 (27.625)	mem 62.134
Train: [0][734/750]	BT 0.317 (1.303)	DT 0.268 (1.246)	loss 10.018 (10.018)	gnorm 2715682.500 (2715682.500)	prob 0.473 (0.4732)	GS 35.000 (35.000)	mem 59.653
Train: [0][735/750]	BT 0.057 (1.301)	DT 0.004 (1.244)	loss 10.193 (10.193)	gnorm 2200046.000 (2200046.000)	prob 0.369 (0.3692)	GS 34.891 (34.891)	mem 59.391
Train: [0][736/750]	BT 0.027 (1.300)	DT 0.002 (1.242)	loss 10.143 (10.143)	gnorm 2189871.000 (2189871.000)	prob 0.268 (0.2683)	GS 32.391 (32.391)	mem 59.388
Train: [0][737/750]	BT 0.025 (1.298)	DT 0.001 (1.241)	loss 10.095 (10.095)	gnorm 2046730.125 (2046730.125)	prob 0.456 (0.4557)	GS 33.031 (33.031)	mem 59.426
Train: [0][738/750]	BT 0.041 (1.296)	DT 0.001 (1.239)	loss 10.233 (10.233)	gnorm 2347523.000 (2347523.000)	prob 0.026 (0.0264)	GS 34.297 (34.297)	mem 59.490
Train: [0][739/750]	BT 0.058 (1.295)	DT 0.003 (1.237)	loss 10.398 (10.398)	gnorm 2025683.500 (2025683.500)	prob 0.019 (0.0185)	GS 29.578 (29.578)	mem 59.510
Train: [0][740/750]	BT 5.010 (1.300)	DT 4.980 (1.242)	loss 10.293 (10.293)	gnorm 2342380.500 (2342380.500)	prob -0.445 (-0.4450)	GS 31.672 (31.672)	mem 41.027
Train: [0][741/750]	BT 0.029 (1.298)	DT 0.001 (1.241)	loss 10.416 (10.416)	gnorm 2306991.500 (2306991.500)	prob 0.172 (0.1723)	GS 30.625 (30.625)	mem 41.039
Train: [0][742/750]	BT 0.030 (1.296)	DT 0.001 (1.239)	loss 9.954 (9.954)	gnorm 2122850.250 (2122850.250)	prob 0.569 (0.5685)	GS 33.203 (33.203)	mem 41.069
Train: [0][743/750]	BT 0.053 (1.295)	DT 0.001 (1.237)	loss 9.805 (9.805)	gnorm 1964656.625 (1964656.625)	prob 0.731 (0.7315)	GS 28.922 (28.922)	mem 41.093
Train: [0][744/750]	BT 2.977 (1.297)	DT 2.937 (1.240)	loss 10.392 (10.392)	gnorm 2547785.500 (2547785.500)	prob 0.066 (0.0655)	GS 32.297 (32.297)	mem 36.324
Train: [0][745/750]	BT 0.040 (1.295)	DT 0.002 (1.238)	loss 10.627 (10.627)	gnorm 2964754.750 (2964754.750)	prob 0.130 (0.1297)	GS 32.531 (32.531)	mem 36.339
Train: [0][746/750]	BT 0.031 (1.293)	DT 0.002 (1.236)	loss 10.240 (10.240)	gnorm 3959119.750 (3959119.750)	prob 0.204 (0.2041)	GS 30.781 (30.781)	mem 36.350
Train: [0][747/750]	BT 0.032 (1.292)	DT 0.001 (1.235)	loss 9.956 (9.956)	gnorm 3106056.500 (3106056.500)	prob 0.376 (0.3760)	GS 33.094 (33.094)	mem 36.318
Train: [0][748/750]	BT 0.032 (1.290)	DT 0.001 (1.233)	loss 10.328 (10.328)	gnorm 3209425.250 (3209425.250)	prob 0.388 (0.3877)	GS 36.875 (36.875)	mem 36.324
Train: [0][749/750]	BT 0.024 (1.288)	DT 0.001 (1.231)	loss 9.886 (9.886)	gnorm 2496309.000 (2496309.000)	prob 0.933 (0.9334)	GS 25.281 (25.281)	mem 36.336
Train: [0][750/750]	BT 0.024 (1.287)	DT 0.001 (1.230)	loss 10.336 (10.336)	gnorm 2919157.500 (2919157.500)	prob 0.897 (0.8965)	GS 31.688 (31.688)	mem 36.350
Train: [0][751/750]	BT 0.028 (1.285)	DT 0.001 (1.228)	loss 9.892 (9.892)	gnorm 3111077.750 (3111077.750)	prob 1.873 (1.8731)	GS 31.531 (31.531)	mem 36.363
Train: [0][752/750]	BT 0.032 (1.283)	DT 0.007 (1.226)	loss 10.161 (10.161)	gnorm 2823472.500 (2823472.500)	prob 1.591 (1.5911)	GS 29.094 (29.094)	mem 36.376
Train: [0][753/750]	BT 0.034 (1.282)	DT 0.002 (1.225)	loss 9.798 (9.798)	gnorm 2681117.750 (2681117.750)	prob 1.523 (1.5235)	GS 35.031 (35.031)	mem 36.389
Train: [0][754/750]	BT 0.031 (1.280)	DT 0.001 (1.223)	loss 10.983 (10.983)	gnorm 3807351.000 (3807351.000)	prob 0.149 (0.1490)	GS 37.594 (37.594)	mem 36.395
Train: [0][755/750]	BT 0.031 (1.278)	DT 0.001 (1.221)	loss 10.009 (10.009)	gnorm 2829614.250 (2829614.250)	prob 1.272 (1.2725)	GS 32.969 (32.969)	mem 36.403
Train: [0][756/750]	BT 2.451 (1.280)	DT 2.369 (1.223)	loss 10.077 (10.077)	gnorm 3445248.000 (3445248.000)	prob 1.475 (1.4754)	GS 33.531 (33.531)	mem 33.438
epoch 0, total time 967.81
==> Saving...
==> Saving...
==> training...
moco1
moco2
moco3
moco4
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
/home/shakir/.local/lib/python3.9/site-packages/dgl/data/graph_serialize.py:189: DGLWarning: You are loading a graph file saved by old version of dgl.              Please consider saving it again with the current format.
  dgl_warning(
Train: [1][1/750]	BT 22.057 (22.057)	DT 21.949 (21.949)	loss 10.058 (10.058)	gnorm 2244483.000 (2244483.000)	prob 1.608 (1.6083)	GS 32.734 (32.734)	mem 64.733
Train: [1][2/750]	BT 5.247 (13.652)	DT 5.157 (13.553)	loss 10.524 (10.524)	gnorm 3229330.750 (3229330.750)	prob 0.956 (0.9559)	GS 37.500 (37.500)	mem 65.073
Train: [1][3/750]	BT 0.098 (9.134)	DT 0.002 (9.036)	loss 10.365 (10.365)	gnorm 2052590.125 (2052590.125)	prob 1.147 (1.1469)	GS 28.812 (28.812)	mem 65.042
Train: [1][4/750]	BT 1.224 (7.156)	DT 1.182 (7.072)	loss 10.221 (10.221)	gnorm 2412519.250 (2412519.250)	prob 0.989 (0.9893)	GS 37.328 (37.328)	mem 65.095
Train: [1][5/750]	BT 0.047 (5.735)	DT 0.014 (5.661)	loss 10.031 (10.031)	gnorm 1831252.125 (1831252.125)	prob 0.817 (0.8174)	GS 30.328 (30.328)	mem 65.067
Train: [1][6/750]	BT 0.068 (4.790)	DT 0.003 (4.718)	loss 10.408 (10.408)	gnorm 2502835.750 (2502835.750)	prob 0.681 (0.6813)	GS 37.000 (37.000)	mem 65.091
Train: [1][7/750]	BT 0.105 (4.121)	DT 0.009 (4.045)	loss 10.541 (10.541)	gnorm 2212610.750 (2212610.750)	prob 0.830 (0.8302)	GS 35.469 (35.469)	mem 65.148
Train: [1][8/750]	BT 0.092 (3.617)	DT 0.002 (3.540)	loss 10.529 (10.529)	gnorm 2600419.750 (2600419.750)	prob 0.446 (0.4457)	GS 36.359 (36.359)	mem 65.149
Train: [1][9/750]	BT 0.077 (3.224)	DT 0.011 (3.148)	loss 10.354 (10.354)	gnorm 2390270.750 (2390270.750)	prob 0.847 (0.8473)	GS 34.469 (34.469)	mem 65.150
Train: [1][10/750]	BT 0.057 (2.907)	DT 0.003 (2.833)	loss 10.337 (10.337)	gnorm 2559338.000 (2559338.000)	prob 0.491 (0.4913)	GS 35.297 (35.297)	mem 65.069
Train: [1][11/750]	BT 0.049 (2.647)	DT 0.016 (2.577)	loss 10.263 (10.263)	gnorm 2221411.250 (2221411.250)	prob 0.160 (0.1601)	GS 34.516 (34.516)	mem 65.069
Train: [1][12/750]	BT 0.038 (2.430)	DT 0.004 (2.363)	loss 10.823 (10.823)	gnorm 3987759.500 (3987759.500)	prob -0.180 (-0.1797)	GS 38.531 (38.531)	mem 65.068
Train: [1][13/750]	BT 3.801 (2.536)	DT 3.769 (2.471)	loss 10.393 (10.393)	gnorm 2343375.500 (2343375.500)	prob 0.543 (0.5434)	GS 29.484 (29.484)	mem 58.732
Train: [1][14/750]	BT 7.113 (2.862)	DT 7.065 (2.799)	loss 10.029 (10.029)	gnorm 2323017.250 (2323017.250)	prob 0.661 (0.6605)	GS 34.641 (34.641)	mem 60.196
Train: [1][15/750]	BT 0.067 (2.676)	DT 0.012 (2.613)	loss 9.854 (9.854)	gnorm 2054989.375 (2054989.375)	prob 1.273 (1.2726)	GS 32.406 (32.406)	mem 60.214
Train: [1][16/750]	BT 2.291 (2.652)	DT 2.255 (2.591)	loss 10.061 (10.061)	gnorm 2054867.625 (2054867.625)	prob 0.805 (0.8047)	GS 33.453 (33.453)	mem 60.642
Train: [1][17/750]	BT 0.039 (2.498)	DT 0.003 (2.439)	loss 9.921 (9.921)	gnorm 2177800.000 (2177800.000)	prob 0.851 (0.8509)	GS 29.453 (29.453)	mem 60.658
Train: [1][18/750]	BT 0.677 (2.397)	DT 0.597 (2.336)	loss 10.140 (10.140)	gnorm 2381406.500 (2381406.500)	prob 0.336 (0.3361)	GS 31.656 (31.656)	mem 60.763
Train: [1][19/750]	BT 0.061 (2.274)	DT 0.003 (2.213)	loss 10.388 (10.388)	gnorm 1885901.000 (1885901.000)	prob 0.130 (0.1299)	GS 26.609 (26.609)	mem 60.794
Train: [1][20/750]	BT 0.110 (2.166)	DT 0.009 (2.103)	loss 10.147 (10.147)	gnorm 2350362.000 (2350362.000)	prob 0.197 (0.1975)	GS 32.781 (32.781)	mem 60.844
Train: [1][21/750]	BT 0.080 (2.067)	DT 0.014 (2.004)	loss 10.199 (10.199)	gnorm 1917401.125 (1917401.125)	prob 0.147 (0.1471)	GS 28.250 (28.250)	mem 60.853
Train: [1][22/750]	BT 2.420 (2.083)	DT 2.385 (2.021)	loss 10.448 (10.448)	gnorm 2112581.000 (2112581.000)	prob -0.064 (-0.0636)	GS 30.938 (30.938)	mem 61.331
Train: [1][23/750]	BT 0.050 (1.994)	DT 0.002 (1.933)	loss 10.420 (10.420)	gnorm 2178609.500 (2178609.500)	prob 0.040 (0.0399)	GS 30.906 (30.906)	mem 61.339
Train: [1][24/750]	BT 0.086 (1.915)	DT 0.002 (1.853)	loss 10.049 (10.049)	gnorm 1954717.375 (1954717.375)	prob -0.002 (-0.0016)	GS 31.766 (31.766)	mem 61.345
Train: [1][25/750]	BT 1.160 (1.885)	DT 1.077 (1.822)	loss 10.036 (10.036)	gnorm 2049857.500 (2049857.500)	prob 0.548 (0.5484)	GS 30.469 (30.469)	mem 61.591
Train: [1][26/750]	BT 9.236 (2.167)	DT 9.194 (2.105)	loss 10.554 (10.554)	gnorm 2694424.750 (2694424.750)	prob 0.160 (0.1599)	GS 34.281 (34.281)	mem 63.345
Train: [1][27/750]	BT 0.065 (2.090)	DT 0.008 (2.028)	loss 10.183 (10.183)	gnorm 1932086.875 (1932086.875)	prob 0.005 (0.0055)	GS 31.938 (31.938)	mem 63.386
Train: [1][28/750]	BT 1.736 (2.077)	DT 1.696 (2.016)	loss 10.021 (10.021)	gnorm 2534705.750 (2534705.750)	prob 0.373 (0.3733)	GS 31.141 (31.141)	mem 63.769
Train: [1][29/750]	BT 0.035 (2.007)	DT 0.003 (1.946)	loss 11.043 (11.043)	gnorm 2320855.250 (2320855.250)	prob -0.404 (-0.4039)	GS 30.547 (30.547)	mem 63.775
Train: [1][30/750]	BT 0.033 (1.941)	DT 0.004 (1.882)	loss 10.181 (10.181)	gnorm 2220396.000 (2220396.000)	prob -0.186 (-0.1858)	GS 31.734 (31.734)	mem 63.786
Train: [1][31/750]	BT 0.036 (1.879)	DT 0.002 (1.821)	loss 10.518 (10.518)	gnorm 2327160.250 (2327160.250)	prob 0.071 (0.0706)	GS 32.359 (32.359)	mem 63.793
Train: [1][32/750]	BT 0.033 (1.822)	DT 0.002 (1.764)	loss 10.362 (10.362)	gnorm 2952912.750 (2952912.750)	prob -0.011 (-0.0114)	GS 35.125 (35.125)	mem 63.798
Train: [1][33/750]	BT 0.041 (1.768)	DT 0.002 (1.711)	loss 10.337 (10.337)	gnorm 2583759.500 (2583759.500)	prob 0.500 (0.4996)	GS 33.203 (33.203)	mem 63.805
Train: [1][34/750]	BT 1.933 (1.772)	DT 1.897 (1.716)	loss 10.416 (10.416)	gnorm 2627084.000 (2627084.000)	prob 0.156 (0.1562)	GS 36.297 (36.297)	mem 64.108
Train: [1][35/750]	BT 0.048 (1.723)	DT 0.015 (1.668)	loss 9.829 (9.829)	gnorm 1864776.000 (1864776.000)	prob 0.741 (0.7413)	GS 25.984 (25.984)	mem 64.112
Train: [1][36/750]	BT 0.085 (1.678)	DT 0.011 (1.622)	loss 10.447 (10.447)	gnorm 2925867.500 (2925867.500)	prob 0.255 (0.2545)	GS 32.062 (32.062)	mem 64.140
Train: [1][37/750]	BT 0.070 (1.634)	DT 0.006 (1.578)	loss 10.288 (10.288)	gnorm 2214271.750 (2214271.750)	prob 0.902 (0.9021)	GS 31.266 (31.266)	mem 64.153
Train: [1][38/750]	BT 11.607 (1.897)	DT 11.545 (1.840)	loss 10.791 (10.791)	gnorm 2789064.250 (2789064.250)	prob 0.446 (0.4460)	GS 33.781 (33.781)	mem 59.538
Train: [1][39/750]	BT 0.085 (1.850)	DT 0.012 (1.793)	loss 9.934 (9.934)	gnorm 2342148.000 (2342148.000)	prob 1.164 (1.1642)	GS 32.578 (32.578)	mem 59.557
Train: [1][40/750]	BT 0.063 (1.806)	DT 0.011 (1.749)	loss 10.294 (10.294)	gnorm 2967912.750 (2967912.750)	prob 0.640 (0.6403)	GS 34.078 (34.078)	mem 59.567
Train: [1][41/750]	BT 0.046 (1.763)	DT 0.003 (1.706)	loss 10.977 (10.977)	gnorm 2473647.750 (2473647.750)	prob 0.403 (0.4028)	GS 30.797 (30.797)	mem 59.611
Train: [1][42/750]	BT 0.135 (1.724)	DT 0.011 (1.666)	loss 10.281 (10.281)	gnorm 2259233.250 (2259233.250)	prob 0.257 (0.2568)	GS 32.875 (32.875)	mem 59.599
Train: [1][43/750]	BT 0.078 (1.686)	DT 0.007 (1.627)	loss 10.515 (10.515)	gnorm 2393775.250 (2393775.250)	prob 0.778 (0.7776)	GS 32.125 (32.125)	mem 59.609
Train: [1][44/750]	BT 0.079 (1.649)	DT 0.009 (1.590)	loss 10.576 (10.576)	gnorm 2471846.000 (2471846.000)	prob 0.200 (0.2003)	GS 37.562 (37.562)	mem 59.622
Train: [1][45/750]	BT 0.059 (1.614)	DT 0.007 (1.555)	loss 9.834 (9.834)	gnorm 2067071.750 (2067071.750)	prob 1.597 (1.5975)	GS 33.219 (33.219)	mem 59.640
Train: [1][46/750]	BT 3.776 (1.661)	DT 3.735 (1.603)	loss 11.170 (11.170)	gnorm 4136392.250 (4136392.250)	prob 0.028 (0.0279)	GS 35.516 (35.516)	mem 60.228
Train: [1][47/750]	BT 0.114 (1.628)	DT 0.009 (1.569)	loss 10.084 (10.084)	gnorm 2075075.750 (2075075.750)	prob 0.827 (0.8273)	GS 26.922 (26.922)	mem 60.237
Train: [1][48/750]	BT 0.046 (1.595)	DT 0.014 (1.536)	loss 10.411 (10.411)	gnorm 2187179.000 (2187179.000)	prob 0.624 (0.6241)	GS 35.578 (35.578)	mem 60.243
Train: [1][49/750]	BT 0.047 (1.563)	DT 0.008 (1.505)	loss 10.058 (10.058)	gnorm 2009076.625 (2009076.625)	prob 1.252 (1.2523)	GS 27.266 (27.266)	mem 60.253
Train: [1][50/750]	BT 8.080 (1.694)	DT 8.010 (1.635)	loss 10.780 (10.780)	gnorm 2816466.000 (2816466.000)	prob 0.244 (0.2445)	GS 29.359 (29.359)	mem 61.891
Train: [1][51/750]	BT 0.073 (1.662)	DT 0.004 (1.603)	loss 10.090 (10.090)	gnorm 2258210.750 (2258210.750)	prob 0.901 (0.9015)	GS 33.281 (33.281)	mem 61.896
Train: [1][52/750]	BT 1.818 (1.665)	DT 1.748 (1.606)	loss 10.356 (10.356)	gnorm 2555374.750 (2555374.750)	prob 0.479 (0.4791)	GS 37.094 (37.094)	mem 62.313
